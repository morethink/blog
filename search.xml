<?xml version="1.0" encoding="utf-8"?>
<search>
  <entry>
    <title><![CDATA[Docker中执行Shell出现乱码]]></title>
    <url>%2FDocker%E4%B8%AD%E6%89%A7%E8%A1%8CShell%E5%87%BA%E7%8E%B0%E4%B9%B1%E7%A0%81%2F</url>
    <content type="text"><![CDATA[问题描述最近遇到一个问题：执行命令 docker exec f4af9b sh -c &#39;bash /tmp/build.sh&#39; 命令在docker中执行shell，会出现中文乱码的问题。但是在docker容器中单独执行shell脚本却没有出现乱码。查看环境变量存在LANG=en_US.UTF-8，因此从原理上来说是不应该出现乱码的。 但是既然出现了乱码，那么LANG=en_US.UTF-8应该就没有读取到，于是在 build.sh中运行env命令，发现通过docker exec f4af9b sh -c &#39;bash /tmp/build.sh&#39;方式没有LANG=en_US.UTF-8环境变量，那么原因是什么？ 原因定位原因如下：docker exec f4af9b sh -c &#39;bash /tmp/build.sh&#39; 对于docker 容器来说是非登录和非交互式shell，这样就不会读取某些配置文件，导致LANG=en_US.UTF-8没有加载成功。 Linux Shell下面介绍一下Linux交互式和非交互式shell、登录和非登录shell之间的区别。 交互式shell（interactive shell）和非交互式shell（non-interactive shell）： 交互式的shell会有一个输入提示符，并且它的标准输入、输出和错误输出都会显示在控制台上。这种模式也是大多数用户非常熟悉的：登录、执行一些命令、退出。当你退出后，shell也终止了。 非交互式shell是bash script.sh这类的shell。在这种模式下，shell不与你进行交互，而是读取存放在文件中的命令，并且执行它们。当它读到文件的结尾EOF，shell也就终止了。 登录式shell（login shell）和非登陆式shell（no-login shell）： 需要输入用户名和密码的shell就是登陆式shell。因此通常不管以何种方式登陆机器后用户获得的第一个shell就是login shell。不输入密码的ssh是公钥打通的，某种意义上说也是输入密码的。 非登陆式的就是在登陆后启动bash等，即不是远程登陆到主机这种。 对于常用环境变量设置文件，整理出如下加载情况表： 文件 非交互+登陆式 交互+登陆式 交互+非登陆式 非交互+非登陆式 /etc/profile 加载 加载 - - /etc/bashrc 加载 加载 - - ~/.bash_profile 加载 加载 - - ~/.bashrc 加载 加载 加载 - BASH_ENV - - - 加载 执行脚本，如bash script.sh是属于non-login + non-interactive。 解决思路因而，执行命令docker exec f4af9b sh -c &#39;bash /tmp/build.sh&#39;对于docker容器来说是属于non-login + non-interactive。 将上面的bash /tmp/build.sh改为bash --login /tmp/build.sh变为登录shell，就可以读取/etc/profile和~/.bash_profile等文件。 或者在执行bash /tmp/build.sh时在build.sh加入export LANG=&quot;en_US.UTF-8&quot;手动设置。 常见的shell变量PATH：决定了shell将到哪些目录中寻找命令或程序HOME：当前用户主目录MAIL：是指当前用户的邮件存放目录。SHELL：是指当前用户用的是哪种Shell。HISTSIZE：是指保存历史命令记录的条数LOGNAME：是指当前用户的登录名。HOSTNAME：是指主机的名称，许多应用程序如果要用到主机名的话，通常是从这个环境变量中来取得的。LANG/LANGUGE：是和语言相关的环境变量，使用多种语言的用户可以修改此环境变量。PS1：是基本提示符，对于root用户是#，对于普通用户是$。PS2：是附属提示符，默认是&quot;&gt;&quot;。]]></content>
      <categories>
        <category>Linux</category>
      </categories>
      <tags>
        <tag>Docker</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Netty接收HTTP文件上传及文件下载]]></title>
    <url>%2FNetty%E6%8E%A5%E6%94%B6HTTP%E6%96%87%E4%BB%B6%E4%B8%8A%E4%BC%A0%E5%8F%8A%E6%96%87%E4%BB%B6%E4%B8%8B%E8%BD%BD%2F</url>
    <content type="text"><![CDATA[文件上传这个处理器的原理是接收HttpObject对象，按照HttpRequest，HttpContent来做处理，文件内容是在HttpContent消息带来的。 然后在HttpContent中一个chunk一个chunk读，chunk大小可以在初始化HttpServerCodec时设置。将每个chunk交个httpDecoder复制一份，当读到LastHttpContent对象时，表明上传结束，可以将httpDecoder中缓存的文件通过HttpDataFactory写到磁盘上，然后在删除缓存的HttpContent对象。 @Slf4jpublic class HttUploadHandler extends SimpleChannelInboundHandler&lt;HttpObject&gt; &#123; public HttUploadHandler() &#123; super(false); &#125; private static final HttpDataFactory factory = new DefaultHttpDataFactory(true); private static final String FILE_UPLOAD = "/data/"; private static final String URI = "/upload"; private HttpPostRequestDecoder httpDecoder; HttpRequest request; @Override protected void channelRead0(final ChannelHandlerContext ctx, final HttpObject httpObject) throws Exception &#123; if (httpObject instanceof HttpRequest) &#123; request = (HttpRequest) httpObject; if (request.uri().startsWith(URI) &amp;&amp; request.method().equals(HttpMethod.POST)) &#123; httpDecoder = new HttpPostRequestDecoder(factory, request); httpDecoder.setDiscardThreshold(0); &#125; else &#123; //传递给下一个Handler ctx.fireChannelRead(httpObject); &#125; &#125; if (httpObject instanceof HttpContent) &#123; if (httpDecoder != null) &#123; final HttpContent chunk = (HttpContent) httpObject; httpDecoder.offer(chunk); if (chunk instanceof LastHttpContent) &#123; writeChunk(ctx); //关闭httpDecoder httpDecoder.destroy(); httpDecoder = null; &#125; ReferenceCountUtil.release(httpObject); &#125; else &#123; ctx.fireChannelRead(httpObject); &#125; &#125; &#125; private void writeChunk(ChannelHandlerContext ctx) throws IOException &#123; while (httpDecoder.hasNext()) &#123; InterfaceHttpData data = httpDecoder.next(); if (data != null &amp;&amp; HttpDataType.FileUpload.equals(data.getHttpDataType())) &#123; final FileUpload fileUpload = (FileUpload) data; final File file = new File(FILE_UPLOAD + fileUpload.getFilename()); log.info("upload file: &#123;&#125;", file); try (FileChannel inputChannel = new FileInputStream(fileUpload.getFile()).getChannel(); FileChannel outputChannel = new FileOutputStream(file).getChannel()) &#123; outputChannel.transferFrom(inputChannel, 0, inputChannel.size()); ResponseUtil.response(ctx, request, new GeneralResponse(HttpResponseStatus.OK, "SUCCESS", null)); &#125; &#125; &#125; &#125; @Override public void exceptionCaught(ChannelHandlerContext ctx, Throwable cause) throws Exception &#123; log.warn("&#123;&#125;", cause); ctx.channel().close(); &#125; @Override public void channelInactive(ChannelHandlerContext ctx) throws Exception &#123; if (httpDecoder != null) &#123; httpDecoder.cleanFiles(); &#125; &#125;&#125; 文件下载参考官方demo： https://github.com/netty/netty/blob/4.1/example/src/main/java/io/netty/example/file/FileServerHandler.java 做了改动： 为了更高效的传输大数据，实例中用到了ChunkedWriteHandler编码器，它提供了以zero-memory-copy方式写文件。 通过ChannelProgressiveFutureListener对文件下载过程进行监听。 // 新增ChunkedHandler，主要作用是支持异步发送大的码流（例如大文件传输），但是不占用过多的内存，防止发生java内存溢出错误ch.pipeline().addLast(new ChunkedWriteHandler()); @Slf4jpublic class HttpDownloadHandler extends SimpleChannelInboundHandler&lt;FullHttpRequest&gt; &#123; public HttpDownloadHandler() &#123; super(false); &#125; /** * 分块大小 */ private static final int CHUNK_SIZE = 1024 * 1024 * 10; private String filePath = "/data/body.csv"; @Override protected void channelRead0(ChannelHandlerContext ctx, FullHttpRequest request) &#123; String uri = request.uri(); if (uri.startsWith("/download") &amp;&amp; request.method().equals(HttpMethod.GET)) &#123; GeneralResponse generalResponse = null; File file = new File(filePath); try (RandomAccessFile randomAccessFile = new RandomAccessFile(file, "r")) &#123; long fileLength = randomAccessFile.length(); HttpResponse response = new DefaultHttpResponse(HTTP_1_1, HttpResponseStatus.OK); response.headers().set(HttpHeaderNames.CONTENT_LENGTH, fileLength); response.headers().set(HttpHeaderNames.CONTENT_TYPE, "application/octet-stream"); response.headers().add(HttpHeaderNames.CONTENT_DISPOSITION, String.format("attachment; filename=\"%s\"", file.getName())); ctx.write(response); ChannelFuture sendFileFuture = null; sendFileFuture = ctx.write(new ChunkedFile(randomAccessFile, 0, fileLength, CHUNK_SIZE), ctx.newProgressivePromise()); sendFileFuture.addListener(new ChannelProgressiveFutureListener() &#123; @Override public void operationComplete(ChannelProgressiveFuture future) throws Exception &#123; log.info("file &#123;&#125; dowonload complete.", file.getName()); &#125; @Override public void operationProgressed(ChannelProgressiveFuture future, long progress, long total) throws Exception &#123; if (total &lt; 0) &#123; log.warn("file &#123;&#125; transfer progress: &#123;&#125;", file.getName(), progress); &#125; else &#123; log.info("file &#123;&#125; transfer progress: &#123;&#125;/&#123;&#125;", file.getName(), progress, total); &#125; &#125; &#125;); ctx.writeAndFlush(LastHttpContent.EMPTY_LAST_CONTENT); &#125; catch (FileNotFoundException e) &#123; log.warn("file &#123;&#125; not found", file.getPath()); generalResponse = new GeneralResponse(HttpResponseStatus.NOT_FOUND, String.format("file %s not found", file.getPath()), null); ResponseUtil.response(ctx, request, generalResponse); &#125; catch (IOException e) &#123; log.warn("file &#123;&#125; has a IOException: &#123;&#125;", file.getName(), e.getMessage()); generalResponse = new GeneralResponse(HttpResponseStatus.INTERNAL_SERVER_ERROR, String.format("读取 file %s 发生异常", filePath), null); ResponseUtil.response(ctx, request, generalResponse); &#125; &#125; else &#123; ctx.fireChannelRead(request); &#125; &#125; @Override public void exceptionCaught(ChannelHandlerContext ctx, Throwable e) &#123; log.warn("&#123;&#125;", e); ctx.close(); &#125;&#125; 代码放在： https://github.com/morethink/code/tree/master/java/netty-example]]></content>
      <categories>
        <category>Java</category>
      </categories>
      <tags>
        <tag>Netty</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Netty URL路由方案探讨]]></title>
    <url>%2FNetty-URL-routing-scheme%2F</url>
    <content type="text"><![CDATA[最近在用Netty做开发，需要提供一个http web server，供调用方调用。采用Netty本身提供的HttpServerCodec handler进行Http协议的解析，但是需要自己提供路由。 最开始是通过对Http method及uri 采用多层if else 嵌套判断的方法路由到真正的controller类：String uri = request.uri();HttpMethod method = request.method();if (method == HttpMethod.POST) &#123; if (uri.startsWith("/login")) &#123; //url参数解析，调用controller的方法 &#125; else if (uri.startsWith("/logout")) &#123; //同上 &#125;&#125; else if (method == HttpMethod.GET) &#123; if (uri.startsWith("/")) &#123; &#125; else if (uri.startsWith("/status")) &#123; &#125;&#125; 在只需提供login及logoutAPI时，代码可以完成功能，可是随着API的数量越来越多，需要支持的方法及uri越来越多，else if 越来越多，代码越来越复杂。 在阿里开发手册中也提到过： 因此首先考虑采用状态设计模式及策略设计模式重构。 状态模式状态模式的角色： state状态表示状态，定义了根据不同状态进行不同处理的接口，该接口是那些处理内容依赖于状态的方法集合，对应实例的state类 具体的状态实现了state接口，对应daystate和nightstate contextcontext持有当前状态的具体状态的实例，此外，他还定义了供外部调用者使用的状态模式的接口。 首先我们知道每个http请求都是由method及uri来唯一标识的，所谓路由就是通过这个唯一标识定位到controller类的中的某个方法。 因此把HttpLabel作为状态@Data@AllArgsConstructorpublic class HttpLabel &#123; private String uri; private HttpMethod method;&#125; 状态接口：public interface Route &#123; /** * 路由 * * @param request * @return */ GeneralResponse call(FullHttpRequest request);&#125; 为每个状态添加状态实现：public void route() &#123; //单例controller类 final DemoController demoController = DemoController.getInstance(); Map&lt;HttpLabel, Route&gt; map = new HashMap&lt;&gt;(); map.put(new HttpLabel("/login", HttpMethod.POST), demoController::login); map.put(new HttpLabel("/logout", HttpMethod.POST), demoController::login);&#125; 接到请求，判断状态，调用不同接口：public class ServerHandler extends SimpleChannelInboundHandler&lt;FullHttpRequest&gt; &#123; @Override public void channelRead0(ChannelHandlerContext ctx, FullHttpRequest request) &#123; String uri = request.uri(); GeneralResponse generalResponse; if (uri.contains("?")) &#123; uri = uri.substring(0, uri.indexOf("?")); &#125; Route route = map.get(new HttpLabel(uri, request.method())); if (route != null) &#123; ResponseUtil.response(ctx, request, route.call(request)); &#125; else &#123; generalResponse = new GeneralResponse(HttpResponseStatus.BAD_REQUEST, "请检查你的请求方法及url", null); ResponseUtil.response(ctx, request, generalResponse); &#125; &#125;&#125; 使用状态设计模式重构代码，在增加url时只需要网map里面put一个值就行了。 类似SpringMVC路由功能后来看了 JAVA反射+运行时注解实现URL路由 发现反射+注解的方式很优雅，代码也不复杂。 下面介绍Netty使用反射实现URL路由。 路由注解：@Target(ElementType.METHOD)@Retention(RetentionPolicy.RUNTIME)public @interface RequestMapping &#123; /** * 路由的uri * * @return */ String uri(); /** * 路由的方法 * * @return */ String method();&#125; 扫描classpath下带有@RequestMapping注解的方法，将这个方法放进一个路由Map：Map&lt;HttpLabel, Action&lt;GeneralResponse&gt;&gt; httpRouterAction，key为上面提到过的Http唯一标识 HttpLabel，value为通过反射调用的方法：@Slf4jpublic class HttpRouter extends ClassLoader &#123; private Map&lt;HttpLabel, Action&lt;GeneralResponse&gt;&gt; httpRouterAction = new HashMap&lt;&gt;(); private String classpath = this.getClass().getResource("").getPath(); private Map&lt;String, Object&gt; controllerBeans = new HashMap&lt;&gt;(); @Override protected Class&lt;?&gt; findClass(String name) throws ClassNotFoundException &#123; String path = classpath + name.replaceAll("\\.", "/"); byte[] bytes; try (InputStream ins = new FileInputStream(path)) &#123; try (ByteArrayOutputStream out = new ByteArrayOutputStream()) &#123; byte[] buffer = new byte[1024 * 5]; int b = 0; while ((b = ins.read(buffer)) != -1) &#123; out.write(buffer, 0, b); &#125; bytes = out.toByteArray(); &#125; &#125; catch (Exception e) &#123; throw new ClassNotFoundException(); &#125; return defineClass(name, bytes, 0, bytes.length); &#125; public void addRouter(String controllerClass) &#123; try &#123; Class&lt;?&gt; cls = loadClass(controllerClass); Method[] methods = cls.getDeclaredMethods(); for (Method invokeMethod : methods) &#123; Annotation[] annotations = invokeMethod.getAnnotations(); for (Annotation annotation : annotations) &#123; if (annotation.annotationType() == RequestMapping.class) &#123; RequestMapping requestMapping = (RequestMapping) annotation; String uri = requestMapping.uri(); String httpMethod = requestMapping.method().toUpperCase(); // 保存Bean单例 if (!controllerBeans.containsKey(cls.getName())) &#123; controllerBeans.put(cls.getName(), cls.newInstance()); &#125; Action action = new Action(controllerBeans.get(cls.getName()), invokeMethod); //如果需要FullHttpRequest，就注入FullHttpRequest对象 Class[] params = invokeMethod.getParameterTypes(); if (params.length == 1 &amp;&amp; params[0] == FullHttpRequest.class) &#123; action.setInjectionFullhttprequest(true); &#125; // 保存映射关系 httpRouterAction.put(new HttpLabel(uri, new HttpMethod(httpMethod)), action); &#125; &#125; &#125; &#125; catch (Exception e) &#123; log.warn("&#123;&#125;", e); &#125; &#125; public Action getRoute(HttpLabel httpLabel) &#123; return httpRouterAction.get(httpLabel); &#125;&#125; 通过反射调用controller 类中的方法：@Data@RequiredArgsConstructor@Slf4jpublic class Action&lt;T&gt; &#123; @NonNull private Object object; @NonNull private Method method; private boolean injectionFullhttprequest; public T call(Object... args) &#123; try &#123; return (T) method.invoke(object, args); &#125; catch (IllegalAccessException | InvocationTargetException e) &#123; log.warn("&#123;&#125;", e); &#125; return null; &#125; ServerHandler.java处理如下： //根据不同的请求API做不同的处理(路由分发)Action&lt;GeneralResponse&gt; action = httpRouter.getRoute(new HttpLabel(uri, request.method()));if (action != null) &#123; if (action.isInjectionFullhttprequest()) &#123; ResponseUtil.response(ctx, request, action.call(request)); &#125; else &#123; ResponseUtil.response(ctx, request, action.call()); &#125;&#125; else &#123; //错误处理 generalResponse = new GeneralResponse(HttpResponseStatus.BAD_REQUEST, "请检查你的请求方法及url", null); ResponseUtil.response(ctx, request, generalResponse);&#125; DemoController 方法配置：@RequestMapping(uri = &quot;/login&quot;, method = &quot;POST&quot;)public GeneralResponse login(FullHttpRequest request) &#123; User user = JsonUtil.fromJson(request, User.class); log.info(&quot;/login called,user: &#123;&#125;&quot;, user); return new GeneralResponse(null);&#125; 测试结果如下： 完整代码在 https://github.com/morethink/Netty-Route]]></content>
      <categories>
        <category>Java</category>
      </categories>
      <tags>
        <tag>Netty</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Java使用UDP发送数据到InfluxDB]]></title>
    <url>%2FJava-USES-InfluxDB-UDP-sending-data%2F</url>
    <content type="text"><![CDATA[最近在做压测引擎相关的开发，需要将聚合数据发送到InfluxDB保存以便实时分析和控制QPS。 下面介绍对InfluxDB的使用。 什么是InfluxDBInfluxDB是一款用Go语言编写的开源分布式时序、事件和指标数据库，无需外部依赖。该数据库现在主要用于存储涉及大量的时间戳数据，如DevOps监控数据，APP metrics, loT传感器数据和实时分析数据。 InfluxDB特征： 无结构(无模式)：可以是任意数量的列(tags)。 可以设置metric的保存时间。 支持与时间有关的相关函数(如min、max、sum、count、mean、median等)，方便统计。 支持存储策略：可以用于数据的删改(influxDB没有提供数据的删除与修改方法)。 支持连续查询：是数据库中自动定时启动的一组语句，和存储策略搭配可以降低InfluxDB的系统占用量。 原生的HTTP支持，内置HTTP API。 支持类似SQL语法。 支持设置数据在集群中的副本数。 支持定期采样数据，写入另外的measurement，方便分粒度存储数据。 自带web管理界面，方便使用(登入方式：http://&lt; InfluxDB-IP &gt;:8083)。 支持Grafana画图展示。 PS：有了InfluxDB+Grafana后，你就可以写一些简单的程序了，可以只负责写后端逻辑部分，数据都可以存入InfluxDB，然后通过Grafana展示出来。 Mac安装InfluxDB# 安装brew install influxdb# 启动influxd -config /usr/local/etc/influxdb.conf# 查看influxdb运行配置influxd config# 启动客户端influx -precision rfc3339 InfluxDB开启UDP配置vim /usr/local/etc/influxdb.conf 开启udp配置，其他为默认值[[udp]] enabled = true udp配置含义：[[udp]] – udp配置 enabled：是否启用该模块，默认值：false。 bind-address：绑定地址，默认值：”:8089″。 database：数据库名称，默认值：”udp”。 retention-policy：存储策略，无默认值。 batch-size：默认值：5000。 batch-pending：默认值：10。 read-buffer：udp读取buffer的大小，0表示使用操作系统提供的值，如果超过操作系统的默认配置则会出错。 该配置的默认值：0。 batch-timeout：超时时间，默认值：”1s”。 precision：时间精度，无默认值。 Java发送UDP数据报我们知道InfluxDB是支持Http的，为什么我们还要采用UDP方式发送数据呢？ 基于下列原因： TCP数据传输慢，UDP数据传输快。 网络带宽需求较小，而实时性要求高。 InfluxDB和服务器在同机房，发生数据丢包的可能性较小，即使真的发生丢包，对整个请求流量的收集影响也较小。 我们采用了worker线程调用addMetric方法将数据存储到缓存 map 中，send线程池来进行每个指定时间发送数据到Influxdb。 代码如下(也可参考Jmeter的UdpMetricsSender类)：@Slf4jpublic class InfluxDBClient implements Runnable &#123; private String measurement = "example"; private final Object lock = new Object(); private InetAddress hostAddress; private int udpPort; private volatile Map&lt;String, List&lt;Response&gt;&gt; metrics = new HashMap&lt;&gt;(); private long time; private String transaction; public InfluxDBClient(String influxdbUrl, String transaction) &#123; this.transaction = transaction; try &#123; log.debug("Setting up with url:&#123;&#125;", influxdbUrl); String[] urlComponents = influxdbUrl.split(":"); if (urlComponents.length == 2) &#123; hostAddress = InetAddress.getByName(urlComponents[0]); udpPort = Integer.parseInt(urlComponents[1]); &#125; else &#123; throw new IllegalArgumentException("InfluxDBClient url '" + influxdbUrl + "' is wrong. The format shoule be &lt;host/ip&gt;:&lt;port&gt;"); &#125; &#125; catch (Exception e) &#123; throw new IllegalArgumentException("InfluxDBClient url '" + influxdbUrl + "' is wrong. The format shoule be &lt;host/ip&gt;:&lt;port&gt;", e); &#125; &#125; public void addMetric(Response response) &#123; synchronized (lock) &#123; if (metrics.containsKey(response.getLabel())) &#123; metrics.get(response.getLabel()).add(response); &#125; else &#123; metrics.put(response.getLabel(), new ArrayList&lt;&gt;(Collections.singletonList(response))); &#125; &#125; &#125; @Override public void run() &#123; sendMetrics(); &#125; private void sendMetrics() &#123; Map&lt;String, List&lt;Response&gt;&gt; tempMetrics; //复制数据到tempMetrics，清空原来metrics并初始化上次的大小 synchronized (lock) &#123; if (isEmpty(metrics)) &#123; return; &#125; time = System.currentTimeMillis(); tempMetrics = metrics; metrics = new HashMap&lt;&gt;(); for (Map.Entry&lt;String, List&lt;Response&gt;&gt; entry : tempMetrics.entrySet()) &#123; metrics.put(entry.getKey(), new ArrayList&lt;&gt;(entry.getValue().size())); &#125; &#125; final Map&lt;String, List&lt;Response&gt;&gt; copyMetrics = tempMetrics; final List&lt;MetricTuple&gt; aggregateMetrics = aggregate(copyMetrics); StringBuilder sb = new StringBuilder(aggregateMetrics.size() * 200); //发送tempMetrics,生成一行数据，然后换行 for (MetricTuple metric : aggregateMetrics) &#123; sb.append(metric.getMeasurement()).append(metric.getTag()).append(" ") .append(metric.getField()).append(" ").append(metric.getTimestamp() + "000000").append("\n"); &#125; //udp发送数据到Influxdb try (DatagramSocket ds = new DatagramSocket()) &#123; byte[] buf = sb.toString().getBytes(); DatagramPacket dp = new DatagramPacket(buf, buf.length, this.hostAddress, this.udpPort); ds.send(dp); log.debug("send &#123;&#125; to influxdb", sb.toString()); &#125; catch (SocketException e) &#123; log.error("Cannot open udp port!", e); &#125; catch (IOException e) &#123; log.error("Error in transferring udp package", e); &#125; &#125; /** * 得到聚合数据 * * @param metrics * @return */ private List&lt;MetricTuple&gt; aggregate(Map&lt;String, List&lt;Response&gt;&gt; metrics) &#123; &#125; public boolean isEmpty(Map&lt;String, List&lt;Response&gt;&gt; map) &#123; for (Map.Entry&lt;String, List&lt;Response&gt;&gt; entry : map.entrySet()) &#123; if (!entry.getValue().isEmpty()) &#123; return false; &#125; &#125; return true; &#125;&#125; 参考文档： InfluxDB中文文档 玩转时序数据库InfluxDB]]></content>
      <categories>
        <category>数据库</category>
      </categories>
      <tags>
        <tag>InfluxDB</tag>
        <tag>UDP</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Java压缩/解压 .zip、.tar.gz、.tar.bz2(支持中文)]]></title>
    <url>%2FJava%E5%8E%8B%E7%BC%A9%2F%E8%A7%A3%E5%8E%8B%20.zip%E3%80%81.tar.gz%E3%80%81.tar.bz2(%E6%94%AF%E6%8C%81%E4%B8%AD%E6%96%87)%2F</url>
    <content type="text"><![CDATA[本文介绍Java压缩/解压.zip、.tar.gz、.tar.bz2的方式。 对于zip文件：使用java.util.zip.ZipEntry 和 java.util.zip.ZipFile，通过设置Charset为StandardCharsets.UTF_8支持中文。 对于.tar.gz、tgz文件：可以看做先用tar打包，再使用gz进行压缩。使用commons-compress包的TarArchiveInputStream和GzipCompressorInputStream。 对于.tar.bz2文件：可以看做先用tar打包，再使用bz2进行压缩。使用commons-compress包的TarArchiveInputStream和BZip2CompressorInputStream。 在这里有个问题如果使用TarInputStream搭配jdk的 GZIPInputStream会产生乱码。而使用commons-compress包的TarArchiveInputStream和GzipCompressorInputStream则可解决乱码问题。 代码如下： public class ZipUtil &#123; private static final Logger LOG = LoggerFactory.getLogger(ZipUtil.class); private static final int BUFFER_SIZE = 1024 * 100; private ZipUtil() &#123; &#125; /** * 私有函数将文件集合压缩成tar包后返回 * * @param files 要压缩的文件集合 * @param target tar 输出流的目标文件 * @return File 指定返回的目标文件 */ private static File pack(List&lt;File&gt; files, File target) throws IOException&#123; try (FileOutputStream fos = new FileOutputStream(target)) &#123; try (BufferedOutputStream bos = new BufferedOutputStream(fos, BUFFER_SIZE)) &#123; try (TarArchiveOutputStream taos = new TarArchiveOutputStream(bos)) &#123; //解决文件名过长问题 taos.setLongFileMode(TarArchiveOutputStream.LONGFILE_GNU); for (File file : files) &#123; taos.putArchiveEntry(new TarArchiveEntry(file)); try (FileInputStream fis = new FileInputStream(file)) &#123; IOUtils.copy(fis, taos); taos.closeArchiveEntry(); &#125; &#125; &#125; &#125; &#125; return target; &#125; /** * 将tar压缩成tar.gz文件 * * @param list * @param outPutPath * @param fileName */ public static File compress(List&lt;File&gt; list, String outPutPath, String fileName) throws IOException &#123; File outPutFile = new File(outPutPath + File.separator + fileName + ".tar.gz"); File tempTar = new File("temp.tar"); try (FileInputStream fis = new FileInputStream(pack(list, tempTar))) &#123; try (BufferedInputStream bis = new BufferedInputStream(fis, BUFFER_SIZE)) &#123; try (FileOutputStream fos = new FileOutputStream(outPutFile)) &#123; try (GZIPOutputStream gzp = new GZIPOutputStream(fos)) &#123; int count; byte[] data = new byte[BUFFER_SIZE]; while ((count = bis.read(data, 0, BUFFER_SIZE)) != -1) &#123; gzp.write(data, 0, count); &#125; &#125; &#125; &#125; &#125; try &#123; Files.deleteIfExists(tempTar.toPath()); &#125; catch (IOException e) &#123; e.printStackTrace(); &#125; return outPutFile; &#125; public static boolean decompress(String filePath, String outputDir, boolean isDeleted) &#123; File file = new File(filePath); if (!file.exists()) &#123; LOG.error("decompress file not exist."); return false; &#125; try &#123; if (filePath.endsWith(".zip")) &#123; unZip(file, outputDir); &#125; if (filePath.endsWith(".tar.gz") || filePath.endsWith(".tgz")) &#123; decompressTarGz(file, outputDir); &#125; if (filePath.endsWith(".tar.bz2")) &#123; decompressTarBz2(file, outputDir); &#125; filterFile(new File(outputDir)); if (isDeleted) &#123; FileUtils.deleteQuietly(file); &#125; return true; &#125; catch (IOException e) &#123; LOG.error("decompress occur error."); &#125; return false; &#125; /** * 解压 .zip 文件 * * @param file 要解压的zip文件对象 * @param outputDir 要解压到某个指定的目录下 * @throws IOException */ public static void unZip(File file, String outputDir) throws IOException &#123; try (ZipFile zipFile = new ZipFile(file, StandardCharsets.UTF_8)) &#123; //创建输出目录 createDirectory(outputDir, null); Enumeration&lt;?&gt; enums = zipFile.entries(); while (enums.hasMoreElements()) &#123; ZipEntry entry = (ZipEntry) enums.nextElement(); if (entry.isDirectory()) &#123; //创建空目录 createDirectory(outputDir, entry.getName()); &#125; else &#123; try (InputStream in = zipFile.getInputStream(entry)) &#123; try (OutputStream out = new FileOutputStream( new File(outputDir + File.separator + entry.getName()))) &#123; writeFile(in, out); &#125; &#125; &#125; &#125; &#125; &#125; public static void decompressTarGz(File file, String outputDir) throws IOException &#123; try (TarArchiveInputStream tarIn = new TarArchiveInputStream( new GzipCompressorInputStream( new BufferedInputStream( new FileInputStream(file))))) &#123; //创建输出目录 createDirectory(outputDir, null); TarArchiveEntry entry = null; while ((entry = tarIn.getNextTarEntry()) != null) &#123; //是目录 if (entry.isDirectory()) &#123; //创建空目录 createDirectory(outputDir, entry.getName()); &#125; else &#123; //是文件 try (OutputStream out = new FileOutputStream( new File(outputDir + File.separator + entry.getName()))) &#123; writeFile(tarIn, out); &#125; &#125; &#125; &#125; &#125; /** * 解压缩tar.bz2文件 * * @param file 压缩包文件 * @param outputDir 目标文件夹 */ public static void decompressTarBz2(File file, String outputDir) throws IOException &#123; try (TarArchiveInputStream tarIn = new TarArchiveInputStream( new BZip2CompressorInputStream( new FileInputStream(file)))) &#123; createDirectory(outputDir, null); TarArchiveEntry entry; while ((entry = tarIn.getNextTarEntry()) != null) &#123; if (entry.isDirectory()) &#123; createDirectory(outputDir, entry.getName()); &#125; else &#123; try (OutputStream out = new FileOutputStream( new File(outputDir + File.separator + entry.getName()))) &#123; writeFile(tarIn, out); &#125; &#125; &#125; &#125; &#125; /** * 写文件 * * @param in * @param out * @throws IOException */ public static void writeFile(InputStream in, OutputStream out) throws IOException &#123; int length; byte[] b = new byte[BUFFER_SIZE]; while ((length = in.read(b)) != -1) &#123; out.write(b, 0, length); &#125; &#125; /** * 创建目录 * * @param outputDir * @param subDir */ public static void createDirectory(String outputDir, String subDir) &#123; File file = new File(outputDir); //子目录不为空 if (!(subDir == null || subDir.trim().equals(""))) &#123; file = new File(outputDir + File.separator + subDir); &#125; if (!file.exists()) &#123; if (!file.getParentFile().exists()) &#123; file.getParentFile().mkdirs(); &#125; file.mkdirs(); &#125; &#125; /** * 删除Mac压缩再解压产生的 __MACOSX 文件夹和 .开头的其他文件 * * @param filteredFile */ public static void filterFile(File filteredFile) &#123; if (filteredFile != null) &#123; File[] files = filteredFile.listFiles(); for (File file : files) &#123; if (file.getName().startsWith(".") || (file.isDirectory() &amp;&amp; file.getName().equals("__MACOSX"))) &#123; FileUtils.deleteQuietly(file); &#125; &#125; &#125; &#125;&#125; 代码地址： https://github.com/morethink/code/blob/master/java/tools/src/main/java/cn/morethink/tools/util/ZipUtil.java 参考文档： commons-compress官方实例]]></content>
      <categories>
        <category>工具</category>
      </categories>
  </entry>
  <entry>
    <title><![CDATA[Git为某个域名设置代理]]></title>
    <url>%2FGit-set-proxy-for-a-domain%2F</url>
    <content type="text"><![CDATA[国内访问Github很慢，可以通过配置代理来加快访问速度，但是对公司内部git服务器却不能使用代理。 下面通过更改Git配置文件对不同的域名使用不同的代理配置。 打开Git 配置文件 vi ~/.gitconfig 添加如下配置： [http "https://github.com/"] proxy = http://127.0.0.1:1081[https "https://github.com/"] proxy = http://127.0.0.1:1081[http "https://my.comapnyserver.com/"] proxy = ""]]></content>
      <categories>
        <category>工具</category>
      </categories>
      <tags>
        <tag>Git</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[MongoDB 及 Mysql 背后的 B/B+树]]></title>
    <url>%2Fdata-structure-behind-the-index%2F</url>
    <content type="text"><![CDATA[索引是数据库常见的数据结构，每个后台开发人员都应该对索引背后的数据结构有所了解。 本文通过分析B-Tree及B-/+Tree数据结构及索引性能分析及磁盘存取原理尝试着回答一下问题： 为什么B-Tree适合数据库索引及红黑树的二叉平衡树不适合作为索引 B+Tree比BTree做索引的优势 为什么MongoDB采用B-Tree作为索引结构而MySQL采用B+Tree作为索引存储结构 B-TreeB 树（B-Tree）是为磁盘等辅助存取设备设计的一种平衡查找树，它实现了以 $O(\lg n)$ 时间复杂度执行查找、顺序读取、插入和删除操作。由于 B 树和 B 树的变种在降低磁盘 I/O 操作次数方面表现优异，所以经常用于设计文件系统和数据库。 使用阶来定义 B 树，一棵 m 阶的 B 树，需要满足下列条件： 每个节点最多拥有m个子节点且m&gt;=2，空树除外 除根节点外每个节点的关键字数量大于等于ceil(m/2)-1，小于等于m-1，非根节点关键字数必须&gt;=2 所有叶子节点均在同一层、叶子节点除了包含了关键字和关键字记录的指针外也有指向其子节点的指针只不过其指针地址都为null对应下图最后一层节点的空格子 如果一个非叶节点有n个子节点，则该节点的关键字数等于n-1 所有节点关键字是按递增次序排列，并遵循左小右大原则 注： m阶代表一个树节点最多有多少个查找路径，m阶=m路,当m=2则是2叉树，m=3则是3叉。 ceil()是个朝正无穷方向取整的函数，如ceil(1.1)结果为2，即向上取整。 B 树中的节点分为内部节点（Internal Node）和叶节点（Leaf Node），内部节点也就是非叶节点（Non-Leaf Node）。 B-Tree的查找B-Tree的查找过程：根据给定值查找结点和在结点的关键字中进行查找交叉进行。 首先从根结点开始重复如下过程：若比结点的第一个关键字小，则查找在该结点第一个指针指向的结点进行；若等于结点中某个关键字，则查找成功；若在两个关键字之间，则查找在它们之间的指针指向的结点进行；若比该结点所有关键字大，则查找在该结点最后一个指针指向的结点进行；若查找已经到达某个叶结点，则说明给定值对应的数据记录不存在，查找失败。 例如：在一棵 5 阶B-树中查找元素 29 首先29比根节点值大，所以找根节点的右子数，然后再根据值得判断，发现 29 介于 28 和 48 之间，然后在从中间子树继续查找下去。 B-Tree的插入插入的过程分两步完成： 利用前述的B-树的查找算法查找关键字的插入位置。若找到，则说明该关键字已经存在，直接返回。否则查找操作必失败于某个最低层的非终端结点上。 判断该结点是否还有空位置。即判断该结点的关键字总数是否满足n&lt;=m-1。若满足，则说明该结点还有空位置，直接把关键字k插入到该结点的合适位置上。若不满足，说明该结点己没有空位置，需要把结点分裂成两个。 分裂的方法是：生成一新结点。把原结点上的关键字和k按升序排序后，从中间位置把关键字（不包括中间位置的关键字）分成两部分。左部分所含关键字放在旧结点中，右部分所含关键字放在新结点中，中间位置的关键字连同新结点的存储位置插入到父结点中。如果父结点的关键字个数也超过（m-1），则要再分裂，再往上插。直至这个过程传到根结点为止。 例子： 如果该节点的元素个数还没达到 m，则插入完后无需处理比如： 如果该节点元素个数达到 m 时，这时候将元素插入到合适的位置，将最中间的元素取出，成为该节点的父节点元素，然后将其余左右元素拆成两个新节点 比如： 刚才的操作可能导致父节点的元素个数达到 m，这时候用情况 2 迭代处理，直到如果遇到根结点元素个数达到 m，则最中间元素将成为新的根结点。 比如： B-Tree 的删除我们需要分两种情况进行讨论： 如果该元素存在于叶子结点，直接删除它，无需进行其它处理。 如果该元素存在于非叶子节点，那么删除它将会留下一个空位，这时候我们需要一些处理来填充该位置。因为节点的元素个数在 [M/2, M] 的范围内，所以比如这里我们以 5 阶B-树为例，判断节点元素是否充足即满足个数则至少拥有三（2 + 1）个元素的节点才算是有充足的元素。 如果被删元素的左子树拥有足够的元素，这时候我们只需拿左子节点的最大值元素上来填充即可 当左子树不够元素而右子树元素充足时，这时候我们拿右子树的最小值元素上来进行填充 当左右子树所含元素均不足时，但左子树的左边兄弟节点的元素个数充足，这时我们需要拿左边的兄弟节点来进行调整。 当左右子树所含元素均不足时，但左子树的左边兄弟节点的元素个数也不足时，这时候我们还是拿左子树的最大值元素进行填充，之后再将该节点与其他节点合并形成新的节点。 B+TreeB-Tree有许多变种，其中最常见的是B+Tree，例如MySQL就普遍使用B+Tree实现其索引结构。 与B-Tree相比，B+Tree有以下不同点： 每个节点的指针上限为2d而不是2d+1。 B+Tree叶子节点保存了父节点的所有关键字和关键字记录的指针，每个叶子节点的关键字从小到大链接 内节点不存储data，只存储key；叶子节点不存储指针。因此所有数据地址必须要到叶子节点才能获取到，所以每次数据查询的次数都一样。 索引红黑树等数据结构也可以用来实现索引，但是文件系统及数据库系统普遍采用B-/+Tree作为索引结构，这一节将结合计算机组成原理相关知识讨论B-/+Tree作为索引的理论基础。 一般来说，索引本身也很大，不可能全部存储在内存中，因此索引往往以索引文件的形式存储的磁盘上。这样的话，索引查找过程中就要产生磁盘I/O消耗，相对于内存存取，I/O存取的消耗要高几个数量级，所以评价一个数据结构作为索引的优劣最重要的指标就是在查找过程中磁盘I/O操作次数的渐进复杂度。换句话说，索引的结构组织要尽量减少查找过程中磁盘I/O的存取次数。 下面先介绍内存和磁盘存取原理，然后再结合这些原理分析B-/+Tree作为索引的效率。 磁盘存取原理索引一般以文件形式存储在磁盘上，索引检索需要磁盘I/O操作。与主存不同，磁盘I/O存在机械运动耗费，因此磁盘I/O的时间消耗是巨大的。 下面是磁盘的整体结构示意图： 一个磁盘由大小相同且同轴的圆形盘片组成，磁盘可以转动（各个磁盘必须同步转动）。在磁盘的一侧有磁头支架，磁头支架固定了一组磁头，每个磁头负责存取一个磁盘的内容。磁头不能转动，但是可以沿磁盘半径方向运动（实际是斜切向运动），每个磁头同一时刻也必须是同轴的，即从正上方向下看，所有磁头任何时候都是重叠的（不过目前已经有多磁头独立技术，可不受此限制）。 下面是磁盘结构的示意图： 盘片被划分成一系列同心环，圆心是盘片中心，每个同心环叫做一个磁道，所有半径相同的磁道组成一个柱面。磁道被沿半径线划分成一个个小的段，每个段叫做一个扇区，每个扇区是磁盘的最小存储单元。为了简单起见，我们下面假设磁盘只有一个盘片和一个磁头。 当需要从磁盘读取数据时，系统会将数据逻辑地址传给磁盘，磁盘的控制电路按照寻址逻辑将逻辑地址翻译成物理地址，即确定要读的数据在哪个磁道，哪个扇区。为了读取这个扇区的数据，需要将磁头放到这个扇区上方，为了实现这一点，磁头需要移动对准相应磁道，这个过程叫做寻道，所耗费时间叫做寻道时间，然后磁盘旋转将目标扇区旋转到磁头下，这个过程耗费的时间叫做旋转时间。 局部性原理与磁盘预读由于存储介质的特性，磁盘本身存取就比主存慢很多，再加上机械运动耗费，磁盘的存取速度往往是主存的几百分分之一，因此为了提高效率，要尽量减少磁盘I/O。为了达到这个目的，磁盘往往不是严格按需读取，而是每次都会预读，即使只需要一个字节，磁盘也会从这个位置开始，顺序向后读取一定长度的数据放入内存。 这样做的理论依据是计算机科学中著名的局部性原理： 当一个数据被用到时，其附近的数据也通常会马上被使用。 程序运行期间所需要的数据通常比较集中。 由于磁盘顺序读取的效率很高（不需要寻道时间，只需很少的旋转时间），因此对于具有局部性的程序来说，预读可以提高I/O效率。 预读的长度一般为页（page）的整倍数。页是计算机管理存储器的逻辑块，硬件及操作系统往往将主存和磁盘存储区分割为连续的大小相等的块，每个存储块称为一页（在许多操作系统中，页得大小通常为4k），主存和磁盘以页为单位交换数据。当程序要读取的数据不在主存中时，会触发一个缺页异常，此时系统会向磁盘发出读盘信号，磁盘会找到数据的起始位置并向后连续读取一页或几页载入内存中，然后异常返回，程序继续运行。 B-/+Tree索引的性能分析一般使用磁盘I/O次数评价索引结构的优劣。先从B-Tree分析，根据B-Tree的定义，可知检索一次最多需要访问h个节点。数据库系统的设计者巧妙利用了磁盘预读原理，将一个节点的大小设为等于一个页，这样每个节点只需要一次I/O就可以完全载入。为了达到这个目的，在实际实现B-Tree还需要使用如下技巧： 每次新建节点时，直接申请一个页的空间，这样就保证一个节点物理上也存储在一个页里，加之计算机存储分配都是按页对齐的，就实现了一个node只需一次I/O。 B-Tree中一次检索最多需要h-1次I/O（根节点常驻内存），渐进复杂度为$O(h)=O(\log_d N)$。一般实际应用中，出度d是非常大的数字，通常超过100，因此h非常小（通常不超过3）。 综上所述，用B-Tree作为索引结构效率是非常高的。 而红黑树这种结构，h明显要深的多。由于逻辑上很近的节点（父子）物理上可能很远，无法利用局部性，所以红黑树的I/O渐进复杂度也为O(h)，效率明显比B-Tree差很多。 上文还说过，B+Tree更适合外存索引，原因和内节点出度d有关。从上面分析可以看到，d越大索引的性能越好，而出度的上限取决于节点内key和data的大小： $$d_{max}=floor({pagesize \over keysize+datasize+pointsize})$$ floor表示向下取整。 由于B+Tree内节点去掉了data域，因此可以拥有更大的出度，容纳更多的节点，能够有效减少磁盘IO次数。 一般在数据库系统或文件系统中使用的B+Tree结构都在经典B+Tree的基础上进行了优化，增加了顺序访问指针。 如上图图所示，在B+Tree的每个叶子节点增加一个指向相邻叶子节点的指针，就形成了带有顺序访问指针的B+Tree。做这个优化的目的是为了提高区间访问的性能，例如图4中如果要查询key为从18到49的所有数据记录，当找到18后，只需顺着节点和指针顺序遍历就可以一次性访问到所有数据节点，极大提到了区间查询效率。 综上所述：B+Tree做索引的优势是： 内部节点取消data域，每一页可以容纳更多的数据，有效减少磁盘IO次数。 数据都存储在叶子节点，所以任何关键字的查找必须走一条从根结点到叶子结点的路。所有关键字查询的路径长度相同，导致每一个数据的查询效率相当。所以B+树查询时间复杂度为log n，而B树查询时间复杂度不固定，与所查结点在树中的位置有关，最好为O(1)。 通过增加顺序访问指针提高区间查询效率。 而MongoDB索引选择B树可能是因为：MongoDB 是文档型的数据库，是一种nosql，它使用BSON格式保存数据，归属于聚合型数据库。被设计用在数据模型简单，性能要求高的场合。之所以采用B树，是因为B树key和data域聚合在一起。因此并不需要类似于区间查询的操作。 参考文档： MySQL索引背后的数据结构及算法原理 人人都是 DBA（VII）B 树和 B+ 树 平衡二叉树、B-Tree、B+Tree、B*树 理解其中一种你就都明白了 https://zh.wikipedia.org/wiki/B%E6%A0%91 B-Tree gif：https://www.cs.usfca.edu/~galles/visualization/BTree.html 6. 数据结构 - B 树]]></content>
      <categories>
        <category>算法</category>
      </categories>
      <tags>
        <tag>索引</tag>
        <tag>树</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[MAC安装RabbitMQ]]></title>
    <url>%2FMAC%E5%AE%89%E8%A3%85RabbitMQ%2F</url>
    <content type="text"><![CDATA[安装brew updatebrew install rabbitmq 配置 添加环境变量 打开配置文件$ vi ~/.bash_profile 添加 export PATH=$PATH:/usr/local/sbin到末尾，编辑完后:wq保存退出。 使环境变量立即生效 $ source ~/.bash_profile 启动RabbitMQ服务上面配置完成后，需要关闭终端窗口，重新打开，然后输入下面命令即可启动RabbitMQ服务：rabbitmq-server 登录Web管理界面浏览器输入localhost：15672，账号密码全输入guest即可登录。 这里需要注意下，从3.3.1版本开始，RabbitMQ默认不允许远程ip登录，即只能使用localhost登录。如果希望远程登录，需要添加用户权限。 设置RabbitMQ远程ip登录由于账号guest具有所有的操作权限，并且又是默认账号，出于安全因素的考虑，guest用户只能通过localhost登陆使用，并建议修改guest用户的密码以及新建其他账号管理使用rabbitmq。这里我们以创建个test帐号，密码123456为例，创建一个账号并支持远程ip访问。 创建账号 rabbitmqctl add_user test 123456 设置用户角色 rabbitmqctl set_user_tags test administrator 设置用户权限 rabbitmqctl set_permissions -p &quot;/&quot; test &quot;.*&quot; &quot;.*&quot; &quot;.*&quot; 设置完成后可以查看当前用户和角色(需要开启服务) rabbitmqctl list_users 这是你就可以通过其他主机的访问RabbitMQ的Web管理界面了，访问方式，浏览器输入：serverip:15672。其中serverip是RabbitMQ-Server所在主机的ip。 RabbitMQ常用操作 用户管理用户管理包括增加用户，删除用户，查看用户列表，修改用户密码。 新增一个用户 rabbitmqctl add_user Username Password 删除一个用户 rabbitmqctl delete_user Username 修改用户的密码 rabbitmqctl change_password Username Newpassword 查看当前用户列表 rabbitmqctl list_users]]></content>
      <categories>
        <category>工具</category>
      </categories>
      <tags>
        <tag>MQ</tag>
        <tag>MAC</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[科学上网之在VPS上安装SSR]]></title>
    <url>%2F%E7%A7%91%E5%AD%A6%E4%B8%8A%E7%BD%91%E4%B9%8B%E5%9C%A8VPS%E4%B8%8A%E5%AE%89%E8%A3%85SSR%2F</url>
    <content type="text"><![CDATA[本文对VPS安装SSR的过程进行总结，免得因为XX重新安装SSR时找不到安装方法。 安装SSR简单的来说，如果你什么都不懂，那么你直接一路回车就可以了！本脚本需要Linux root账户权限才能正常安装运行，所以 如果不是 root账号，请先切换为root，如果是 root账号，那么请跳过！ sudo su 输入上面代码回车后会提示你输入当前用户的密码，输入并回车后，没有报错就继续下面的步骤安装ShadowsocksR。 v2.0.0 版本以后的脚本，请先卸载旧脚本ShadowsocksR服务端，再重新安装！ wget -N --no-check-certificate https://softs.fun/Bash/ssr.sh &amp;&amp; chmod +x ssr.sh &amp;&amp; bash ssr.sh 备用下载地址（上面的链接无法下载，就用这个）：wget -N --no-check-certificate https://raw.githubusercontent.com/ToyoDAdoubi/doubi/master/ssr.sh &amp;&amp; chmod +x ssr.sh &amp;&amp; bash ssr.sh 下载运行后会提示你输入数字来选择要做什么。 运行脚本： bash ssr.sh 输入对应的数字来执行相应的命令。 请输入一个数字来选择菜单选项 1. 安装 ShadowsocksR 2. 更新 ShadowsocksR 3. 卸载 ShadowsocksR 4. 安装 libsodium(chacha20)———————————— 5. 查看 账号信息 6. 显示 连接信息 7. 设置 用户配置 8. 手动 修改配置 9. 切换 端口模式———————————— 10. 启动 ShadowsocksR 11. 停止 ShadowsocksR 12. 重启 ShadowsocksR 13. 查看 ShadowsocksR 日志———————————— 14. 其他功能 15. 升级脚本 当前状态: 已安装 并 已启动 当前模式: 单端口请输入数字(1-15)： 文件位置 安装目录：/usr/local/shadowsocksr 配置文件：/etc/shadowsocksr/user-config.json 设置为系统服务ShadowsocksR 安装后，自动设置为 系统服务，所以支持使用服务来启动/停止等操作，同时支持开机启动。 启动 ShadowsocksR：/etc/init.d/ssr start 停止 ShadowsocksR：/etc/init.d/ssr stop 重启 ShadowsocksR：/etc/init.d/ssr restart 查看 ShadowsocksR状态：/etc/init.d/ssr status ShadowsocksR 默认支持UDP转发，服务端无需任何设置。 定时重启一些人可能需要定时重启ShadowsocksR服务端来保证稳定性等，所以这里用 crontab 定时。 # 输入这个命令可以查看当前配置的定时任务crontab -l# 如果提示命令不存在，下面是安装命令# CentOS系统：yum updateyum install -y crond# Debian/Ubuntu系统：apt-get updateapt-get install -y cron 安装 crontab 后，我们就能开始添加定时任务了： crontab -l &gt; &quot;crontab.bak&quot;sed -i &quot;/ssr restart/d&quot; &quot;crontab.bak&quot;echo -e &quot;\n10 3 * * * /etc/init.d/ssr restart&quot; &gt;&gt; &quot;crontab.bak&quot;crontab &quot;crontab.bak&quot;rm -r &quot;crontab.bak&quot; 下面是定时任务规则(代码前面的 * 分别对应：分钟 小时 日 月 星期)参考： 10 2 * * * /etc/init.d/ssr restart# 这个代表 每天2点10分重启一次 ShadowsocksR10 2 */2 * * /etc/init.d/ssr restart# 这个代表 每隔2天的2点10分重启一次 ShadowsocksR10 */4 * * * /etc/init.d/ssr restart# 这个代表 每隔4小时的第10分重启一次 ShadowsocksR BBR和锐速BBR和锐速都是用来提高翻墙速度的。 BBR是来自于Google的黑科技，目的是通过优化和控制TCP的拥塞，充分利用带宽并降低延迟，其目的就是要尽量跑满带宽，并且尽量不要有排队的情况。BBR 这个特性其实是在 Linux 内核 4.9 才计划加入的。所以，要开启BBR，需要内核版本在Linux kernel 4.9以上，脚本会帮助我们安装。 在BBR之前，比较有名的就是国产的锐速了，不过，由于锐速是个国产的闭源软件，可能存在安全性问题，因此 推荐使用安装BBR。 启动脚本： bash ssr.sh 选择14. 其他功能 1. 配置 BBR 2. 配置 锐速(ServerSpeeder) 3. 配置 LotServer(锐速母公司) 注意： 锐速/LotServer/BBR 不支持 OpenVZ！ 注意： 锐速/LotServer/BBR 不能共存！———————————— 4. 一键封禁 BT/PT/SPAM (iptables) 5. 一键解封 BT/PT/SPAM (iptables) 6. 切换 ShadowsocksR日志输出模式 ——说明：SSR默认只输出错误日志，此项可切换为输出详细的访问日志 安装BBR或者锐速(推荐BBR) TCP优化 增加TCP连接数量 vi /etc/security/limits.conf 添加两行： * soft nofile 51200* hard nofile 51200 设置ulimit： ulimit -n 51200 添加一些优化内容 修改sysctl.conf vi /etc/sysctl.conf 插入代码： #TCP配置优化(不然你自己根本不知道你在干什么)fs.file-max = 51200#提高整个系统的文件限制net.core.rmem_max = 67108864net.core.wmem_max = 67108864net.core.netdev_max_backlog = 250000net.core.somaxconn = 4096net.ipv4.tcp_syncookies = 1net.ipv4.tcp_tw_reuse = 1net.ipv4.tcp_tw_recycle = 0net.ipv4.tcp_fin_timeout = 30net.ipv4.tcp_keepalive_time = 1200net.ipv4.ip_local_port_range = 10000 65000net.ipv4.tcp_max_syn_backlog = 8192net.ipv4.tcp_max_tw_buckets = 5000net.ipv4.tcp_fastopen = 3net.ipv4.tcp_mem = 25600 51200 102400net.ipv4.tcp_rmem = 4096 87380 67108864net.ipv4.tcp_wmem = 4096 65536 67108864net.ipv4.tcp_mtu_probing = 1net.ipv4.tcp_congestion_control = bbr#END OF LINE 应用 sysctl -p 重启SSR /etc/init.d/ssr restart 参考文档： 科学上网教程（一）——VPS上搭建SSR 『原创』CentOS/Debian/Ubuntu ShadowsocksR 单/多端口 一键管理脚本]]></content>
      <categories>
        <category>工具</category>
      </categories>
  </entry>
  <entry>
    <title><![CDATA[给你的博客园图片添加标题]]></title>
    <url>%2F%E7%BB%99%E4%BD%A0%E7%9A%84%E5%8D%9A%E5%AE%A2%E5%9B%AD%E5%9B%BE%E7%89%87%E6%B7%BB%E5%8A%A0%E6%A0%87%E9%A2%98%2F</url>
    <content type="text"><![CDATA[一直以来都觉得在图片下面添加一个标题可以更加清晰的表示这张图片的含义，可是博客园原生并不支持这种渲染方式，再加上博客园可以自己写js来更改主题，于是通过搜索资料完成给博客园图片添加标题的功能。 当我们如下书写markdown时：![](https://images.morethink.cn/092017231747399.jpg "TCP的三次握手和四次挥手") 会被博客园渲染成&lt;p&gt;&lt;img src="https://images.morethink.cn/092017231747399.jpg" title="TCP的三次握手和四次挥手"&gt;&lt;/p&gt; 于是我就想通过在img标签后面动态添加一个带有title的p标签来给博客园图片添加标题。 将下面代码放入页首Html代码代码中即可(需要申请js权限)。 &lt;!-- 引入jQuery --&gt;&lt;script src="http://cdn.bootcss.com/jquery/1.10.2/jquery.min.js"&gt;&lt;/script&gt;&lt;script type="text/javascript"&gt; $(window).load(function () &#123; //给每张图片添加标题,div.cnblogs_post_body是博客主体 $("div[id=cnblogs_post_body] img").each(function () &#123; var title = $(this).attr("title"); if (title != undefined) &#123; var boardp_style = "style='display: block; text-align: center; color: #969696;padding: 10px;border-bottom: 1px solid #d9d9d9;margin: 0 auto;" + "width: " + ($(this).width() * 0.8) + "px;" + "height: 28px;" + "'&gt;"; var boardp = "&lt;p " + boardp_style + title + "&lt;/p"; $(this).after(boardp); &#125; &#125;); &#125;);&lt;/script&gt;&lt;!-- 将img变为块级元素 --&gt;&lt;style type="text/css"&gt; img &#123; margin: 0 auto; display: block; &#125;&lt;/style&gt; markdown图片：![](https://images.morethink.cn/092017231747399.jpg "TCP的三次握手和四次挥手")]]></content>
      <categories>
        <category>Web前端</category>
      </categories>
  </entry>
  <entry>
    <title><![CDATA[GitHub更新已经fork的项目]]></title>
    <url>%2FGitHub%E6%9B%B4%E6%96%B0%E5%B7%B2%E7%BB%8Ffork%E7%9A%84%E9%A1%B9%E7%9B%AE%2F</url>
    <content type="text"><![CDATA[当我们fork一个项目后，在我们使用代码的时候就会以我们本地为准，不会跟随我们fork前的项目，如果需要同步对方的代码，需要进行同步操作。 clone 自己的 fork 分支到本地可以直接使用 GitHub 客户端，clone 到本地，如果使用命令行，命令为：$ git clone git@github.com:morethink/git-recipes.git 进入仓库，增加源分支地址到你项目远程分支列表中此处是关键，先得将原来的仓库指定为 upstream，命令为：$ git remote add upstream git@github.com:geeeeeeeeek/git-recipes.git此处可使用 git remote -v 查看远程分支列表 $ git remote -vorigin git@github.com:morethink/git-recipes.git (fetch)origin git@github.com:morethink/git-recipes.git (push)upstream git@github.com:geeeeeeeeek/git-recipes.git (fetch)upstream git@github.com:geeeeeeeeek/git-recipes.git (push) fetch 源分支的新版本到本地$ git fetch upstream 切换到本地master分支$ git checkout master 合并两个版本的代码$ git merge upstream/master 将合并后的代码 push 到 GitHub 上去$ git push origin master 参考文档： 添加远程分支https://help.github.com/articles/configuring-a-remote-for-a-fork/ 完成同步https://help.github.com/articles/syncing-a-fork/]]></content>
      <categories>
        <category>工具</category>
      </categories>
      <tags>
        <tag>Git</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Cron表达式]]></title>
    <url>%2FCron%E8%A1%A8%E8%BE%BE%E5%BC%8F%2F</url>
    <content type="text"><![CDATA[Cron表达式范例： */5 * * * * ? ：每隔5秒执行一次 0 */1 * * * ? ：每隔1分钟执行一次 0 0 23 * * ? ：每天23点执行一次 0 0 1 * * ? ：每天凌晨1点执行一次： 0 0 1 1 * ? ：每月1号凌晨1点执行一次 0 0 23 L * ? ： 每月最后一天23点执行一次 0 0 1 ? * L ：每周星期天凌晨1点实行一次 0 26,29,33 * * * ? ： 在26分、29分、33分执行一次 0 0 0,13,18,21 * * ? ： 每天的0点、13点、18点、21点都执行一次 Cron表达式 Cron表达式由7个部分组成，各部分用空格隔开，Cron表达式的7个部分从左到右代表的含义如下： 秒 分 时 日 月 周 年其中 年是可选的。 字段名 允许的值 允许的特殊字符 秒 0-59 ,- * / 分 0-59 ,- * / 时 0-23 ,- * / 日 1-31 ,- * ? / L W C 月 1-12 or JAN-DEC ,- * / 周 1-7 or SUN-SAT ,- * ? / L C # 年 (可选字段) empty，1970-2099 ,- * / 符号说明 ,：表示列出枚举值值。例如在分使用5,20，则意味着在5和20分每分钟触发一次。 -：表示范围。例如在分使用5-20，表示从5分到20分钟每分钟触发一次。 * ：表示匹配该域的任意值。假如在分域使用*，即表示每分钟都会触发事件。 / ：表示起始时间开始触发，然后每隔固定时间触发一次，例如在Minutes域使用5/20,则意味着5分钟触发一次，而25，45等分别触发一次。 ? ：只能用在周和日。它也匹配域的任意值，但实际不会。因为周和日会相互影响。例如想在每月的20日触发调度，不管20日到底是星期几，则只能使用如下写法： 13 13 15 20 ?,其中最后一位只能用？，而不能使用，如果使用*表示不管星期几都会触发，实际上并不是这样。 L ： 表示最后，只能出现在日和周，如果在日使用5L,意味着在最后的一个星期四触发。 W：表示有效工作日(周一到周五),只能出现在周域，系统将在离指定日期的最近的有效工作日触发事件。例如：在日使用5W，如果5日是星期六，则将在最近的工作日：星期五，即4日触发。如果5日是星期天，则在6日触发；如果5日在星期一到星期五中的一天，则就在5日触发。另外一点，W的最近寻找不会跨过月份。 #：用于确定每个月第几个星期几，只能出现在周。例如在4#2，表示某月的第二个星期三。 参考文档： 在线Cron表达式生成器 QuartZ Cron表达式详解]]></content>
      <categories>
        <category>Linux</category>
      </categories>
  </entry>
  <entry>
    <title><![CDATA[常见web攻击总结]]></title>
    <url>%2F%E5%B8%B8%E8%A7%81web%E6%94%BB%E5%87%BB%E6%80%BB%E7%BB%93%2F</url>
    <content type="text"><![CDATA[搞Web开发离不开安全这个话题，确保网站或者网页应用的安全性，是每个开发人员都应该了解的事。本篇主要简单介绍在Web领域几种常见的攻击手段及Java Web中的预防方式。 XSS SQL注入 DDOS CSRF 项目地址： https://github.com/morethink/web-security XSS什么是XSSXSS攻击：跨站脚本攻击(Cross-Site Scripting)，为了不和层叠样式表(Cascading Style Sheets, CSS)的缩写混淆，故将跨站脚本攻击缩写为XSS。XSS是一种常见的web安全漏洞，它允许攻击者将恶意代码植入到提供给其它用户使用的页面中。不同于大多数攻击(一般只涉及攻击者和受害者)，XSS涉及到三方，即攻击者、客户端与Web应用。XSS的攻击目标是为了盗取存储在客户端的cookie或者其他网站用于识别客户端身份的敏感信息。一旦获取到合法用户的信息后，攻击者甚至可以假冒合法用户与网站进行交互。 XSS通常可以分为两大类： 存储型XSS，主要出现在让用户输入数据，供其他浏览此页的用户进行查看的地方，包括留言、评论、博客日志和各类表单等。应用程序从数据库中查询数据，在页面中显示出来，攻击者在相关页面输入恶意的脚本数据后，用户浏览此类页面时就可能受到攻击。这个流程简单可以描述为：恶意用户的Html输入Web程序-&gt;进入数据库-&gt;Web程序-&gt;用户浏览器。 反射型XSS，主要做法是将脚本代码加入URL地址的请求参数里，请求参数进入程序后在页面直接输出，用户点击类似的恶意链接就可能受到攻击。 比如说我写了一个网站，然后攻击者在上面发布了一个文章，内容是这样的 &lt;script&gt;alert(document.cookie)&lt;/script&gt;,如果我没有对他的内容进行处理，直接存储到数据库，那么下一次当其他用户访问他的这篇文章的时候，服务器从数据库读取后然后响应给客户端，浏览器执行了这段脚本，就会将cookie展现出来，这就是典型的存储型XSS。 如图： 如何预防XSS答案很简单，坚决不要相信用户的任何输入，并过滤掉输入中的所有特殊字符。这样就能消灭绝大部分的XSS攻击。 目前防御XSS主要有如下几种方式： 过滤特殊字符避免XSS的方法之一主要是将用户所提供的内容进行过滤(如上面的script标签)。 使用HTTP头指定类型w.Header().Set(&quot;Content-Type&quot;,&quot;text/javascript&quot;)这样就可以让浏览器解析javascript代码，而不会是html输出。 SQL注入什么是SQL注入攻击者成功的向服务器提交恶意的SQL查询代码，程序在接收后错误的将攻击者的输入作为查询语句的一部分执行，导致原始的查询逻辑被改变，额外的执行了攻击者精心构造的恶意代码。 举例：&#39; OR &#39;1&#39;=&#39;1 这是最常见的 SQL注入攻击，当我们输如用户名 admin ，然后密码输如&#39; OR &#39;1&#39;=1=&#39;1的时候，我们在查询用户名和密码是否正确的时候，本来要执行的是SELECT * FROM user WHERE username=&#39;&#39; and password=&#39;&#39;,经过参数拼接后，会执行 SQL语句 SELECT * FROM user WHERE username=&#39;&#39; and password=&#39;&#39; OR &#39;1&#39;=&#39;1&#39;，这个时候1=1是成立，自然就跳过验证了。如下图所示： 但是如果再严重一点，密码输如的是&#39;;DROP TABLE user;--，那么 SQL命令为SELECT * FROM user WHERE username=&#39;admin&#39; and password=&#39;&#39;;drop table user;--&#39; 这个时候我们就直接把这个表给删除了。 如何预防SQL注入 在Java中，我们可以使用预编译语句(PreparedStatement)，这样的话即使我们使用 SQL语句伪造成参数，到了服务端的时候，这个伪造 SQL语句的参数也只是简单的字符，并不能起到攻击的作用。 对进入数据库的特殊字符（&#39;&quot;\尖括号&amp;*;等）进行转义处理，或编码转换。 在应用发布之前建议使用专业的SQL注入检测工具进行检测，以及时修补被发现的SQL注入漏洞。网上有很多这方面的开源工具，例如sqlmap、SQLninja等。 避免网站打印出SQL错误信息，比如类型错误、字段不匹配等，把代码里的SQL语句暴露出来，以防止攻击者利用这些错误信息进行SQL注入。 在上图展示中，使用了Java JDBC中的PreparedStatement预编译预防SQL注入，可以看到将所有输入都作为了字符串，避免执行恶意SQL。 DDOS什么是DDOSDDOS：分布式拒绝服务攻击（Distributed Denial of Service），简单说就是发送大量请求是使服务器瘫痪。DDos攻击是在DOS攻击基础上的，可以通俗理解，dos是单挑，而ddos是群殴，因为现代技术的发展，dos攻击的杀伤力降低，所以出现了DDOS，攻击者借助公共网络，将大数量的计算机设备联合起来，向一个或多个目标进行攻击。 在技术角度上，DDoS攻击可以针对网络通讯协议的各层，手段大致有：TCP类的SYN Flood、ACK Flood，UDP类的Fraggle、Trinoo，DNS Query Flood，ICMP Flood，Slowloris类等等。一般会根据攻击目标的情况，针对性的把技术手法混合，以达到最低的成本最难防御的目的，并且可以进行合理的节奏控制，以及隐藏保护攻击资源。 下面介绍一下TCP协议中的SYN攻击。 SYN攻击在三次握手过程中，服务器发送 SYN-ACK 之后，收到客户端的 ACK 之前的 TCP 连接称为半连接(half-open connect)。此时服务器处于 SYN_RCVD 状态。当收到 ACK 后，服务器才能转入 ESTABLISHED 状态. SYN攻击指的是，攻击客户端在短时间内伪造大量不存在的IP地址，向服务器不断地发送SYN包，服务器回复确认包，并等待客户的确认。由于源地址是不存在的，服务器需要不断的重发直至超时，这些伪造的SYN包将长时间占用未连接队列，正常的SYN请求被丢弃，导致目标系统运行缓慢，严重者会引起网络堵塞甚至系统瘫痪。 如何预防DDOS阿里巴巴的安全团队在实战中发现，DDoS 防御产品的核心是检测技术和清洗技术。检测技术就是检测网站是否正在遭受 DDoS 攻击，而清洗技术就是清洗掉异常流量。而检测技术的核心在于对业务深刻的理解，才能快速精确判断出是否真的发生了 DDoS 攻击。清洗技术对检测来讲，不同的业务场景下要求的粒度不一样。 CSRF什么是CSRFCSRF（Cross-site request forgery），中文名称：跨站请求伪造，也被称为：one click attack/session riding，缩写为：CSRF/XSRF。 你这可以这么理解CSRF攻击：攻击者盗用了你的身份，以你的名义发送恶意请求。CSRF能够做的事情包括：以你名义发送邮件，发消息，盗取你的账号，甚至于购买商品，虚拟货币转账……造成的问题包括：个人隐私泄露以及财产安全。 CSRF的原理下图简单阐述了CSRF攻击的思 从上图可以看出，要完成一次CSRF攻击，受害者必须依次完成两个步骤： 登录受信任网站A，并在本地生成Cookie。 在不登出A的情况下，访问危险网站B。 看到这里，你也许会说：“如果我不满足以上两个条件中的一个，我就不会受到CSRF的攻击”。是的，确实如此，但你不能保证以下情况不会发生： 你不能保证你登录了一个网站后，不再打开一个tab页面并访问另外的网站。 你不能保证你关闭浏览器了后，你本地的Cookie立刻过期，你上次的会话已经结束。（事实上，关闭浏览器不能结束一个会话，但大多数人都会错误的认为关闭浏览器就等于退出登录/结束会话了……） 上图中所谓的攻击网站，可能是一个存在其他漏洞的可信任的经常被人访问的网站。 下面讲一讲java解决CSRF攻击的方式。 模拟CSRF攻击登录A网站用户名和密码都是admin。 http://localhost:8081/login.html: 你有权限删除1号帖子http://localhost:8081/deletePost.html: 登录有CSRF攻击A网站的B网站http://localhost:8082/deletePost.html: 明显看到B网站是8082端口，A网站是8081端口，但是B网站的删除2号帖子功能依然实现。 如何预防CSRF攻击简单来说，CSRF 就是网站 A 对用户建立信任关系后，在网站 B 上利用这种信任关系，跨站点向网站 A 发起一些伪造的用户操作请求，以达到攻击的目的。 而之所以可以完成攻击是因为B向A发起攻击的时候会把A网站的cookie带给A网站，也就是说cookie已经不安全了。 通过Synchronizer TokensSynchronizer Tokens： 在表单里隐藏一个随机变化的 csrf_token csrf_token 提交到后台进行验证，如果验证通过则可以继续执行操作。这种情况有效的主要原因是网站 B 拿不到网站 A 表单里的 csrf_token 这种方式的使用条件是PHP和JSP等。因为cookie已经不安全了，因此把csrf_token值存储在session中，然后每次表单提交时都从session取出来放到form表单的隐藏域中，这样B网站不可以得到这个存储到session中的值。 下面是JSP的：&lt;input type="hidden" name="random_form" value=&lt;%=random%&gt;&gt;&lt;/input&gt; 但是我现在的情况是html，不是JSP，并不能动态的从session中取出csrf_token值。只能采用加密的方式了。 Hash加密cookie中csrf_token值这可能是最简单的解决方案了，因为攻击者不能获得第三方的Cookie(理论上)，所以表单中的数据也就构造失败了。 我采用的hash加密方法是JS实现Java的HashCode方法，得到hash值，这个比较简单。也可以采用其他的hash算法。 前端向后台传递hash之后的csrf_token值和cookie中的csrf_token值，后台拿到cookie中的csrf_token值后得到hashCode值然后与前端传过来的值进行比较，一样则通过。 你有权限删除3号帖子http://localhost:8081/deletePost.html B网站的他已经没有权限了我们通过UserFilter.java给攻击者返回的是403错误，表示服务器理解用户客户端的请求但拒绝处理。 http://localhost:8082/deletePost.html: 攻击者不能删除4号帖子。 前端代码： deletePost.html &lt;!DOCTYPE html&gt;&lt;html lang="en"&gt;&lt;head&gt; &lt;meta charset="UTF-8"&gt; &lt;title&gt;deletePost&lt;/title&gt; &lt;script type="text/javascript" src="js/jquery.min.js"&gt;&lt;/script&gt; &lt;script type="text/javascript"&gt; function deletePost() &#123; var url = '/post/' + document.getElementById("postId").value; var csrf_token = document.cookie.replace(/(?:(?:^|.*;\s*)csrf_token\s*\=\s*([^;]*).*$)|^.*$/, "$1"); console.log('csrf_token=' + csrf_token); $.ajax(&#123; type: "post",//请求方式 url: url, //发送请求地址 timeout: 30000,//超时时间：30秒 data: &#123; "_method": "delete", "csrf_token": hash(csrf_token) // 对csrf_token进行hash加密 &#125;, dataType: "json",//设置返回数据的格式 success: function (result) &#123; if (result.message == "success") &#123; $("#result").text("删除成功"); &#125; else &#123; $("#result").text("删除失败"); &#125; &#125;, error: function () &#123; //请求出错的处理 $("#result").text("请求出错"); &#125; &#125;); &#125; // javascript的String到int(32位)的hash算法 function hash(str) &#123; var hash = 0; if (str.length == 0) return hash; for (i = 0; i &lt; str.length; i++) &#123; char = str.charCodeAt(i); hash = ((hash &lt;&lt; 5) - hash) + char; hash = hash &amp; hash; // Convert to 32bit integer &#125; return hash; &#125; &lt;/script&gt;&lt;/head&gt;&lt;body&gt;&lt;h3&gt;删除帖子&lt;/h3&gt;帖子编号 ： &lt;input type="text" id="postId"/&gt;&lt;button onclick="deletePost();"&gt;deletePost&lt;/button&gt;&lt;br/&gt;&lt;br/&gt;&lt;br/&gt;&lt;div&gt; &lt;p id="result"&gt;&lt;/p&gt;&lt;/div&gt;&lt;/body&gt;&lt;/html&gt; 后台代码： UserInterceptor.java package cn.morethink.interceptor;import cn.morethink.util.JsonUtil;import cn.morethink.util.Result;import org.springframework.web.servlet.HandlerInterceptor;import org.springframework.web.servlet.ModelAndView;import javax.servlet.http.Cookie;import javax.servlet.http.HttpServletRequest;import javax.servlet.http.HttpServletResponse;import java.io.PrintWriter;/** * @author 李文浩 * @date 2018/1/4 */public class UserInterceptor implements HandlerInterceptor &#123; @Override public boolean preHandle(HttpServletRequest request, HttpServletResponse response, Object handler) throws Exception &#123; String method = request.getMethod(); System.out.println(method); if (method.equalsIgnoreCase("POST") || method.equalsIgnoreCase("DELETE") || method.equalsIgnoreCase("PUT")) &#123; String csrf_token = request.getParameter("csrf_token"); Cookie[] cookies = request.getCookies(); if (cookies != null &amp;&amp; cookies.length &gt; 0 &amp;&amp; csrf_token != null) &#123; for (Cookie cookie : cookies) &#123; if (cookie.getName().equals("csrf_token")) &#123; if (Integer.valueOf(csrf_token) == cookie.getValue().hashCode()) &#123; return true; &#125; &#125; &#125; &#125; &#125; Result result = new Result("403", "你还想攻击我??????????", ""); PrintWriter out = response.getWriter(); out.write(JsonUtil.toJson(result)); out.close(); return false; &#125; @Override public void postHandle(HttpServletRequest request, HttpServletResponse response, Object handler, ModelAndView modelAndView) throws Exception &#123; &#125; @Override public void afterCompletion(HttpServletRequest request, HttpServletResponse response, Object handler, Exception ex) throws Exception &#123; &#125;&#125; 注意： cookie必须要设置PATH才可以生效，否则在下一次请求的时候无法带给服务器。 Spring Boot 出现启动找不到主类的问题时可以mvn clean一下。 Filter设置response.sendError(403)在Spring Boot没有效果。 总结上面一共提到了4种攻击方式，分别是XSS攻击（关键是脚本，利用恶意脚本发起攻击），SQL注入（关键是通过用SQL语句伪造参数发出攻击），DDOS攻击（关键是发出大量请求，最后令服务器崩溃），CSRF攻击（关键是借助本地cookie进行认证，伪造发送请求）。 参考文档： XSS实战：我是如何拿下你的百度账号 总结几种常见web攻击手段及其防御方式 浅谈CSRF攻击方式 jQueue 动态设置form表单的action属性的值和方法 javascript的String到int(32位)的hash算法]]></content>
      <categories>
        <category>Web安全</category>
      </categories>
  </entry>
  <entry>
    <title><![CDATA[Git同时push到多个远程仓库]]></title>
    <url>%2FGit%E5%90%8C%E6%97%B6push%E5%88%B0%E5%A4%9A%E4%B8%AA%E8%BF%9C%E7%A8%8B%E4%BB%93%E5%BA%93%2F</url>
    <content type="text"><![CDATA[添加第二个远程地址时使用以下命令：git remote set-url --add origin git@github.com:morethink/programming.git 查看远程分支：git remote -v origin git@git.coding.net:morethink/programming.git (fetch)origin git@git.coding.net:morethink/programming.git (push)origin hexo@MyHost2:/var/repo/gitbook.git (push) 也可以同时 push 到多个远程地址：git push origin master Everything up-to-dateEverything up-to-date]]></content>
      <categories>
        <category>工具</category>
      </categories>
      <tags>
        <tag>Git</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[二叉树从右边看到的节点]]></title>
    <url>%2F%E4%BA%8C%E5%8F%89%E6%A0%91%E4%BB%8E%E5%8F%B3%E8%BE%B9%E7%9C%8B%E5%88%B0%E7%9A%84%E8%8A%82%E7%82%B9%2F</url>
    <content type="text"><![CDATA[本文是对 LeetCode Binary Tree Right Side View 解法的探讨。 题目：Given a binary tree, imagine yourself standing on the right side of it, return the values of the nodes you can see ordered from top to bottom.For example:Given the following binary tree, 1 &lt;--- / \2 3 &lt;--- \ \ 5 4 &lt;--- You should return [1, 3, 4]. 解法如下： /** * 因为题目的二叉树并不是满二叉树，所以采用层序遍历的方式。 * 将以前层序遍历中一个个出队的方式变为一层层出队， * 这样就能定位最右边的节点。 * @param root * @return */public List&lt;Integer&gt; rightSideView(TreeNode root) &#123; List&lt;Integer&gt; list = new ArrayList&lt;&gt;(); if (root == null) &#123; return list; &#125; Queue&lt;TreeNode&gt; queue = new LinkedList&lt;&gt;(); queue.offer(root); while (!queue.isEmpty()) &#123; int size = queue.size(); //将每一层的节点都出队 for (int i = 0; i &lt; size; i++) &#123; TreeNode treeNode = queue.poll(); if (i == 0) &#123; list.add(treeNode.val); &#125; if (treeNode.right != null) &#123; queue.offer(treeNode.right); &#125; if (treeNode.left != null) &#123; queue.offer(treeNode.left); &#125; &#125; &#125; return list;&#125;]]></content>
      <categories>
        <category>算法</category>
      </categories>
      <tags>
        <tag>树</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[求小于n的素数个数]]></title>
    <url>%2F%E6%B1%82%E5%B0%8F%E4%BA%8En%E7%9A%84%E7%B4%A0%E6%95%B0%E4%B8%AA%E6%95%B0%2F</url>
    <content type="text"><![CDATA[本文是对 LeetCode Count Primes 解法的探讨。 题目：Count the number of prime numbers less than a non-negative number, n. 尽管题目并没有要我们写一个最优的算法，但是身为一个程序员，优化应该是一种习惯，在编程的过程中，随着思考进行优化。只要求我们满足给定的时间和空间即可。 如果你只能想出一个最简单的方法，难道你会有什么竞争力吗？ 穷举最开始我用的就是这个方法，可以说这是最简单的一种方法了，而且最开始，我就是想的这种方法，说明：我没有对这个问题进行思考，没有去优化它，而作为一个程序员，如何提高效率是拿到一个问题首先要思考的事情。 public int countPrimes(int n) &#123; int num = 0; for (int i = 2; i &lt; n; i++) &#123; boolean flag = true; for (int j = 2; j &lt; i - 1; j++) if (i % j == 0) &#123; flag = false; break; &#125; if (flag) &#123; num++; &#125; &#125; return num;&#125; 测试代码： public static void main(String[] args) &#123; //获取开始时 long startTime = System.currentTimeMillis(); System.out.println("The num is " + new L_204_Count_Primes().countPrimes(2000000)); long endTime = System.currentTimeMillis(); //获取结束时间 System.out.println("程序运行时间： " + (endTime - startTime) + "ms");&#125; 时间太长，已经不能计算。 只能是奇数且小于$\sqrt{n}$思考后发现 素数一定是奇数 若 n=ab 是个合数（其中 a 与 b ≠ 1）, 则其中一个约数 a 或 b 必定至大为 $\sqrt{n}$. public int countPrimes2(int n) &#123; int num = 1; for (int i = 3; i &lt; n; i += 2) &#123; boolean flag = true; for (int j = 2; j &lt;= (int) Math.sqrt(i); j++) if (i % j == 0) &#123; flag = false; break; &#125; if (flag) &#123; num++; &#125; &#125; return num;&#125; The num is 148933程序运行时间： 1124ms 试除法：数学知识的运用查阅 算术基本定理可知： 算术基本定理 :每个大于1的整数均可写成一个以上的素数之乘积，且除了质约数的排序不同外是唯一的 也就是说我们可以每个数来除以得到的素数，这样可大大减少运行次数。 public int countPrimes3(int n) &#123; if (n &lt; 3) &#123; return 0; &#125; //0 1 不算做素数,2一定是素数 List&lt;Integer&gt; list = new ArrayList&lt;&gt;(); list.add(2); boolean flag; for (int i = 3; i &lt; n; i += 2) &#123; flag = true; for (int j = 0; j &lt; list.size() &amp;&amp; list.get(j) &lt;= (int) Math.sqrt(n); j++) &#123; if (i % list.get(j) == 0) &#123; flag = false; break; &#125; &#125; if (flag) &#123; list.add(i); &#125; &#125; return list.size();&#125; The num is 148933程序运行时间： 383ms 筛选法 埃拉托斯特尼筛法，简称埃氏筛，也有人称素数筛。这是一种简单且历史悠久的筛法，用来找出一定范围内所有的素数。 所使用的原理是从2开始，将每个素数的各个倍数，标记成合数。一个素数的各个倍数，是一个差为此素数本身的等差数列。此为这个筛法和试除法不同的关键之处，后者是以素数来测试每个待测数能否被整除。 筛选法的策略是将素数的倍数全部筛掉，剩下的就是素数了，下图很生动的体现了筛选的过程： 筛选的过程是先筛掉非素数，针对本文的题目，每筛掉一个，素数数量-1即可，上面说过素数的一个特点，除了2，其它的素数都是奇数，所以我们只需在奇数范围内筛选就可以了。 public int countPrimes4(int n) &#123; if (n &lt; 3) &#123; return 0; &#125; //false代表素数，true代表非素数 boolean[] flags = new boolean[n]; //0不是素数 flags[0] = true; //1不是素数 flags[1] = true; int num = n - 2; for (int i = 2; i &lt;= (int) Math.sqrt(n); i++) &#123; //当i为素数时，i的所有倍数都不是素数 if (!flags[i]) &#123; for (int j = 2 * i; j &lt; n; j += i) &#123; if (!flags[j]) &#123; flags[j] = true; num--; &#125; &#125; &#125; &#125; return num;&#125; The num is 148933程序运行时间： 43ms 全部代码放在： https://github.com/morethink/algorithm/blob/master/src/algorithm/leetcode/L_204_Count_Primes.java 参考文档： 求质数算法的N种境界[1] - 试除法和初级筛法 求素数个数 埃拉托斯特尼筛法]]></content>
      <categories>
        <category>算法</category>
      </categories>
      <tags>
        <tag>素数</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Java实现单链表的快速排序和归并排序]]></title>
    <url>%2FJava%E5%AE%9E%E7%8E%B0%E5%8D%95%E9%93%BE%E8%A1%A8%E7%9A%84%E5%BF%AB%E9%80%9F%E6%8E%92%E5%BA%8F%E5%92%8C%E5%BD%92%E5%B9%B6%E6%8E%92%E5%BA%8F%2F</url>
    <content type="text"><![CDATA[本文描述了LeetCode 148题 sort-list 的解法。 题目描述如下:Sort a linked list in O(n log n) time using constant space complexity. 题目要求我们在O(n log n)时间复杂度下完成对单链表的排序，我们知道平均时间复杂度为O(n log n)的排序方法有快速排序、归并排序和堆排序。而一般是用数组来实现二叉堆，当然可以用二叉树来实现，但是这么做太麻烦，还得花费额外的空间构建二叉树，于是不采用堆排序。故本文采用快速排序和归并排序来对单链表进行排序。 快速排序在一般实现的快速排序中，我们通过首尾指针来对元素进行切分，下面采用快排的另一种方法来对元素进行切分。 我们只需要两个指针p1和p2，这两个指针均往next方向移动，移动的过程中保持p1之前的key都小于选定的key，p1和p2之间的key都大于选定的key，那么当p2走到末尾时交换p1与key值便完成了一次切分。 图示如下： 代码如下：public ListNode sortList(ListNode head) &#123; //采用快速排序 quickSort(head, null); return head;&#125;public static void quickSort(ListNode head, ListNode end) &#123; if (head != end) &#123; ListNode node = partion(head, end); quickSort(head, node); quickSort(node.next, end); &#125;&#125;public static ListNode partion(ListNode head, ListNode end) &#123; ListNode p1 = head, p2 = head.next; //走到末尾才停 while (p2 != end) &#123; //大于key值时，p1向前走一步，交换p1与p2的值 if (p2.val &lt; head.val) &#123; p1 = p1.next; int temp = p1.val; p1.val = p2.val; p2.val = temp; &#125; p2 = p2.next; &#125; //当有序时，不交换p1和key值 if (p1 != head) &#123; int temp = p1.val; p1.val = head.val; head.val = temp; &#125; return p1;&#125; 归并排序归并排序应该算是链表排序最佳的选择了，保证了最好和最坏时间复杂度都是nlogn，而且它在数组排序中广受诟病的空间复杂度在链表排序中也从O(n)降到了O(1)。 归并排序的一般步骤为： 将待排序数组（链表）取中点并一分为二； 递归地对左半部分进行归并排序； 递归地对右半部分进行归并排序； 将两个半部分进行合并（merge）,得到结果。 首先用快慢指针(快慢指针思路，快指针一次走两步，慢指针一次走一步，快指针在链表末尾时，慢指针恰好在链表中点)的方法找到链表中间节点，然后递归的对两个子链表排序，把两个排好序的子链表合并成一条有序的链表。 代码如下： public ListNode sortList(ListNode head) &#123; //采用归并排序 if (head == null || head.next == null) &#123; return head; &#125; //获取中间结点 ListNode mid = getMid(head); ListNode right = mid.next; mid.next = null; //合并 return mergeSort(sortList(head), sortList(right));&#125;/** * 获取链表的中间结点,偶数时取中间第一个 * * @param head * @return */private ListNode getMid(ListNode head) &#123; if (head == null || head.next == null) &#123; return head; &#125; //快慢指针 ListNode slow = head, quick = head; //快2步，慢一步 while (quick.next != null &amp;&amp; quick.next.next != null) &#123; slow = slow.next; quick = quick.next.next; &#125; return slow;&#125;/** * * 归并两个有序的链表 * * @param head1 * @param head2 * @return */private ListNode mergeSort(ListNode head1, ListNode head2) &#123; ListNode p1 = head1, p2 = head2, head; //得到头节点的指向 if (head1.val &lt; head2.val) &#123; head = head1; p1 = p1.next; &#125; else &#123; head = head2; p2 = p2.next; &#125; ListNode p = head; //比较链表中的值 while (p1 != null &amp;&amp; p2 != null) &#123; if (p1.val &lt;= p2.val) &#123; p.next = p1; p1 = p1.next; p = p.next; &#125; else &#123; p.next = p2; p2 = p2.next; p = p.next; &#125; &#125; //第二条链表空了 if (p1 != null) &#123; p.next = p1; &#125; //第一条链表空了 if (p2 != null) &#123; p.next = p2; &#125; return head;&#125; 完整代码放在：https://github.com/morethink/algorithm/blob/master/src/main/java/algorithm/leetcode/L_148_SortList.java 参考文档： 链表排序（冒泡、选择、插入、快排、归并、希尔、堆排序）]]></content>
      <categories>
        <category>算法</category>
      </categories>
      <tags>
        <tag>链表</tag>
        <tag>排序</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Java实现八大排序算法]]></title>
    <url>%2FJava%E5%AE%9E%E7%8E%B0%E5%85%AB%E5%A4%A7%E6%8E%92%E5%BA%8F%E7%AE%97%E6%B3%95%2F</url>
    <content type="text"><![CDATA[本文对常见的排序算法进行了总结。 常见排序算法如下： 直接插入排序 希尔排序 简单选择排序 堆排序 冒泡排序 快速排序 归并排序 基数排序 它们都属于内部排序，也就是只考虑数据量较小仅需要使用内存的排序算法，他们之间关系如下：$$\begin{cases}内部排序 \begin{cases}插入排序\begin{cases}直接插入排序\希尔排序\end{cases}\选择排序\begin{cases}简单选择排序\堆排序\end{cases}\交换排序\begin{cases}冒泡排序\快速排序 \end{cases}\归并排序\基数排序\end{cases}\外部排序 \end{cases}$$ $$\left{\begin{matrix}内部排序\外部排序\end{matrix}\right.$$ 稳定与非稳定: 如果一个排序算法能够保留数组中重复元素的相对位置则可以被称为是 稳定 的。反之，则是 非稳定 的。 直接插入排序基本思想通常人们整理桥牌的方法是一张一张的来，将每一张牌插入到其他已经有序的牌中的适当位置。在计算机的实现中，为了要给插入的元素腾出空间，我们需要将其余所有元素在插入之前都向右移动一位。 算法描述一般来说，插入排序都采用in-place在数组上实现。具体算法描述如下： 从第一个元素开始，该元素可以认为已经被排序 取出下一个元素，在已经排序的元素序列中从后向前扫描 如果该元素（已排序）大于新元素，将该元素移到下一位置 重复步骤3，直到找到已排序的元素小于或者等于新元素的位置 将新元素插入到该位置后 重复步骤2~5 动态效果如下： 注意：如果 比较操作 的代价比 交换操作 大的话，可以采用二分查找法来减少 比较操作 的数目。该算法可以认为是 插入排序 的一个变种，称为二分查找插入排序。 代码实现/** * 通过交换进行插入排序，借鉴冒泡排序 * * @param a */public static void sort(int[] a) &#123; for (int i = 0; i &lt; a.length - 1; i++) &#123; for (int j = i + 1; j &gt; 0; j--) &#123; if (a[j] &lt; a[j - 1]) &#123; int temp = a[j]; a[j] = a[j - 1]; a[j - 1] = temp; &#125; &#125; &#125;&#125;/** * 通过将较大的元素都向右移动而不总是交换两个元素 * * @param a */public static void sort2(int[] a) &#123; for (int i = 1; i &lt; a.length; i++) &#123; int num = a[i]; int j; for (j = i; j &gt; 0 &amp;&amp; num &lt; a[j]; j--) &#123; a[j] = a[j - 1]; &#125; a[j] = num; &#125;&#125; 复杂度分析直接插入排序复杂度如下： 平均时间复杂度 最好情况 最坏情况 空间复杂度 O(n²) O(n²) O(n²) O(1) 比较与总结插入排序所需的时间取决于输入元素的初始顺序。例如，对一个很大且其中的元素已经有序(或接近有序)的数组进行排序将会比随机顺序的数组或是逆序数组进行排序要快得多。 希尔排序希尔排序，也称 递减增量排序算法，是插入排序的一种更高效的改进版本。希尔排序是 非稳定排序算法。 希尔排序是基于插入排序的以下两点性质而提出改进方法的： 插入排序在对几乎已经排好序的数据操作时，效率高，即可以达到线性排序的效率 但插入排序一般来说是低效的，因为插入排序每次只能将数据移动一 希尔排序是先将整个待排序的记录序列分割成为若干子序列分别进行直接插入排序，待整个序列中的记录“基本有序”时，再对全体记录进行依次直接插入排序。 基本思想将待排序数组按照步长gap进行分组，然后将每组的元素利用直接插入排序的方法进行排序；每次再将gap折半减小，循环上述操作；当gap=1时，利用直接插入，完成排序。 可以看到步长的选择是希尔排序的重要部分。只要最终步长为1任何步长序列都可以工作。一般来说最简单的步长取值是初次取数组长度的一半为增量，之后每次再减半，直到增量为1。更好的步长序列取值可以参考维基百科。 算法描述 选择一个增量序列 t1，t2，……，tk，其中 ti &gt; tj, tk = 1； 按增量序列个数 k，对序列进行 k 趟排序； 每趟排序，根据对应的增量 ti，将待排序列分割成若干长度为 m 的子序列，分别对各子表进行直接插入排序。仅增量因子为 1 时，整个序列作为一个表来处理，表长度即为整个序列的长度。 效果如下： 代码实现下面参考《算法》中给出的步长选择策略，《算法》中给出的解释是 下面代码中递增序列的计算和使用都很简单，和复杂递增序列的性能接近。当可以证明复杂的序列在最坏情况下的性能要好于我们所使用的递增序列。更加优秀的递增序列有待我们去发现。 public static void sort(int[] a) &#123; int length = a.length; int h = 1; while (h &lt; length / 3) h = 3 * h + 1; for (; h &gt;= 1; h /= 3) &#123; for (int i = 0; i &lt; a.length - h; i += h) &#123; for (int j = i + h; j &gt; 0; j -= h) &#123; if (a[j] &lt; a[j - h]) &#123; int temp = a[j]; a[j] = a[j - h]; a[j - h] = temp; &#125; &#125; &#125; &#125;&#125; 复杂度分析以下是希尔排序复杂度: 平均时间复杂度 最好情况 最坏情况 空间复杂度 O(nlog2 n) O(nlog2 n) O(nlog2 n) O(1) 总结与思考希尔排序更高效的原因是它权衡了子数组的规模和有序性。排序之初，各个子数组都很短，排序之后子数组都是部分有序的，这两种情况都很适合插入排序。 简单选择排序基本思想选择排序（Selection sort）是一种简单直观的排序算法。它的工作原理如下。首先在未排序序列中找到最小（大）元素，存放到排序序列的起始位置，然后，再从剩余未排序元素中继续寻找最小（大）元素，然后放到已排序序列的末尾。以此类推，直到所有元素均排序完毕。 选择排序的主要优点与数据移动有关。如果某个元素位于正确的最终位置上，则它不会被移动。选择排序每次交换一对元素，它们当中至少有一个将被移到其最终位置上，因此对 n个元素的表进行排序总共进行至多 n-1 次交换。在所有的完全依靠交换去移动元素的排序方法中，选择排序属于非常好的一种。 算法描述 从未排序序列中，找到关键字最小的元素 如果最小元素不是未排序序列的第一个元素，将其和未排序序列第一个元素互换 重复1、2步，直到排序结束。 动图效果如下所示： 代码实现public static void sort(int[] a) &#123; for (int i = 0; i &lt; a.length; i++) &#123; int min = i; //选出之后待排序中值最小的位置 for (int j = i + 1; j &lt; a.length; j++) &#123; if (a[j] &lt; a[min]) &#123; min = j; &#125; &#125; //最小值不等于当前值时进行交换 if (min != i) &#123; int temp = a[i]; a[i] = a[min]; a[min] = temp; &#125; &#125;&#125; 复杂度分析以下是选择排序复杂度: 平均时间复杂度 最好情况 最坏情况 空间复杂度 O(n²) O(n²) O(n²) O(1) 总结与思考选择排序的简单和直观名副其实，这也造就了它”出了名的慢性子”，无论是哪种情况，哪怕原数组已排序完成，它也将花费将近n²/2次遍历来确认一遍。即便是这样，它的排序结果也还是不稳定的。 唯一值得高兴的是，它并不耗费额外的内存空间。 堆排序 1991年的计算机先驱奖获得者、斯坦福大学计算机科学系教授罗伯特·弗洛伊德(Robert W．Floyd) 和威廉姆斯(J．Williams) 在1964年共同发明了著名的堆排序算法(Heap Sort). 堆的定义如下：$n$个元素的序列{k1,k2,..,kn}当且仅当满足下关系时，称之为堆。 把此序列对应的二维数组看成一个完全二叉树。那么堆的含义就是：完全二叉树中任何一个非叶子节点的值均不大于（或不小于）其左，右孩子节点的值。 由上述性质可知大顶堆的堆顶的关键字肯定是所有关键字中最大的，小顶堆的堆顶的关键字是所有关键字中最小的。因此我们可使用大顶堆进行升序排序, 使用小顶堆进行降序排序。 基本思想此处以大顶堆为例，堆排序的过程就是将待排序的序列构造成一个堆，选出堆中最大的移走，再把剩余的元素调整成堆，找出最大的再移走，重复直至有序。 算法描述 先将初始序列$K[1..n]$建成一个大顶堆, 那么此时第一个元素$K_1$最大, 此堆为初始的无序区. 再将关键字最大的记录$K_1$ (即堆顶, 第一个元素)和无序区的最后一个记录 $K_n$ 交换, 由此得到新的无序区$K[1..n-1]$和有序区$K[n]$, 且满足$K[1..n-1].keys \leqslant K[n].key$ 交换$K_1$ 和 $K_n$ 后, 堆顶可能违反堆性质, 因此需将$K[1..n-1]$调整为堆. 然后重复步骤2, 直到无序区只有一个元素时停止。 动图效果如下所示： 代码实现从算法描述来看，堆排序需要两个过程，一是建立堆，二是堆顶与堆的最后一个元素交换位置。所以堆排序有两个函数组成。一是建堆函数，二是反复调用建堆函数以选择出剩余未排元素中最大的数来实现排序的函数。 总结起来就是定义了以下几种操作： 最大堆调整（Max_Heapify）：将堆的末端子节点作调整，使得子节点永远小于父节点 创建最大堆（Build_Max_Heap）：将堆所有数据重新排序 堆排序（HeapSort）：移除位在第一个数据的根节点，并做最大堆调整的递归运算 对于堆节点的访问： 父节点i的左子节点在位置：(2*i+1); 父节点i的右子节点在位置：(2*i+2); 子节点i的父节点在位置：floor((i-1)/2); /** * @param a */public static void sort(int[] a) &#123; for (int i = a.length - 1; i &gt; 0; i--) &#123; max_heapify(a, i); //堆顶元素(第一个元素)与Kn交换 int temp = a[0]; a[0] = a[i]; a[i] = temp; &#125;&#125;/*** * * 将数组堆化 * i = 第一个非叶子节点。 * 从第一个非叶子节点开始即可。无需从最后一个叶子节点开始。 * 叶子节点可以看作已符合堆要求的节点，根节点就是它自己且自己以下值为最大。 * * @param a * @param n */public static void max_heapify(int[] a, int n) &#123; int child; for (int i = (n - 1) / 2; i &gt;= 0; i--) &#123; //左子节点位置 child = 2 * i + 1; //右子节点存在且大于左子节点，child变成右子节点 if (child != n &amp;&amp; a[child] &lt; a[child + 1]) &#123; child++; &#125; //交换父节点与左右子节点中的最大值 if (a[i] &lt; a[child]) &#123; int temp = a[i]; a[i] = a[child]; a[child] = temp; &#125; &#125;&#125; 复杂度分析 建立堆的过程, 从length/2 一直处理到0, 时间复杂度为O(n); 调整堆的过程是沿着堆的父子节点进行调整, 执行次数为堆的深度, 时间复杂度为O(lgn); 堆排序的过程由n次第2步完成, 时间复杂度为O(nlgn). 平均时间复杂度 最好情况 最坏情况 空间复杂度 $O(n \log_{2}n)$ $O(n \log_{2}n)$ $O(n \log_{2}n)$ $O(1)$ 总结与思考由于堆排序中初始化堆的过程比较次数较多, 因此它不太适用于小序列。 同时由于多次任意下标相互交换位置, 相同元素之间原本相对的顺序被破坏了, 因此, 它是不稳定的排序。 冒泡排序 我想对于它每个学过C语言的都会了解，这可能是很多人接触的第一个排序算法。 基本思想冒泡排序（Bubble Sort）是一种简单的排序算法。它重复地走访过要排序的数列，一次比较两个元素，如果他们的顺序错误就把他们交换过来。走访数列的工作是重复地进行直到没有再需要交换，也就是说该数列已经排序完成。这个算法的名字由来是因为越小的元素会经由交换慢慢“浮”到数列的顶端。 算法描述冒泡排序算法的运作如下： 比较相邻的元素。如果第一个比第二个大，就交换他们两个。 对每一对相邻元素作同样的工作，从开始第一对到结尾的最后一对。这步做完后，最后的元素会是最大的数。 针对所有的元素重复以上的步骤，除了最后一个。 持续每次对越来越少的元素重复上面的步骤，直到没有任何一对数字需要比较。 代码实现public static void sort(int[] a) &#123; //外层循环控制比较的次数 for (int i = 0; i &lt; a.length - 1; i++) &#123; //内层循环控制到达位置 for (int j = 0; j &lt; a.length - i - 1; j++) &#123; //前面的元素比后面大就交换 if (a[j] &gt; a[j + 1]) &#123; int temp = a[j]; a[j] = a[j + 1]; a[j + 1] = temp; &#125; &#125; &#125;&#125; 复杂度分析以下是冒泡排序算法复杂度: 平均时间复杂度 最好情况 最坏情况 空间复杂度 O(n²) O(n) O(n²) O(1) 冒泡排序是最容易实现的排序, 最坏的情况是每次都需要交换, 共需遍历并交换将近n²/2次, 时间复杂度为O(n²). 最佳的情况是内循环遍历一次后发现排序是对的, 因此退出循环, 时间复杂度为O(n). 平均来讲, 时间复杂度为O(n²). 由于冒泡排序中只有缓存的temp变量需要内存空间, 因此空间复杂度为常量O(1). 总结与思考由于冒泡排序只在相邻元素大小不符合要求时才调换他们的位置, 它并不改变相同元素之间的相对顺序, 因此它是稳定的排序算法。 快速排序快速排序是由东尼·霍尔所发展的一种排序算法。在平均状况下，排序 n 个项目要 Ο(nlogn) 次比较。在最坏状况下则需要 Ο(n2) 次比较，但这种状况并不常见。事实上，快速排序通常明显比其他 Ο(nlogn) 算法更快，因为它的内部循环（inner loop）可以在大部分的架构上很有效率地被实现出来。 基本思想快速排序的基本思想：挖坑填数+分治法。 快速排序使用分治法（Divide and conquer）策略来把一个串行（list）分为两个子串行（sub-lists）。 快速排序又是一种分而治之思想在排序算法上的典型应用。本质上来看，快速排序应该算是在冒泡排序基础上的递归分治法。 快速排序的名字起的是简单粗暴，因为一听到这个名字你就知道它存在的意义，就是快，而且效率高！它是处理大数据最快的排序算法之一了。虽然 Worst Case 的时间复杂度达到了 O(n²)，但是人家就是优秀，在大多数情况下都比平均时间复杂度为 O(n logn) 的排序算法表现要更好。 算法描述快速排序使用分治策略来把一个序列（list）分为两个子序列（sub-lists）。步骤为： 从数列中挑出一个元素，称为”基准”（pivot）。 重新排序数列，所有比基准值小的元素摆放在基准前面，所有比基准值大的元素摆在基准后面（相同的数可以到任一边）。在这个分区结束之后，该基准就处于数列的中间位置。这个称为分区（partition）操作。 递归地（recursively）把小于基准值元素的子数列和大于基准值元素的子数列排序。 递归到最底部时，数列的大小是零或一，也就是已经排序好了。这个算法一定会结束，因为在每次的迭代（iteration）中，它至少会把一个元素摆到它最后的位置去。 代码实现用伪代码描述如下： i = L; j = R; 将基准数挖出形成第一个坑a[i]。 j--，由后向前找比它小的数，找到后挖出此数填前一个坑a[i]中。 i++，由前向后找比它大的数，找到后也挖出此数填到前一个坑a[j]中。 再重复执行2，3二步，直到i==j，将基准数填入a[i]中 public static void sort(int[] a, int low, int high) &#123; //已经排完 if (low &gt;= high) &#123; return; &#125; int left = low; int right = high; //保存基准值 int pivot = a[left]; while (left &lt; right) &#123; //从后向前找到比基准小的元素 while (left &lt; right &amp;&amp; a[right] &gt;= pivot) right--; a[left] = a[right]; //从前往后找到比基准大的元素 while (left &lt; right &amp;&amp; a[left] &lt;= pivot) left++; a[right] = a[left]; &#125; // 放置基准值，准备分治递归快排 a[left] = pivot; sort(a, low, left - 1); sort(a, left + 1, high);&#125; 上面是递归版的快速排序：通过把基准插入到合适的位置来实现分治，并递归地对分治后的两个划分继续快排。那么非递归版的快排如何实现呢？ 因为 递归的本质是栈 ，所以我们非递归实现的过程中，可以借助栈来保存中间变量就可以实现非递归了。在这里中间变量也就是通过Pritation函数划分区间之后分成左右两部分的首尾指针，只需要保存这两部分的首尾指针即可。 public static void sortByStack(int[] a) &#123; Stack&lt;Integer&gt; stack = new Stack&lt;Integer&gt;(); //初始状态的左右指针入栈 stack.push(0); stack.push(a.length - 1); while (!stack.isEmpty()) &#123; //出栈进行划分 int high = stack.pop(); int low = stack.pop(); int pivotIndex = partition(a, low, high); //保存中间变量 if (pivotIndex &gt; low) &#123; stack.push(low); stack.push(pivotIndex - 1); &#125; if (pivotIndex &lt; high &amp;&amp; pivotIndex &gt;= 0) &#123; stack.push(pivotIndex + 1); stack.push(high); &#125; &#125;&#125;private static int partition(int[] a, int low, int high) &#123; if (low &gt;= high) return -1; int left = low; int right = high; //保存基准的值 int pivot = a[left]; while (left &lt; right) &#123; //从后向前找到比基准小的元素，插入到基准位置中 while (left &lt; right &amp;&amp; a[right] &gt;= pivot) &#123; right--; &#125; a[left] = a[right]; //从前往后找到比基准大的元素 while (left &lt; right &amp;&amp; a[left] &lt;= pivot) &#123; left++; &#125; a[right] = a[left]; &#125; //放置基准值，准备分治递归快排 a[left] = pivot; return left;&#125; 算法改进切换到插入排序和大多数递归排序算法一样，改进快速排序性能的一个简单方法基于以下两点： 对于小数组，快速排序比插入排序慢 因为递归，快速排序的sort()方法在小数组中叶会调用自己 因此，在排序小数组时应该切换到插入排序。 三者取中法快速排序是通常被认为在同数量级（O(nlog2n)）的排序方法中平均性能最好的。但若初始序列按关键码有序或基本有序时，快排序反而蜕化为冒泡排序。为改进之，通常以“三者取中法”来选取基准记录，即将排序区间的两个端点与中点三个记录关键码居中的调整为支点记录。 三向快速排序实际应用中经常会出现含有大量重复元素的数组。例如，一个元素全部重复的子数组就不需要继续排序了，但我们的算法还会继续将它切分为更小的数组。在有大量重复元素的情况下，快速排序的递归性会使元素全部重复的子数组经常出现，这就有很大的改进潜力，经当前实现的线性对数级的性能提高到线性级别。 算法描述： 在lt之前的(lo~lt-1)都小于中间值 在gt之前的(gt+1~hi)都大于中间值 在lt~i-1的都等于中间值 在i~gt的都还不确定（最终i会大于gt，即不确定的将不复存在） 代码实现： public static void sortThreeWay(int[] a, int lo, int hi) &#123; if (lo &gt;= hi) &#123; return; &#125; int v = a[lo], lt = lo, i = lo + 1, gt = hi; while (i &lt;= gt) &#123; if (a[i] &lt; v) &#123; swap(a, i++, lt++); &#125; else if (a[i] &gt; v) &#123; swap(a, i, gt--); &#125; else &#123; i++; &#125; &#125; sortThreeWay(a, lo, lt - 1); sortThreeWay(a, gt + 1, hi);&#125;private static void swap(int[] a, int i, int j) &#123; int t = a[i]; a[i] = a[j]; a[j] = t;&#125; 复杂度分析以下是快速排序算法复杂度: 平均时间复杂度 最好情况 最坏情况 空间复杂度 O(nlog₂n) O(nlog₂n) O(n²) O(1)（原地分区递归版） 归并排序归并排序是建立在归并操作上的一种有效的排序算法，1945年由约翰·冯·诺伊曼首次提出。该算法是采用分治法（Divide and Conquer）的一个非常典型的应用，且各层分治递归可以同时进行。 基本思想归并排序算法是将两个（或两个以上）有序表合并成一个新的有序表，即把待排序序列分为若干个子序列，每个子序列是有序的。然后再把有序子序列合并为整体有序序列。 算法描述归并排序可通过两种方式实现: 自上而下的递归 自下而上的迭代 递归法（假设序列共有n个元素）： 将序列每相邻两个数字进行归并操作，形成 floor(n/2)个序列，排序后每个序列包含两个元素； 将上述序列再次归并，形成 floor(n/4)个序列，每个序列包含四个元素； 重复步骤2，直到所有元素排序完毕。 迭代法 申请空间，使其大小为两个已经排序序列之和，该空间用来存放合并后的序列 设定两个指针，最初位置分别为两个已经排序序列的起始位置 比较两个指针所指向的元素，选择相对小的元素放入到合并空间，并移动指针到下一位置 重复步骤3直到某一指针到达序列尾 将另一序列剩下的所有元素直接复制到合并序列尾 代码实现归并排序其实要做两件事： 分解：将序列每次折半拆分 合并：将划分后的序列段两两排序合并 因此，归并排序实际上就是两个操作，拆分+合并 下面是递归的方法： public class Merge &#123; //归并所需的辅助数组 private static int[] aux; public static void sort(int[] a) &#123; //一次性分配空间 aux = new int[a.length]; sort(a, 0, a.length - 1); &#125; public static void sort(int[] a, int low, int high) &#123; if (low &gt;= high) &#123; return; &#125; int mid = (low + high) / 2; //将左半边排序 sort(a, low, mid); //将右半边排序 sort(a, mid + 1, high); merge(a, low, mid, high); &#125; /** * 该方法先将所有元素复制到aux[]中，然后在归并会a[]中。方法咋归并时(第二个for循环) * 进行了4个条件判断： * - 左半边用尽(取右半边的元素) * - 右半边用尽(取左半边的元素) * - 右半边的当前元素小于左半边的当前元素(取右半边的元素) * - 右半边的当前元素大于等于左半边的当前元素(取左半边的元素) * @param a * @param low * @param mid * @param high */ public static void merge(int[] a, int low, int mid, int high) &#123; //将a[low..mid]和a[mid+1..high]归并 int i = low, j = mid + 1; for (int k = low; k &lt;= high; k++) &#123; aux[k] = a[k]; &#125; for (int k = low; k &lt;= high; k++) &#123; if (i &gt; mid) &#123; a[k] = aux[j++]; &#125; else if (j &gt; high) &#123; a[k] = aux[i++]; &#125; else if (aux[j] &lt; aux[i]) &#123; a[k] = aux[j++]; &#125; else &#123; a[k] = aux[i++]; &#125; &#125; &#125;&#125; 复杂度分析以下是归并排序算法复杂度: 平均时间复杂度 最好情况 最坏情况 空间复杂度 O(nlog₂n) O(nlog₂n) O(nlog₂n) O(n) 从效率上看，归并排序可算是排序算法中的”佼佼者”. 假设数组长度为n，那么拆分数组共需logn，, 又每步都是一个普通的合并子数组的过程， 时间复杂度为O(n)， 故其综合时间复杂度为O(nlogn)。另一方面， 归并排序多次递归过程中拆分的子数组需要保存在内存空间， 其空间复杂度为O(n)。 总结与思考归并排序最吸引人的性质是它能够保证将任意长度为N的数组排序所需时间和NlogN成正比，它的主要缺点则是他所需的额外空间和N成正比。 基数排序基数排序的发明可以追溯到1887年赫尔曼·何乐礼在打孔卡片制表机（Tabulation Machine）, 排序器每次只能看到一个列。它是基于元素值的每个位上的字符来排序的。 对于数字而言就是分别基于个位，十位， 百位或千位等等数字来排序。 基数排序（Radix sort）是一种非比较型整数排序算法，其原理是将整数按位数切割成不同的数字，然后按每个位数分别比较。由于整数也可以表达字符串（比如名字或日期）和特定格式的浮点数，所以基数排序也不是只能使用于整数。 基本思想它是这样实现的：将所有待比较数值（正整数）统一为同样的数位长度，数位较短的数前面补零。然后，从最低位开始，依次进行一次排序。这样从最低位排序一直到最高位排序完成以后，数列就变成一个有序序列。 基数排序按照优先从高位或低位来排序有两种实现方案： MSD（Most significant digital） 从最左侧高位开始进行排序。先按k1排序分组, 同一组中记录, 关键码k1相等, 再对各组按k2排序分成子组, 之后, 对后面的关键码继续这样的排序分组, 直到按最次位关键码kd对各子组排序后. 再将各组连接起来, 便得到一个有序序列。MSD方式适用于位数多的序列。 LSD （Least significant digital）从最右侧低位开始进行排序。先从kd开始排序，再对kd-1进行排序，依次重复，直到对k1排序后便得到一个有序序列。LSD方式适用于位数少的序列。 算法描述我们以LSD为例，从最低位开始，具体算法描述如下： 取得数组中的最大数，并取得位数； arr为原始数组，从最低位开始取每个位组成radix数组； 对radix进行计数排序（利用计数排序适用于小范围数的特点）； 代码实现基数排序：通过序列中各个元素的值，对排序的N个元素进行若干趟的“分配”与“收集”来实现排序。 分配：我们将L[i]中的元素取出，首先确定其个位上的数字，根据该数字分配到与之序号相同的桶中 收集：当序列中所有的元素都分配到对应的桶中，再按照顺序依次将桶中的元素收集形成新的一个待排序列L[]。对新形成的序列L[]重复执行分配和收集元素中的十位、百位…直到分配完该序列中的最高位，则排序结束 public static void sort(int[] arr) &#123; if (arr.length &lt;= 1) return; //取得数组中的最大数，并取得位数 int max = 0; for (int i = 0; i &lt; arr.length; i++) &#123; if (max &lt; arr[i]) &#123; max = arr[i]; &#125; &#125; int maxDigit = 1; while (max / 10 &gt; 0) &#123; maxDigit++; max = max / 10; &#125; //申请一个桶空间 int[][] buckets = new int[10][arr.length]; int base = 10; //从低位到高位，对每一位遍历，将所有元素分配到桶中 for (int i = 0; i &lt; maxDigit; i++) &#123; int[] bktLen = new int[10]; //存储各个桶中存储元素的数量 //分配：将所有元素分配到桶中 for (int j = 0; j &lt; arr.length; j++) &#123; int whichBucket = (arr[j] % base) / (base / 10); buckets[whichBucket][bktLen[whichBucket]] = arr[j]; bktLen[whichBucket]++; &#125; //收集：将不同桶里数据挨个捞出来,为下一轮高位排序做准备,由于靠近桶底的元素排名靠前,因此从桶底先捞 int k = 0; for (int b = 0; b &lt; buckets.length; b++) &#123; for (int p = 0; p &lt; bktLen[b]; p++) &#123; arr[k++] = buckets[b][p]; &#125; &#125; System.out.println("Sorting: " + Arrays.toString(arr)); base *= 10; &#125;&#125; 复杂度分析以下是基数排序算法复杂度，其中k为最大数的位数： 平均时间复杂度 最好情况 最坏情况 空间复杂度 O(d*(n+r)) O(d*(n+r)) O(d*(n+r)) O(n+r) 其中，d 为位数，r 为基数，n 为原数组个数。在基数排序中，因为没有比较操作，所以在复杂上，最好的情况与最坏的情况在时间上是一致的，均为 O(d*(n + r))。 总结和思考基数排序更适合用于对时间, 字符串等这些 整体权值未知的数据 进行排序。 基数排序不改变相同元素之间的相对顺序，因此它是稳定的排序算法。 基数排序 vs 计数排序 vs 桶排序 这三种排序算法都利用了桶的概念，但对桶的使用方法上有明显差异： 基数排序：根据键值的每位数字来分配桶 计数排序：每个桶只存储单一键值 桶排序：每个桶存储一定范围的数值 八大排序算法总结各种排序性能对比如下: 排序类型 平均情况 最好情况 最坏情况 辅助空间 稳定性 冒泡排序 O(n²) O(n) O(n²) O(1) 稳定 选择排序 O(n²) O(n²) O(n²) O(1) 不稳定 直接插入排序 O(n²) O(n) O(n²) O(1) 稳定 折半插入排序 O(n²) O(n) O(n²) O(1) 稳定 希尔排序 O(n^1.3) O(nlogn) O(n²) O(1) 不稳定 归并排序 O(nlog₂n) O(nlog₂n) O(nlog₂n) O(n) 稳定 快速排序 O(nlog₂n) O(nlog₂n) O(n²) O(nlog₂n) 不稳定 堆排序 O(nlog₂n) O(nlog₂n) O(nlog₂n) O(1) 不稳定 计数排序 O(n+k) O(n+k) O(n+k) O(k) 稳定 桶排序 O(n+k) O(n+k) O(n²) O(n+k) (不)稳定 基数排序 O(d(n+k)) O(d(n+k)) O(d(n+kd)) O(n+kd) 稳定 从时间复杂度来说： 平方阶O(n²)排序：各类简单排序：直接插入、直接选择和冒泡排序 线性对数阶O(nlog₂n)排序：快速排序、堆排序和归并排序 O(n1+§))排序，§是介于0和1之间的常数：希尔排序 线性阶O(n)排序：基数排序，此外还有桶、箱排序 论是否有序的影响： 当原表有序或基本有序时，直接插入排序和冒泡排序将大大减少比较次数和移动记录的次数，时间复杂度可降至O（n）； 而快速排序则相反，当原表基本有序时，将蜕化为冒泡排序，时间复杂度提高为O（n2）； 原表是否有序，对简单选择排序、堆排序、归并排序和基数排序的时间复杂度影响不大。 代码地址 参考资料： 《算法》第四版 维基百科 八大排序算法总结与java实现 前端面试必备——十大经典排序算法 必须知道的八大种排序算法【java实现】]]></content>
      <categories>
        <category>算法</category>
      </categories>
      <tags>
        <tag>排序</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Java实现二分查找算法]]></title>
    <url>%2FJava%E5%AE%9E%E7%8E%B0%E4%BA%8C%E5%88%86%E6%9F%A5%E6%89%BE%E7%AE%97%E6%B3%95%2F</url>
    <content type="text"><![CDATA[二分查找（binary search），也称折半搜索，是一种在 有序数组 中 查找某一特定元素 的搜索算法。搜索过程从数组的中间元素开始，如果中间元素正好是要查找的元素，则搜索过程结束；如果某一特定元素大于或者小于中间元素，则在数组大于或小于中间元素的那一半中查找，而且跟开始一样从中间元素开始比较。如果在某一步骤数组为空，则代表找不到。这种搜索算法每一次比较都使搜索范围缩小一半。 时间复杂度:折半搜索每次把搜索区域减少一半，时间复杂度为O(log n)。（n代表集合中元素的个数） 空间复杂度: O(1)。虽以递归形式定义，但是尾递归，可改写为循环。 动图演示 代码描述递归int binarysearch(int array[], int low, int high, int target) &#123; if (low &gt; high) return -1; int mid = low + (high - low) / 2; if (array[mid] &gt; target) return binarysearch(array, low, mid - 1, target); if (array[mid] &lt; target) return binarysearch(array, mid + 1, high, target); return mid;&#125; 非递归int bsearchWithoutRecursion(int a[], int key) &#123; int low = 0; int high = a.length - 1; while (low &lt;= high) &#123; int mid = low + (high - low) / 2; if (a[mid] &gt; key) high = mid - 1; else if (a[mid] &lt; key) low = mid + 1; else return mid; &#125; return -1;&#125; 二分查找中值的计算这是一个经典的话题，如何计算二分查找中的中值？大家一般给出了两种计算方法： 算法一： mid = (low + high) / 2 算法二： mid = low + (high – low)/2 乍看起来，算法一简洁，算法二提取之后，跟算法一没有什么区别。但是实际上，区别是存在的。算法一的做法，在极端情况下，(low + high)存在着溢出的风险，进而得到错误的mid结果，导致程序错误。而算法二能够保证计算出来的mid，一定大于low，小于high，不存在溢出的问题。 二分查找法的缺陷二分查找法的O(log n)让它成为十分高效的算法。不过它的缺陷却也是那么明显的。就在它的限定之上：必须有序，我们很难保证我们的数组都是有序的。当然可以在构建数组的时候进行排序，可是又落到了第二个瓶颈上：它必须是数组。 数组读取效率是O(1)，可是它的插入和删除某个元素的效率却是O(n)。因而导致构建有序数组变成低效的事情。 解决这些缺陷问题更好的方法应该是使用二叉查找树了，最好自然是自平衡二叉查找树了，既能高效的（O(n log n)）构建有序元素集合，又能如同二分查找法一样快速（O(log n)）的搜寻目标数。 参考资料： 二分查找法的实现和应用汇总 二分查找(Binary Search)需要注意的问题，以及在数据库内核中的实现]]></content>
      <categories>
        <category>算法</category>
      </categories>
      <tags>
        <tag>查找</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Win10下通过IIS调试ASP程序遇到的问题和解决方案]]></title>
    <url>%2FWin10-through-the-IIS-debugging-ASP-program%2F</url>
    <content type="text"><![CDATA[最近维护了以前别人的写的一个ASP的系统，记录一下调试过程中的问题和解决方案。 环境篇万维网发布服务(W3SVC)已经停止问题：万维网发布服务(W3SVC)已经停止。除非万维网发布服务(W3SVC)正在运行,否则无法启动网站。 解决方法： 需要先启动整个应用。 IIS服务控制面板&gt;&gt;程序和功能&gt;&gt;启动或关闭Windows功能&gt;&gt;IIS服务 但是这样仅仅是开启了IIS服务，会出现Http500错误，不能运行ASP程序，因为IIS服务器默认并没有帮我们配置ASP或者ASP.NET环境，需要自己手动配置(在此过程中，我启动过多次电脑)。 配置ASP环境ASP配置如下： 如果需要ASP.NET，需要如下配置： IIS7中出现An error occurred on the server when processing the URL错误错误描述：An error occurred on the server when processing the URL. Please contact the system administrator.If you are the system administrator please click here to find out more about this error. 打开控制面板→管理工具→Internet 信息服务(IIS)管理器→双击“ASP”图标 在左边的窗口中找到你的网站，然后在右边的窗口中展开“调试属性”，把“将错误发送到浏览器”设为True即可 此时你再运行ASP程序时就会看到具体的错误了，然后再根据错误提示进行相应的修改即可。 代码篇ADODB.Connection 错误 ‘800a0e7a’具体错误：ADODB.Connection 错误 ‘800a0e7a’未找到提供程序。该程序可能未正确安装。 原因： 因为系统是64位的win10，所以会出现这个问题。 解决办法：找到IIS应用程序池，“设置应用程序池默认属性”-&gt;“常规”-&gt;”启用 32 位应用程序”，设置为 True。 height=”100%” width=”100%” style=”width:757px; height:455px;”这样问题就解决了。 ADODB.Recordset 错误 ‘800a0cc1’描述：ADODB.Recordset 错误 ‘800a0cc1’在对应所需名称或序数的集合中，未找到项目。 解决：一般是字段写错了或者，你的数据库没有这个字段。 iframe自适应JS代码：//iframe高度自适应function IFrameReSize(iframename) &#123; var pTar = document.getElementByIdx_x_x(iframename); if (pTar) &#123; //ff if (pTar.contentDocument &amp;&amp; pTar.contentDocument.body.offsetHeight) &#123; pTar.height = pTar.contentDocument.body.offsetHeight; &#125; //ie else if (pTar.Document &amp;&amp; pTar.Document.body.scrollHeight) &#123; pTar.height = pTar.Document.body.scrollHeight; &#125; &#125;&#125;//iframe宽度自适应function IFrameReSizeWidth(iframename) &#123; var pTar = document.getElementByIdx_x_x(iframename); if (pTar) &#123; //ff if (pTar.contentDocument &amp;&amp; pTar.contentDocument.body.offsetWidth) &#123; pTar.width = pTar.contentDocument.body.offsetWidth; &#125; //ie else if (pTar.Document &amp;&amp; pTar.Document.body.scrollWidth) &#123; pTar.width = pTar.Document.body.scrollWidth; &#125; &#125;&#125; Iframe框配置：&lt;iframe src="Main.htm" scrolling="no" frameborder="0" height="100%"id="mainFrame" width="100%" onload='IFrameReSize("mainFrame");IFrameReSizeWidth("mainFrame");'&gt;&lt;/iframe&gt; ACCESS分页select * from news where nid between(SELECT min(nid) from(select top 4 nid from newsdata order by nid desc))and(SELECT min(nid) from(select top 1 nid from newsdata order by nid desc))order by nid desc 利用top和min函数分别找出分页的起始ID和结束ID，如果要按照升序排列，就要用top和max来找出起始ID和结束ID，之后在使用between语句直接选取。注意三个地方的排序方式必须一致，查询条件也必须一致。 参考文档： 简单又高效的Access分页语句]]></content>
      <categories>
        <category>服务器</category>
      </categories>
      <tags>
        <tag>IIS</tag>
        <tag>ASP</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Nginx几种负载均衡算法及配置实例]]></title>
    <url>%2FNginx-several-load-balancing-algorithm%2F</url>
    <content type="text"><![CDATA[本文装载自： https://yq.aliyun.com/articles/114683 Nginx负载均衡（工作在七层“应用层”）功能主要是通过upstream模块实现，Nginx负载均衡默认对后端服务器有健康检测的能力，仅限于端口检测，在后端服务器比较少的情况下负载均衡能力表现突出。 Nginx的几种负载均衡算法： 轮询（默认）：每个请求按时间顺序逐一分配到不同的后端服务器，如果后端某台服务器宕机，则自动剔除故障机器，使用户访问不受影响。 weight：指定轮询权重，weight值越大，分配到的几率就越高，主要用于后端每台服务器性能不均衡的情况。 ip_hash：每个请求按访问IP的哈希结果分配，这样每个访客固定访问一个后端服务器，可以有效的解决动态网页存在的session共享问题。 fair（第三方）：更智能的一个负载均衡算法，此算法可以根据页面大小和加载时间长短智能地进行负载均衡，也就是根据后端服务器的响应时间来分配请求，响应时间短的优先分配。如果想要使用此调度算法，需要Nginx的upstream_fair模块。 url_hash（第三方）：按访问URL的哈希结果来分配请求，使每个URL定向到同一台后端服务器，可以进一步提高后端缓存服务器的效率。如果想要使用此调度算法，需要Nginx的hash软件包。 在upstream模块中，可以通过server命令指定后端服务器的IP地址和端口，同时还可以设置每台后端服务器在负载均衡调度中的状态，常用的状态有以下几种： down：表示当前server暂时不参与负载均衡。 backup：预留的备份机，当其他所有非backup机器出现故障或者繁忙的时候，才会请求backup机器，这台机器的访问压力最轻。 max_fails：允许请求的失败次数，默认为1，配合fail_timeout一起使用 fail_timeout：经历max_fails次失败后，暂停服务的时间，默认为10s（某个server连接失败了max_fails次，则nginx会认为该server不工作了。同时，在接下来的 fail_timeout时间内，nginx不再将请求分发给失效的server。） 下面是一个负载均衡的配置示例，这里只列出http配置段，省略了其他部分配置： http &#123; upstream whsirserver &#123; server 192.168.0.120:80 weight=5 max_fails=3 fail_timeout=20s; server 192.168.0.121:80 weight=1 max_fails=3 fail_timeout=20s; server 192.168.0.122:80 weight=3 max_fails=3 fail_timeout=20s; server 192.168.0.123:80 weight=4 max_fails=3 fail_timeout=20s; &#125; server &#123; listen 80; server_name blog.whsir.com; index index.html index.htm; root /data/www; location / &#123; proxy_pass http://whsirserver; proxy_next_upstream http_500 http_502 error timeout invalid_header; &#125; &#125;&#125; upstream负载均衡开始，通过upstream指定了一个负载均衡器的名称为whsirserver，这个名称可以自己定义，在后面proxy_pass直接调用即可。 proxy_next_upstream参数用来定义故障转移策略，当后端服务器节点返回500、502和执行超时等错误时，自动将请求转发到upstream负载均衡器中的另一台服务器，实现故障转移。]]></content>
      <categories>
        <category>服务器</category>
      </categories>
      <tags>
        <tag>Nginx</tag>
        <tag>负载均衡</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Java解决CSRF问题]]></title>
    <url>%2FJava%E8%A7%A3%E5%86%B3CSRF%E9%97%AE%E9%A2%98%2F</url>
    <content type="text"><![CDATA[项目地址： https://github.com/morethink/web-security-csrf CSRF是什么？CSRF（Cross-site request forgery），中文名称：跨站请求伪造，也被称为：one click attack/session riding，缩写为：CSRF/XSRF。 CSRF可以做什么？你这可以这么理解CSRF攻击：攻击者盗用了你的身份，以你的名义发送恶意请求。CSRF能够做的事情包括：以你名义发送邮件，发消息，盗取你的账号，甚至于购买商品，虚拟货币转账……造成的问题包括：个人隐私泄露以及财产安全。 CSRF的原理下图简单阐述了CSRF攻击的思 从上图可以看出，要完成一次CSRF攻击，受害者必须依次完成两个步骤： 登录受信任网站A，并在本地生成Cookie。 在不登出A的情况下，访问危险网站B。 看到这里，你也许会说：“如果我不满足以上两个条件中的一个，我就不会受到CSRF的攻击”。是的，确实如此，但你不能保证以下情况不会发生： 你不能保证你登录了一个网站后，不再打开一个tab页面并访问另外的网站。 你不能保证你关闭浏览器了后，你本地的Cookie立刻过期，你上次的会话已经结束。（事实上，关闭浏览器不能结束一个会话，但大多数人都会错误的认为关闭浏览器就等于退出登录/结束会话了……） 上图中所谓的攻击网站，可能是一个存在其他漏洞的可信任的经常被人访问的网站。 下面讲一讲java解决CSRF攻击的方式。 模拟CSRF攻击登录A网站用户名和密码都是admin。 http://localhost:8081/login.html: 你有权限删除1号帖子http://localhost:8081/deletePost.html: 登录有CSRF攻击A网站的B网站http://localhost:8082/deletePost.html: 明显看到B网站是8082端口，A网站是8081端口，但是B网站的删除2号帖子功能依然实现。 预防CSRF攻击简单来说，CSRF 就是网站 A 对用户建立信任关系后，在网站 B 上利用这种信任关系，跨站点向网站 A 发起一些伪造的用户操作请求，以达到攻击的目的。 而之所以可以完成攻击是因为B向A发起攻击的时候会把A网站的cookie带给A网站，也就是说cookie已经不安全了。 通过Synchronizer TokensSynchronizer Tokens： 在表单里隐藏一个随机变化的 csrf_token csrf_token 提交到后台进行验证，如果验证通过则可以继续执行操作。这种情况有效的主要原因是网站 B 拿不到网站 A 表单里的 csrf_token 这种方式的使用条件是PHP和JSP等。因为cookie已经不安全了，因此把csrf_token值存储在session中，然后每次表单提交时都从session取出来放到form表单的隐藏域中，这样B网站不可以得到这个存储到session中的值。 下面是JSP的：&lt;input type="hidden" name="random_form" value=&lt;%=random%&gt;&gt;&lt;/input&gt; 但是我现在的情况是html，不是JSP，并不能动态的从session中取出csrf_token值。只能采用加密的方式了。 Hash加密cookie中csrf_token值这可能是最简单的解决方案了，因为攻击者不能获得第三方的Cookie(理论上)，所以表单中的数据也就构造失败了。 我采用的hash加密方法是JS实现Java的HashCode方法，得到hash值，这个比较简单。也可以采用其他的hash算法。 前端向后台传递hash之后的csrf_token值和cookie中的csrf_token值，后台拿到cookie中的csrf_token值后得到hashCode值然后与前端传过来的值进行比较，一样则通过。 你有权限删除3号帖子http://localhost:8081/deletePost.html B网站的他已经没有权限了我们通过UserFilter.java给攻击者返回的是403错误，表示服务器理解用户客户端的请求但拒绝处理。 http://localhost:8082/deletePost.html: 攻击者不能删除4号帖子。 前端代码： deletePost.html &lt;!DOCTYPE html&gt;&lt;html lang="en"&gt;&lt;head&gt; &lt;meta charset="UTF-8"&gt; &lt;title&gt;deletePost&lt;/title&gt; &lt;script type="text/javascript" src="js/jquery.min.js"&gt;&lt;/script&gt; &lt;script type="text/javascript"&gt; function deletePost() &#123; var url = '/post/' + document.getElementById("postId").value; var csrf_token = document.cookie.replace(/(?:(?:^|.*;\s*)csrf_token\s*\=\s*([^;]*).*$)|^.*$/, "$1"); console.log('csrf_token=' + csrf_token); $.ajax(&#123; type: "post",//请求方式 url: url, //发送请求地址 timeout: 30000,//超时时间：30秒 data: &#123; "_method": "delete", "csrf_token": hash(csrf_token) // 对csrf_token进行hash加密 &#125;, dataType: "json",//设置返回数据的格式 success: function (result) &#123; if (result.message == "success") &#123;// window.location.href = "index.html"; $("#result").text("删除成功"); &#125; else &#123; $("#result").text("删除失败"); &#125; &#125;, error: function () &#123; //请求出错的处理 $("#result").text("请求出错"); &#125; &#125;); &#125; // javascript的String到int(32位)的hash算法 function hash(str) &#123; var hash = 0; if (str.length == 0) return hash; for (i = 0; i &lt; str.length; i++) &#123; char = str.charCodeAt(i); hash = ((hash &lt;&lt; 5) - hash) + char; hash = hash &amp; hash; // Convert to 32bit integer &#125; return hash; &#125; &lt;/script&gt;&lt;/head&gt;&lt;body&gt;&lt;h3&gt;删除帖子&lt;/h3&gt;帖子编号 ： &lt;input type="text" id="postId"/&gt;&lt;button onclick="deletePost();"&gt;deletePost&lt;/button&gt;&lt;br/&gt;&lt;br/&gt;&lt;br/&gt;&lt;div&gt; &lt;p id="result"&gt;&lt;/p&gt;&lt;/div&gt;&lt;/body&gt;&lt;/html&gt; 后台代码： UserInterceptor.java package cn.morethink.interceptor;import cn.morethink.util.JsonUtil;import cn.morethink.util.Result;import org.springframework.web.servlet.HandlerInterceptor;import org.springframework.web.servlet.ModelAndView;import javax.servlet.http.Cookie;import javax.servlet.http.HttpServletRequest;import javax.servlet.http.HttpServletResponse;import java.io.PrintWriter;/** * @author 李文浩 * @date 2018/1/4 */public class UserInterceptor implements HandlerInterceptor &#123; @Override public boolean preHandle(HttpServletRequest request, HttpServletResponse response, Object handler) throws Exception &#123; String method = request.getMethod(); System.out.println(method); if (method.equalsIgnoreCase("POST") || method.equalsIgnoreCase("DELETE") || method.equalsIgnoreCase("PUT")) &#123; String csrf_token = request.getParameter("csrf_token"); System.out.println(csrf_token + "1222222222222222222222222222222222222222222222"); Cookie[] cookies = request.getCookies(); if (cookies != null &amp;&amp; cookies.length &gt; 0 &amp;&amp; csrf_token != null) &#123; for (Cookie cookie : cookies) &#123; if (cookie.getName().equals("csrf_token")) &#123; if (Integer.valueOf(csrf_token) == cookie.getValue().hashCode()) &#123; return true; &#125; &#125; &#125; &#125; &#125; Result result = new Result("403", "你还想攻击我??????????", ""); PrintWriter out = response.getWriter(); out.write(JsonUtil.toJson(result)); out.close(); return false; &#125; @Override public void postHandle(HttpServletRequest request, HttpServletResponse response, Object handler, ModelAndView modelAndView) throws Exception &#123; &#125; @Override public void afterCompletion(HttpServletRequest request, HttpServletResponse response, Object handler, Exception ex) throws Exception &#123; &#125;&#125; 注意： cookie必须要设置PATH才可以生效，否则在下一次请求的时候无法带给服务器。 Spring Boot 出现启动找不到主类的问题时可以mvn clean一下。 Filter设置response.sendError(403)在Spring Boot没有效果。 参考文档： 浅谈CSRF攻击方式 jQueue 动态设置form表单的action属性的值和方法 javascript的String到int(32位)的hash算法]]></content>
      <categories>
        <category>Web安全</category>
      </categories>
      <tags>
        <tag>CSRF</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Nginx Https配置不带www跳转www]]></title>
    <url>%2FNginx-Https-no-www-jump-www%2F</url>
    <content type="text"><![CDATA[把 morethink.cn和www.morethink.cn合并到一个server上去，使用301永久重定向。然后将 https://morethink.cn 转到 https://www.morethink.cn 去。不过要在配置文件的 server https://www.morethink.cn上配置default_server ssl;。301永久重定向可以把搜索引擎的权重全部集中到 https://www.morethink.cn 上。 配置如下： server &#123; listen 80; server_name morethink.cn,www.morethink.cn; return 301 https://www.morethink.cn$request_uri;&#125;server &#123; listen 443; server_name morethink.cn; return 301 https://www.morethink.cn$request_uri;&#125;server &#123; listen 443 default_server ssl; server_name www.morethink.cn; ssl on; ssl_certificate 1_www.morethink.cn_bundle.crt; ssl_certificate_key 2_www.morethink.cn.key; ssl_session_timeout 5m; ssl_protocols TLSv1 TLSv1.1 TLSv1.2; ssl_ciphers ECDHE-RSA-AES128-GCM-SHA256:HIGH:!aNULL:!MD5:!RC4:!DHE; ssl_prefer_server_ciphers on; root /var/www/hexo; include /etc/nginx/default.d/*.conf; error_page 404 /404.html; location = /40x.html &#123; &#125; error_page 500 502 503 504 /50x.html; location = /50x.html &#123; &#125;&#125; 参考文档： 腾讯云 Nginx Https 证书安装指引 nginx配置http强制跳转https]]></content>
      <categories>
        <category>服务器</category>
      </categories>
      <tags>
        <tag>Https</tag>
        <tag>Nginx</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[面试中的Https]]></title>
    <url>%2FHttps%2F</url>
    <content type="text"><![CDATA[在Http协议中有可能存在信息窃听或身份伪装的安全问题。使用HTTPS通信机制可以有效地防止这些问题。 HttpsHttp的缺点 通信使用明文(不加密)，内容可能会被窃听。 不验证通信方的身份，因此有可能遭遇伪装。 无法验证报文的完整性，所以有可能已遭篡改。 这些问题不仅在Http上出现，其他未加密的协议中也会存在这类问题。 什么是HttpsHttps并非是应用层的一种新的协议。只是Http通信接口部分用SSL(Secure Socket Layer)和TLS(Transport Layer Security)协议而已。 通常，Http直接和TCP通信。当使用SSL时，则演变成先和SSL协议通信，再由SSL和TCP通信了。简言之，所谓Https，就是身披SSL这层协议外壳的Http。 Https有什么作用Http+加密+认证+完整性保护 = Https Https有以下作用： 内容加密 建立一个信息安全通道，来保证数据传输的安全。 身份认证 确认网站的真实性。 数据完整性 防止内容被第三方冒充或者篡改。 下面就是Https的整个架构，现在的https基本都使用TLS了，因为更加安全。 Https 加密对称加密对称加密(也叫私钥加密)指加密和解密使用相同密钥的加密算法。有时又叫传统密码算法，就是加密密钥能够从解密密钥中推算出来，同时解密密钥也可以从加密密钥中推算出来。而在大多数的对称算法中，加密密钥和解密密钥是相同的，所以也称这种加密算法为秘密密钥算法或单密钥算法。 但是我们使用对称加密加密Http通信内容会有一个问题，因为客户端和服务器在通信过程中都必须知道秘钥，而在发送秘钥的过程中又有可能被第三方监听，从而获取到秘钥。 非对称加密非对称加密很好地解决了对称加密的困难。 与对称加密算法不同，非对称加密算法需要两个密钥：公开密钥（publickey）和私有密钥（privatekey），并且加密密钥和解密密钥是成对出现的。非对称加密算法在加密和解密过程使用了不同的密钥，非对称加密也称为公钥加密，在密钥对中，其中一个密钥是对外公开的，所有人都可以获取到，称为公钥，其中一个密钥是不公开的称为私钥。 用公钥加密的只能用私钥解开，用私钥加密的只能用公钥解开。 非对称加密的特性决定了服务器用私钥加密的内容并不是真正的加密，因为公钥所有人都有，所以服务器的密文能被所有人解析。但私钥只掌握在服务器手上，这就带来了两个巨大的优势: 服务器下发的内容不可能被伪造，因为别人都没有私钥，所以无法加密。强行加密的后果是客户端用公钥无法解开。 任何人用公钥加密的内容都是绝对安全的，因为私钥只有服务器有，也就是只有真正的服务器可以看到被加密的原文。 注意：想要根据密文和公钥，恢复信息原文是异常困难的，因为解密过程就是在对离散对数进行求值，这并非轻而易举就能办到。 因此，Https采用对称加密和非对称加密两者并用的混合加密机制。 也就是说，Https通过非对称加密来传递对称加密的秘钥。 那为什么不直接采用非对称加密来加密通信内容？ 非对称加密处理起来比对称加密更为复杂，因此若在通信时使用非对称加密，效率比较低。 证书的私钥加密公钥遗憾的是，非对称加密还是存在一些问题的。那就是无法保证公钥本身就是货真价实的公钥。比如，正准备和某台服务器建立非对称加密下的通信时，如何证明收到的公钥就是原来预想的那台服务器发行的公钥。或许在公开秘钥传输过程中，真正的公钥已经被人替换了。 那怎么办？再加密一次。 每一个使用 HTTPS 的服务器都必须去专门的证书机构注册一个证书，证书中存储了用数字证书机构私钥加密的公钥。这样客户端用数字证书机构的公钥解密就可以了。 而数字证书机构的公钥会直接内置在各大操作系统(或者浏览器)的出厂设置里。所以各个公司要先去数字证书机构认证，申请证书，然后操作系统只会存储数字证书机构的公钥。因为数字证书机构数量有限，所以操作系统厂商相对来说容易管理。 总结：Https通过非对称加密(通常是RSA算法)加密对称加密的秘钥，然后使用证书机构的私钥加密非对称加密的公钥，而证书机构的公钥会内置在浏览器里，从而保证即使被第三方监听，也可以保证安全。 SSL 与 TLSSSL (Secure Socket Layer，安全套接字层)SSL为Netscape所研发，用以保障在Internet上数据传输之安全，利用数据加密(Encryption)技术，可确保数据在网络上之传输过程中不会被截取，当前为3.0版本。 SSL协议可分为两层： SSL记录协议（SSL Record Protocol）：它建立在可靠的传输协议（如TCP）之上，为高层协议提供数据封装、压缩、加密等基本功能的支持。 SSL握手协议（SSL Handshake Protocol）：它建立在SSL记录协议之上，用于在实际的数据传输开始前，通讯双方进行身份认证、协商加密算法、交换加密密钥等。 TLS (Transport Layer Security，传输层安全协议)用于两个应用程序之间提供保密性和数据完整性。TLS 1.0是IETF（Internet Engineering Task Force，Internet工程任务组）制定的一种新的协议，它建立在SSL 3.0协议规范之上，是SSL 3.0的后续版本，可以理解为SSL 3.1，它是写入了 RFC 的。该协议由两层组成： TLS 记录协议（TLS Record）和 TLS 握手协议（TLS Handshake）。较低的层为 TLS 记录协议，位于某个可靠的传输协议（例如 TCP）上面。 SSL/TLS协议作用 认证用户和服务器，确保数据发送到正确的客户机和服务器； 加密数据以防止数据中途被窃取； 维护数据的完整性，确保数据在传输过程中不被改变。 TLS比SSL的优势 对于消息认证使用密钥散列法：TLS 使用“消息认证代码的密钥散列法”（HMAC），当记录在开放的网络（如因特网）上传送时，该代码确保记录不会被变更。SSLv3.0还提供键控消息认证，但HMAC比SSLv3.0使用的（消息认证代码）MAC 功能更安全。 增强的伪随机功能（PRF）：PRF生成密钥数据。在TLS中，HMAC定义PRF。PRF使用两种散列算法保证其安全性。如果任一算法暴露了，只要第二种算法未暴露，则数据仍然是安全的。 改进的已完成消息验证：TLS和SSLv3.0都对两个端点提供已完成的消息，该消息认证交换的消息没有被变更。然而，TLS将此已完成消息基于PRF和HMAC值之上，这也比SSLv3.0更安全。 一致证书处理：与SSLv3.0不同，TLS试图指定必须在TLS之间实现交换的证书类型。 特定警报消息：TLS提供更多的特定和附加警报，以指示任一会话端点检测到的问题。TLS还对何时应该发送某些警报进行记录。 SSL/TLS的握手过程SSL与TLS握手整个过程如下图所示，下面会详细介绍每一步的具体内容： 客户端首次发出请求由于客户端(如浏览器)对一些加解密算法的支持程度不一样，但是在TLS协议传输过程中必须使用同一套加解密算法才能保证数据能够正常的加解密。在TLS握手阶段，客户端首先要告知服务端，自己支持哪些加密算法，所以客户端需要将本地支持的加密套件(Cipher Suite)的列表传送给服务端。除此之外，客户端还要产生一个随机数，这个随机数一方面需要在客户端保存，另一方面需要传送给服务端，客户端的随机数需要跟服务端产生的随机数结合起来产生后面要讲到的 Master Secret 。 客户端需要提供如下信息： 支持的协议版本，比如TLS 1.0版 一个客户端生成的随机数，稍后用于生成”对话密钥” 支持的加密方法，比如RSA公钥加密 支持的压缩方法 服务端首次回应服务端在接收到客户端的Client Hello之后，服务端需要确定加密协议的版本，以及加密的算法，然后也生成一个随机数，以及将自己的证书发送给客户端一并发送给客户端，这里的随机数是整个过程的第二个随机数。 服务端需要提供的信息： 协议的版本 加密的算法 随机数 服务器证书 客户端再次回应客户端首先会对服务器下发的证书进行验证，验证通过之后，则会继续下面的操作，客户端再次产生一个随机数（第三个随机数），然后使用服务器证书中的公钥进行加密，以及放一个ChangeCipherSpec消息即编码改变的消息，还有整个前面所有消息的hash值，进行服务器验证，然后用新秘钥加密一段数据一并发送到服务器，确保正式通信前无误。 客户端使用前面的两个随机数以及刚刚新生成的新随机数，使用与服务器确定的加密算法，生成一个Session Secret。 服务器再次响应服务端在接收到客户端传过来的第三个随机数的 加密数据之后，使用私钥对这段加密数据进行解密，并对数据进行验证，也会使用跟客户端同样的方式生成秘钥，一切准备好之后，也会给客户端发送一个 ChangeCipherSpec，告知客户端已经切换到协商过的加密套件状态，准备使用加密套件和 Session Secret加密数据了。之后，服务端也会使用 Session Secret 加密一段 Finish 消息发送给客户端，以验证之前通过握手建立起来的加解密通道是否成功。 后续客户端与服务器间通信确定秘钥之后，服务器与客户端之间就会通过商定的秘钥加密消息了，进行通讯了。整个握手过程也就基本完成了。 值得特别提出的是： SSL协议在握手阶段使用的是非对称加密，在传输阶段使用的是对称加密，也就是说在SSL上传送的数据是使用对称密钥加密的！因为非对称加密的速度缓慢，耗费资源。其实当客户端和主机使用非对称加密方式建立连接后，客户端和主机已经决定好了在传输过程使用的对称加密算法和关键的对称加密密钥，由于这个过程本身是安全可靠的，也即对称加密密钥是不可能被窃取盗用的，因此，保证了在传输过程中对数据进行对称加密也是安全可靠的，因为除了客户端和主机之外，不可能有第三方窃取并解密出对称加密密钥！如果有人窃听通信，他可以知道双方选择的加密方法，以及三个随机数中的两个。整个通话的安全，只取决于第三个随机数（Premaster secret）能不能被破解。 session的恢复有两种方法可以恢复原来的session：一种叫做session ID，另一种叫做session ticket。 session IDsession ID的思想很简单，就是每一次对话都有一个编号（session ID）。如果对话中断，下次重连的时候，只要客户端给出这个编号，且服务器有这个编号的记录，双方就可以重新使用已有的”对话密钥”，而不必重新生成一把。 session ID是目前所有浏览器都支持的方法，但是它的缺点在于session ID往往只保留在一台服务器上。所以，如果客户端的请求发到另一台服务器，就无法恢复对话。 session ticket客户端发送一个服务器在上一次对话中发送过来的session ticket。这个session ticket是加密的，只有服务器才能解密，其中包括本次对话的主要信息，比如对话密钥和加密方法。当服务器收到session ticket以后，解密后就不必重新生成对话密钥了。 目前只有Firefox和Chrome浏览器支持。 Https的劣势对数据进行加解密决定了它比http慢。需要进行非对称的加解密，且需要三次握手。首次连接比较慢点，当然现在也有很多的优化。 出于安全考虑，浏览器不会在本地保存HTTPS缓存。实际上，只要在HTTP头中使用特定命令，HTTPS是可以缓存的。Firefox默认只在内存中缓存HTTPS。但是，只要头命令中有Cache-Control: Public，缓存就会被写到硬盘上。 IE只要http头允许就可以缓存https内容，缓存策略与是否使用HTTPS协议无关。 HTTPS和HTTP的区别 https协议需要到CA申请证书。 http是超文本传输协议，信息是明文传输；https 则是具有安全性的ssl加密传输协议。 http和https使用的是完全不同的连接方式，用的端口也不一样，前者是80，后者是443。 http的连接很简单，是无状态的；HTTPS协议是由SSL+HTTP协议构建的可进行加密传输、身份认证的网络协议，比http协议安全。 http默认使用80端口，https默认使用443端口 Python等爬虫怎么处理Httpshttps拿爬虫毫无办法，或者说https就不是为了反爬虫的。https的作用是保证服务源授信。比如你访问支付宝，网络被劫持了，你看到的就是个假网站，一旦你登录，账号就泄露了。但用https就能保证你访问的一定是真的支付宝网站，这是由CA证书保证的。回过来再说爬虫，爬虫伪造的是客户端，https是不能保证客户端是授信的，你只要按照ssl协议进行通信，该怎么爬数据还是怎么爬。 https协议里数据的传输是需要经过加密的,在这个过程中，就给爬虫带来了抓包问题，抓出来的数据也是经过加密的，不能解析。 理论上是不行了，因为https保证的就是数据在传输过程中不会被盗取。但解决起来也很简单，就是设置个代理伪装一下，代价就是你要安装个假证书，当然这也肯定是无所谓的。 参考文档： 详解https是如何确保安全的？ 图解SSL/TLS协议 九个问题从入门到熟悉HTTPS SSL/TLS协议运行机制的概述 爬虫 https python？]]></content>
      <categories>
        <category>网络</category>
      </categories>
      <tags>
        <tag>Https</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[面试中的DNS]]></title>
    <url>%2FDNS%2F</url>
    <content type="text"><![CDATA[DNS当DNS客户机需要在程序中使用名称时，它会查询DNS服务器来解析该名称。客户机发送的每条查询信息包括三条信息：指定的DNS域名，指定的查询类型，DNS域名的指定类别。 DNS基于UDP服务，端口53。该应用一般不直接为用户使用，而是为其他应用服务，如HTTP，SMTP等在其中需要完成主机名到IP地址的转换。 递归查询与迭代查询递归查询递归查询是一种DNS 服务器的查询模式，在该模式下DNS 服务器接收到客户机请求，必须使用一个准确的查询结果回复客户机。如果DNS 服务器本地没有存储查询DNS 信息，那么该服务器会询问其他服务器，并将返回的查询结果提交给客户机。 迭代查询DNS 服务器另外一种查询方式为迭代查询，DNS 服务器会向客户机提供其他能够解析查询请求的DNS 服务器地址，当客户机发送查询请求时，DNS 服务器并不直接回复查询结果，而是告诉客户机另一台DNS 服务器地址，客户机再向这台DNS 服务器提交请求，依次循环直到返回查询的结果为止。 从客户端到本地DNS服务器是属于递归查询，而DNS服务器之间就是的交互查询就是迭代查询。 DNS劫持DNS劫持就是通过劫持了DNS服务器，通过某些手段取得某域名的解析记录控制权，进而修改此域名的解析结果，导致对该域名的访问由原IP地址转入到修改后的指定IP，其结果就是对特定的网址不能访问或访问的是假网址，从而实现窃取资料或者破坏原有正常服务的目的。DNS劫持通过篡改DNS服务器上的数据返回给用户一个错误的查询结果来实现的。 DNS劫持症状：在某些地区的用户在成功连接宽带后，首次打开任何页面都指向ISP提供的“电信互联星空”、“网通黄页广告”等内容页面。还有就是曾经出现过用户访问Google域名的时候出现了百度的网站。这些都属于DNS劫持。 解决方法 对于DNS劫持，可以采用使用国外免费公用的DNS服务器解决。例如OpenDNS（208.67.222.222）或GoogleDNS（8.8.8.8）。 总结 DNS劫持就是指用户访问一个被标记的地址时，DNS服务器故意将此地址指向一个错误的IP地址的行为。范例，网通、电信、铁通的某些用户有时候会发现自己打算访问一个地址，却被转向了各种推送广告等网站，这就是DNS劫持。 DNS污染DNS污染是一种让一般用户由于得到虚假目标主机IP而不能与其通信的方法，是一种DNS缓存投毒攻击（DNS cache poisoning）。 其工作方式是：由于通常的DNS查询没有任何认证机制，而且DNS查询通常基于的UDP是无连接不可靠的协议，因此DNS的查询非常容易被篡改，通过对UDP端口53上的DNS查询进行入侵检测，一经发现与关键词相匹配的请求则立即伪装成目标域名的解析服务器（NS，Name Server）给查询者返回虚假结果。 DNS污染发生在用户请求的第一步上，直接从协议上对用户的DNS请求进行干扰。DNS污染症状：目前一些被禁止访问的网站很多就是通过DNS污染来实现的，例如YouTube、Facebook等网站。 解决办法： 对于DNS污染，可以说，个人用户很难单单靠设置解决，通常可以使用VPN或者域名远程解析的方法解决，但这大多需要购买付费的VPN或SSH等，也可以通过修改Hosts的方法，手动设置域名正确的IP地址。 总结： DNS污染，指的是用户访问一个地址，国内的服务器(非DNS)监控到用户访问的已经被标记地址时，服务器伪装成DNS服务器向用户发回错误的地址的行为。范例，访问Youtube、Facebook之类网站等出现的状况。]]></content>
      <categories>
        <category>网络</category>
      </categories>
  </entry>
  <entry>
    <title><![CDATA[Java使用Openoffice将word、ppt转换为PDF]]></title>
    <url>%2FJava-openoffice-pdf%2F</url>
    <content type="text"><![CDATA[最近项目中要实现WORD的文件预览功能，我们可以通过将WORD转换成PDF或者HTML，然后通过浏览器预览。 OpenOfficeOpenOffice.org 是一套跨平台的办公室软件套件，能在 Windows、Linux、MacOS X (X11)、和 Solaris 等操作系统上执行。它与各个主要的办公室软件套件兼容。OpenOffice.org 是自由软件，任何人都可以免费下载、使用、及推广它。 下载地址 http://www.openoffice.org/ JodConverterjodconverter-2.2.2.zip 下载地址：http://sourceforge.net/projects/jodconverter/files/JODConverter/ Word转换启动OpenOffice的服务 进入openoffice安装目录，通过cmd启动一个soffice服务，启动的命令是soffice -headless -accept=&quot;socket,host=127.0.0.1,port=8100;urp;&quot;。 如果觉得后台运行OpenOffice服务比较麻烦，可以通过 运行代码public class PDFDemo &#123; public static boolean officeToPDF(String sourceFile, String destFile) &#123; try &#123; File inputFile = new File(sourceFile); if (!inputFile.exists()) &#123; // 找不到源文件, 则返回false return false; &#125; // 如果目标路径不存在, 则新建该路径 File outputFile = new File(destFile); if (!outputFile.getParentFile().exists()) &#123; outputFile.getParentFile().mkdirs(); &#125; //如果目标文件存在，则删除 if (outputFile.exists()) &#123; outputFile.delete(); &#125; DateFormat df = new SimpleDateFormat("yyyy-MM-dd HH:mm"); OpenOfficeConnection connection = new SocketOpenOfficeConnection("127.0.0.1", 8100); connection.connect(); //用于测试openOffice连接时间 System.out.println("连接时间:" + df.format(new Date())); DocumentConverter converter = new StreamOpenOfficeDocumentConverter( connection); converter.convert(inputFile, outputFile); //测试word转PDF的转换时间 System.out.println("转换时间:" + df.format(new Date())); connection.disconnect(); return true; &#125; catch (ConnectException e) &#123; e.printStackTrace(); System.err.println("openOffice连接失败！请检查IP,端口"); &#125; catch (Exception e) &#123; e.printStackTrace(); &#125; return false; &#125; public static void main(String[] args) &#123; officeToPDF("E:\\test.docx", "E:\\test.pdf"); &#125;&#125; Word、ppt转Html只需要将后缀名从.pdf改为.html即可。 public static void main(String[] args) &#123; officeToPDF("E:\\test.docx", "E:\\test.html");&#125; Maven配置Maven依赖 &lt;dependency&gt; &lt;groupId&gt;com.artofsolving&lt;/groupId&gt; &lt;artifactId&gt;jodconverter&lt;/artifactId&gt; &lt;version&gt;2.2.1&lt;/version&gt;&lt;/dependency&gt;&lt;dependency&gt; &lt;groupId&gt;org.openoffice&lt;/groupId&gt; &lt;artifactId&gt;jurt&lt;/artifactId&gt; &lt;version&gt;3.0.1&lt;/version&gt;&lt;/dependency&gt;&lt;dependency&gt; &lt;groupId&gt;org.openoffice&lt;/groupId&gt; &lt;artifactId&gt;ridl&lt;/artifactId&gt; &lt;version&gt;3.0.1&lt;/version&gt;&lt;/dependency&gt;&lt;dependency&gt; &lt;groupId&gt;org.openoffice&lt;/groupId&gt; &lt;artifactId&gt;juh&lt;/artifactId&gt; &lt;version&gt;3.0.1&lt;/version&gt;&lt;/dependency&gt;&lt;dependency&gt; &lt;groupId&gt;org.openoffice&lt;/groupId&gt; &lt;artifactId&gt;unoil&lt;/artifactId&gt; &lt;version&gt;3.0.1&lt;/version&gt;&lt;/dependency&gt;&lt;dependency&gt; &lt;groupId&gt;org.slf4j&lt;/groupId&gt; &lt;artifactId&gt;slf4j-jdk14&lt;/artifactId&gt; &lt;version&gt;1.4.3&lt;/version&gt;&lt;/dependency&gt; Maven只有 2.2.1版本，2.2.1版本有一个问题，那就是不兼容docx和pptx，如果你们不使用jodconverter-2.2.2 中lib，而想要使用2.2.1版本，需要修改一下 BasicDocumentFormatRegistry 类中的 getFormatByFileExtension方法： 新建包 com.artofsolving.jodconverter 新建类BasicDocumentFormatRegistry，复制下面代码 package com.artofsolving.jodconverter;/** * @author 李文浩 * @date 2017/12/25 */import java.util.ArrayList;import java.util.Iterator;import java.util.List;public class BasicDocumentFormatRegistry implements DocumentFormatRegistry &#123; private List documentFormats = new ArrayList(); public BasicDocumentFormatRegistry() &#123; &#125; public void addDocumentFormat(DocumentFormat documentFormat) &#123; this.documentFormats.add(documentFormat); &#125; protected List getDocumentFormats() &#123; return this.documentFormats; &#125; public DocumentFormat getFormatByFileExtension(String extension) &#123; if (extension == null) &#123; return null; &#125; else &#123; if (extension.indexOf("doc") &gt;= 0) &#123; extension = "doc"; &#125; if (extension.indexOf("ppt") &gt;= 0) &#123; extension = "ppt"; &#125; if (extension.indexOf("xls") &gt;= 0) &#123; extension = "xls"; &#125; String lowerExtension = extension.toLowerCase(); Iterator it = this.documentFormats.iterator(); DocumentFormat format; do &#123; if (!it.hasNext()) &#123; return null; &#125; format = (DocumentFormat)it.next(); &#125; while(!format.getFileExtension().equals(lowerExtension)); return format; &#125; &#125; public DocumentFormat getFormatByMimeType(String mimeType) &#123; Iterator it = this.documentFormats.iterator(); DocumentFormat format; do &#123; if (!it.hasNext()) &#123; return null; &#125; format = (DocumentFormat)it.next(); &#125; while(!format.getMimeType().equals(mimeType)); return format; &#125;&#125; 下面是增加的部分，仅仅增加了将docx按照doc的处理方式处理。而2.2.2版本已经默认增加了。 if (extension.indexOf("doc") &gt;= 0) &#123; extension = "doc";&#125;if (extension.indexOf("ppt") &gt;= 0) &#123; extension = "ppt";&#125;if (extension.indexOf("xls") &gt;= 0) &#123; extension = "xls";&#125; 参考文档： Java实现在线预览–openOffice实现 Java项目中使用OpenOffice转PDF java使用openoffice将office系列文档转换为PDF java 如何将 word,excel,ppt如何转pdf–jacob java 如何将 word,excel,ppt如何转pdf –openoffice (1)]]></content>
      <categories>
        <category>Java</category>
      </categories>
      <tags>
        <tag>PDF</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[单点登录原理与简单实现]]></title>
    <url>%2Fsso%2F</url>
    <content type="text"><![CDATA[本文转载自： https://www.cnblogs.com/ywlaker/p/6113927.html 作者：凌承一出处：http://www.cnblogs.com/ywlaker/声明：本文版权归作者和博客园共有，欢迎转载，但转载必须保留此段声明，并在文章页面明显位置给出原文链接，否则作者将保留追究法律责任的权利。 单系统登录机制web应用采用browser/server架构，http作为通信协议。http是无状态协议，浏览器的每一次请求，服务器会独立处理，不与之前或之后的请求产生关联，这个过程用下图说明，三次请求/响应对之间没有任何联系。 但这也同时意味着，任何用户都能通过浏览器访问服务器资源，如果想保护服务器的某些资源，必须限制浏览器请求；要限制浏览器请求，必须鉴别浏览器请求，响应合法请求，忽略非法请求；要鉴别浏览器请求，必须清楚浏览器请求状态。既然http协议无状态，那就让服务器和浏览器共同维护一个状态吧！这就是会话机制。 会话机制浏览器第一次请求服务器，服务器创建一个会话，并将会话的id作为响应的一部分发送给浏览器，浏览器存储会话id，并在后续第二次和第三次请求中带上会话id，服务器取得请求中的会话id就知道是不是同一个用户了，这个过程用下图说明，后续请求与第一次请求产生了关联。 服务器在内存中保存会话对象，浏览器怎么保存会话id呢？ 你可能会想到两种方式 请求参数 cookie 将会话id作为每一个请求的参数，服务器接收请求自然能解析参数获得会话id，并借此判断是否来自同一会话，很明显，这种方式不靠谱。那就浏览器自己来维护这个会话id吧，每次发送http请求时浏览器自动发送会话id，cookie机制正好用来做这件事。cookie是浏览器用来存储少量数据的一种机制，数据以”key/value”形式存储，浏览器发送http请求时自动附带cookie信息。 tomcat会话机制当然也实现了cookie，访问tomcat服务器时，浏览器中可以看到一个名为JSESSIONID的cookie，这就是tomcat会话机制维护的会话id，使用了cookie的请求响应过程如下图： 登录状态有了会话机制，登录状态就好明白了，我们假设浏览器第一次请求服务器需要输入用户名与密码验证身份，服务器拿到用户名密码去数据库比对，正确的话说明当前持有这个会话的用户是合法用户，应该将这个会话标记为“已授权”或者“已登录”等等之类的状态，既然是会话的状态，自然要保存在会话对象中，tomcat在会话对象中设置登录状态如下： HttpSession session = request.getSession();session.setAttribute("isLogin", true); 用户再次访问时，tomcat在会话对象中查看登录状态： HttpSession session = request.getSession();session.getAttribute("isLogin"); 实现了登录状态的浏览器请求服务器模型如下图描述： 每次请求受保护资源时都会检查会话对象中的登录状态，只有 isLogin=true 的会话才能访问，登录机制因此而实现。 多系统的复杂性web系统早已从久远的单系统发展成为如今由多系统组成的应用群，面对如此众多的系统，用户难道要一个一个登录、然后一个一个注销吗？就像下图描述的这样 web系统由单系统发展成多系统组成的应用群，复杂性应该由系统内部承担，而不是用户。无论web系统内部多么复杂，对用户而言，都是一个统一的整体，也就是说，用户访问web系统的整个应用群与访问单个系统一样，登录/注销只要一次就够了。 虽然单系统的登录解决方案很完美，但对于多系统应用群已经不再适用了，为什么呢？ 单系统登录解决方案的核心是cookie，cookie携带会话id在浏览器与服务器之间维护会话状态。但cookie是有限制的，这个限制就是cookie的域（通常对应网站的域名），浏览器发送http请求时会自动携带与该域匹配的cookie，而不是所有cookie。 子域名cookie共享完成单点登录既然这样，为什么不将web应用群中所有子系统的域名统一在一个顶级域名下，例如“*.baidu.com”，然后将它们的cookie域设置为“baidu.com”，这种做法理论上是可以的，甚至早期很多多系统登录就采用这种同域名共享cookie的方式。 然而，可行并不代表好，共享cookie的方式存在众多局限。 首先，应用群域名得统一。 其次，应用群各系统使用的技术（至少是web服务器）要相同，不然cookie的key值（tomcat为JSESSIONID）不同，无法维持会话，共享cookie的方式是无法实现跨语言技术平台登录的，比如java、php、.net系统之间。 第三，cookie本身不安全。 除上面之外，如果我们在session存放的是User对象，那么我们使用全局cookie共享JSESSIONID值，每一个子域名就可以访问同一个session，登录成功后保存一个user对象，注销后就移除这个user对象。session中的user对象必须先序列化保存到redis中，并且每次访问的时候，都需要去redis中取出session，并且重新序列化成user对象。这样会造成额外的消耗。 因此，我们需要一种全新的登录方式来实现多系统应用群的登录，这就是单点登录。 单点登录什么是单点登录？ 单点登录全称Single Sign On（以下简称SSO），是指在多系统应用群中登录一个系统，便可在其他所有系统中得到授权而无需再次登录，包括单点登录与单点注销两部分。 登录相比于单系统登录，sso需要一个独立的认证中心，只有认证中心能接受用户的用户名密码等安全信息，其他系统不提供登录入口，只接受认证中心的间接授权。间接授权通过令牌实现，sso认证中心验证用户的用户名密码没问题，创建授权令牌，在接下来的跳转过程中，授权令牌作为参数发送给各个子系统，子系统拿到令牌，即得到了授权，可以借此创建局部会话，局部会话登录方式与单系统的登录方式相同。 这个过程，也就是单点登录的原理，用下图说明： 下面对上图简要描述： 用户访问系统1的受保护资源，系统1发现用户未登录，跳转至sso认证中心，并将自己的地址作为参数。 sso认证中心发现用户未登录，将用户引导至登录页面。 用户输入用户名密码提交登录申请。 sso认证中心校验用户信息，创建用户与sso认证中心之间的会话，称为全局会话，同时创建授权令牌。 sso认证中心带着令牌跳转会最初的请求地址（系统1）。 系统1拿到令牌，去sso认证中心校验令牌是否有效。 sso认证中心校验令牌，返回有效，注册系统1。 系统1使用该令牌创建与用户的会话，称为局部会话，返回受保护资源。 用户访问系统2的受保护资源。 系统2发现用户未登录，跳转至sso认证中心，并将自己的地址作为参数。 sso认证中心发现用户已登录，跳转回系统2的地址，并附上令牌。 系统2拿到令牌，去sso认证中心校验令牌是否有效。 sso认证中心校验令牌，返回有效，注册系统2。 系统2使用该令牌创建与用户的局部会话，返回受保护资源。 用户登录成功之后，会与sso认证中心及各个子系统建立会话，用户与sso认证中心建立的会话称为全局会话，用户与各个子系统建立的会话称为局部会话，局部会话建立之后，用户访问子系统受保护资源将不再通过sso认证中心，全局会话与局部会话有如下约束关系： 局部会话存在，全局会话一定存在。 全局会话存在，局部会话不一定存在。 全局会话销毁，局部会话必须销毁。 你可以通过博客园、百度、csdn、淘宝等网站的登录过程加深对单点登录的理解，注意观察登录过程中的跳转url与参数 注销单点登录自然也要单点注销，在一个子系统中注销，所有子系统的会话都将被销毁，用下面的图来说明： sso认证中心一直监听全局会话的状态，一旦全局会话销毁，监听器将通知所有注册系统执行注销操作 下面对上图简要说明： 用户向系统1发起注销请求。 系统1根据用户与系统1建立的会话id拿到令牌，向sso认证中心发起注销请求。 sso认证中心校验令牌有效，销毁全局会话，同时取出所有用此令牌注册的系统地址。 sso认证中心向所有注册系统发起注销请求。 各注册系统接收sso认证中心的注销请求，销毁局部会话。 sso认证中心引导用户至登录页面。 部署图单点登录涉及sso认证中心与众子系统，子系统与sso认证中心需要通信以交换令牌、校验令牌及发起注销请求，因而子系统必须集成sso的客户端，sso认证中心则是sso服务端，整个单点登录过程实质是sso客户端与服务端通信的过程，用下图描述： sso认证中心与sso客户端通信方式有多种，这里以简单好用的httpClient为例，web service、rpc、restful api都可以。 实现只是简要介绍下基于java的实现过程，不提供完整源码，明白了原理，我相信你们可以自己实现。sso采用客户端/服务端架构，我们先看sso-client与sso-server要实现的功能（下面：sso认证中心=sso-server）。 sso-client 拦截子系统未登录用户请求，跳转至sso认证中心。 接收并存储sso认证中心发送的令牌。 与sso-server通信，校验令牌的有效性。 建立局部会话。 拦截用户注销请求，向sso认证中心发送注销请求。 接收sso认证中心发出的注销请求，销毁局部会话。 sso-server 验证用户的登录信息。 创建全局会话。 创建授权令牌。 与sso-client通信发送令牌。 校验sso-client令牌有效性。 系统注册。 接收sso-client注销请求，注销所有会话。 接下来，我们按照原理来一步步实现sso吧！ sso-client拦截未登录请求java拦截请求的方式有servlet、filter、listener三种方式，我们采用filter。在sso-client中新建LoginFilter.java类并实现Filter接口，在doFilter()方法中加入对未登录用户的拦截： public void doFilter(ServletRequest request, ServletResponse response, FilterChain chain) throws IOException, ServletException &#123; HttpServletRequest req = (HttpServletRequest) request; HttpServletResponse res = (HttpServletResponse) response; HttpSession session = req.getSession(); if (session.getAttribute("isLogin")) &#123; chain.doFilter(request, response); return; &#125; //跳转至sso认证中心 res.sendRedirect("sso-server-url-with-system-url");&#125; sso-server拦截未登录请求拦截从sso-client跳转至sso认证中心的未登录请求，跳转至登录页面，这个过程与sso-client完全一样。 sso-server验证用户登录信息用户在登录页面输入用户名密码，请求登录，sso认证中心校验用户信息，校验成功，将会话状态标记为“已登录”。 @RequestMapping("/login")public String login(String username, String password, HttpServletRequest req) &#123; this.checkLoginInfo(username, password); req.getSession().setAttribute("isLogin", true); return "success";&#125; sso-server创建授权令牌授权令牌是一串随机字符，以什么样的方式生成都没有关系，只要不重复、不易伪造即可，下面是一个例子： String token = UUID.randomUUID().toString(); sso-client取得令牌并校验sso认证中心登录后，跳转回子系统并附上令牌，子系统（sso-client）取得令牌，然后去sso认证中心校验，在LoginFilter.java的doFilter()中添加几行：// 请求附带token参数String token = req.getParameter("token");if (token != null) &#123; // 去sso认证中心校验token boolean verifyResult = this.verify("sso-server-verify-url", token); if (!verifyResult) &#123; res.sendRedirect("sso-server-url"); return; &#125; chain.doFilter(request, response);&#125; verify()方法使用httpClient实现，这里仅简略介绍，httpClient详细使用方法请参考官方文档。 HttpPost httpPost = new HttpPost("sso-server-verify-url-with-token");HttpResponse httpResponse = httpClient.execute(httpPost); sso-server接收并处理校验令牌请求 用户在sso认证中心登录成功后，sso-server创建授权令牌并存储该令牌，所以，sso-server对令牌的校验就是去查找这个令牌是否存在以及是否过期，令牌校验成功后sso-server将发送校验请求的系统注册到sso认证中心（就是存储起来的意思） 令牌与注册系统地址通常存储在key-value数据库（如redis）中，redis可以为key设置有效时间也就是令牌的有效期。redis运行在内存中，速度非常快，正好sso-server不需要持久化任何数据。 令牌与注册系统地址可以用下图描述的结构存储在redis中，可能你会问，为什么要存储这些系统的地址？如果不存储，注销的时候就麻烦了，用户向sso认证中心提交注销请求，sso认证中心注销全局会话，但不知道哪些系统用此全局会话建立了自己的局部会话，也不知道要向哪些子系统发送注销请求注销局部会话。 sso-client校验令牌成功创建局部会话令牌校验成功后，sso-client将当前局部会话标记为“已登录”，修改LoginFilter.java，添加几行： if (verifyResult) &#123; session.setAttribute("isLogin", true);&#125; sso-client还需将当前会话id与令牌绑定，表示这个会话的登录状态与令牌相关，此关系可以用java的hashmap保存，保存的数据用来处理sso认证中心发来的注销请求 注销过程用户向子系统发送带有“logout”参数的请求（注销请求），sso-client拦截器拦截该请求，向sso认证中心发起注销请求： String logout = req.getParameter("logout");if (logout != null) &#123; this.ssoServer.logout(token);&#125; sso认证中心也用同样的方式识别出sso-client的请求是注销请求（带有“logout”参数），sso认证中心注销全局会话： @RequestMapping("/logout")public String logout(HttpServletRequest req) &#123; HttpSession session = req.getSession(); if (session != null) &#123; session.invalidate();//触发LogoutListener &#125; return "redirect:/";&#125; sso认证中心有一个全局会话的监听器，一旦全局会话注销，将通知所有注册系统注销 public class LogoutListener implements HttpSessionListener &#123; @Override public void sessionCreated(HttpSessionEvent event) &#123;&#125; @Override public void sessionDestroyed(HttpSessionEvent event) &#123; //通过httpClient向所有注册系统发送注销请求 &#125;&#125; 代码部署GitHub地址： https://github.com/morethink/simple-sso.git IDEA部署 单点登录访问a系统： http://localhost/a/test 访问b系统： http://localhost/b/test a系统登录成功： b系统同时也登录成功:]]></content>
      <categories>
        <category>服务器</category>
      </categories>
      <tags>
        <tag>单点登录</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[面试中的TCP协议]]></title>
    <url>%2FTCP%2F</url>
    <content type="text"><![CDATA[TCP的三次握手和四次挥手三次握手TCP连接是通过三次握手来连接的。 第一次握手 当客户端向服务器发起连接请求时，客户端会发送同步序列标号SYN到服务器，在这里我们设SYN为x，等待服务器确认，这时客户端的状态为SYN_SENT。 第二次握手 当服务器收到客户端发送的SYN后，服务器要做的是确认客户端发送过来的SYN，在这里服务器发送确认包ACK，这里的ACK为x+1，意思是说“我收到了你发送的SYN了”，同时，服务器也会向客户端发送一个SYN包，这里我们设SYN为y。这时服务器的状态为SYN_RECV。 一句话，服务器端发送SYN和ACK两个包。 第三次握手 客户端收到服务器发送的SYN和ACK包后，需向服务器发送确认包ACK，“我也收到你发送的SYN了，我这就给你发个确认过去，然后我们即能合体了”，这里的ACK为y+1，发送完毕后，客户端和服务器的状态为ESTABLISH，即TCP连接成功。 在三次握手中，客户端和服务器端都发送两个包SYN和ACK，只不过服务器端的两个包是一次性发过来的，客户端的两个包是分两次发送的。 四次挥手当A端和B端要断开连接时，需要四次握手，这里称为四次挥手。 断开连接请求可以由客户端发出，也可以由服务器端发出，在这里我们称A端向B端请求断开连接。 第一次挥手 A端向B端请求断开连接时会向B端发送一个带有FIN标记的报文段，这里的FIN是Finish的意思。 第二次挥手 B端收到A发送的FIN后，B段现在可能现在还有数据没有传完，所以B端并不会马上向A端发送FIN，而是先发送一个确认序号ACK，意思是说“你发的断开连接请求我收到了，但是我现在还有数据没有发完，请稍等一下呗”。 第三次挥手 当B端的事情忙完了，那么此时B端就可以断开连接了，此时B端向A端发送FIN序号，意思是这次可以断开连接了。 第四次挥手 A端收到B端发送的FIN后，会向B端发送确认ACK，然后经过两个MSL时长后断开连接。 MSL是Maximum Segment Lifetime，最大报文段生存时间，2个MSL是报文段发送和接收的最长时间。 两次握手可以么？TCP连接时是三次握手，那么两次握手可行吗？ 在谢希仁著《计算机网络》第六版中讲”三次握手”的目的是”为了防止已失效的连接请求报文段突然又传送到了服务端，因而产生错误”。 假定出现一种异常情况下：client发出的第一个连接请求报文段并没有丢失，而是在某个网络结点长时间的滞留了，以致延误到连接释放以后的某个时间才到达server。本来这是一个早已失效的报文段。但server收到此失效的连接请求报文段后，就误认为是client再次发出的一个新的连接请求。于是就向client发出确认报文段，同意建立连接。假设不采用“三次握手”，那么只要server发出确认，新的连接就建立了。由于现在client并没有发出建立连接的请求，因此不会理睬server的确认，也不会向server发送ACK包。这样就会白白浪费资源。 而经过三次握手，客户端和服务器都有应有答，这样可以确保TCP正确连接。 为什么TCP连接是三次，挥手确是四次？在TCP连接中，服务器端的SYN和ACK向客户端发送是一次性发送的，而在断开连接的过程中，B端向A端发送的ACK和FIN是是分两次发送的。因为在B端接收到A端的FIN后，B端可能还有数据要传输，所以先发送ACK，等B端处理完自己的事情后就可以发送FIN断开连接了。 为什么在第四次挥手后会有2个MSL的延时？前文说到:MSL是Maximum Segment Lifetime，最大报文段生存时间，2个MSL是报文段发送和接收的最长时间。 假定网络不可靠，那么第四次发送的ACK可能丢失，即B端无法收到这个ACK，如果B端收不到这个确认ACK，B端会定时向A端重复发送FIN，直到B端收到A的确认ACK。所以这个2MSL就是用来处理这个可能丢失的ACK的。 TCP流量控制一般说来，我们总是希望数据传输得更快一些。但如果发送方把数据发送得过快，接收方可能来不及接收，这就会造成数据的丢失。所谓 流量控制(flow contrl) 就是 让发送方的发送速率不要太快，要让接收方来得及接收 。 TCP利用滑动窗口机制实现对发送方的流量控制，发送方的发送窗口不可以大于接收方给出的接收窗口的大小。窗口两个边沿的相对运动增加或减少了窗口的大小。当接收方没有缓存可用，会发送零窗口大小的报文，此时发送方不能够发送任何数据。 TCP的窗口单位是字节，不是报文段。 滑动窗口滑动窗口协议是传输层进行流控的一种措施，接收方通过通告发送方自己的窗口大小，从而控制发送方的发送速度，从而达到防止发送方发送速度过快而导致自己被淹没的目的。 TCP滑动窗口分为接受窗口，发送窗口。 对ACK的再认识，ack通常被理解为收到数据后给出的一个确认ACK，ACK包含两个非常重要的信息： 期望接收到的下一字节的序号n，该n代表接收方已经接收到了前n-1字节数据，此时如果接收方收到第n+1字节数据而不是第n字节数据，接收方是不会发送序号为n+2的ACK的。举个例子，假如接收端收到1-1024字节，它会发送一个确认号为1025的ACK,但是接下来收到的是2049-3072，它是不会发送确认号为3072的ACK,而依旧发送1025的ACK。 是当前的窗口大小m，如此发送方在接收到ACK包含的这两个数据后就可以计算出还可以发送多少字节的数据给对方，假定当前发送方已发送到第x字节，则可以发送的字节数就是y=m-(x-n). 重点：发送方根据收到ACK当中的期望收到的下一个字节的序号n以及窗口m，还有当前已经发送的字节序号x，算出还可以发送的字节数。 滑动窗口协议如图所示： 在这个图中，我们将字节从1至11进行标号。接收方通告的窗口称为提出的窗口，它覆盖了从第4字节到第9字节的区域，表明接收方已经确认了包括第3字节在内的数据，且通告窗口大小为6。我们知道窗口大小是与确认序号相对应的。发送方计算它的可用窗口，该窗口表明多少数据可以立即被发送。当接收方确认数据后，这个滑动窗口不时地向右移动。窗口两个边沿的相对运动增加或减少了窗口的大小。我们使用三个术语来描述窗口左右边沿的运动： 窗口左边沿向右边沿靠近为窗口合拢。这种现象发生在数据被发送和确认时。 当窗口右边沿向右移动时将允许发送更多的数据，我们称之为窗口张开。这种现象发生在另一端的接收进程读取已经确认的数据并释放了T C P的接收缓存时。 当右边缘向左移动时，称之为窗口收缩。 每个TCP连接有发送窗口和接收窗口这两个窗口。 TCP是双工的协议，会话的双方都可以同时接收、发送数据。 TCP会话的双方都各自维护一个“发送窗口”和一个“接收窗口”。其中各自的“接收窗口”大小取决于应用、系统、硬件的限制（TCP传输速率不能大于应用的数据处理速率）。各自的“发送窗口”则要求取决于对端通告的“接收窗口”，要求相同。 TCP拥塞控制在计算机网络中的链路容量(即带宽)、交换节点中的缓存和处理机等，都是网络中的资源。在某段时间，若对网络中某一资源的需求超过了该资源所能提供的可用部分，网络的性能就会变坏。这种情况就叫做 拥塞。 拥塞控制 就是 防止过多的数据注入网络中，这样可以使网络中的路由器或链路不致过载。 拥塞控制是一个全局性的过程，和流量控制不同，流量控制指点对点通信量的控制。 进行拥塞控制的四种算法： 慢开始（Slow-start) 拥塞避免（Congestion Avoidance) 快重传（Fast Restrangsmit) 快恢复（Fast Recovery）。 慢开始和拥塞避免发送方维持一个叫做拥塞窗口cwnd（congestion window）的状态变量。拥塞窗口的大小取决于网络的拥塞程度，并且动态地在变化。发送方让自己的发送窗口等于拥塞窗口，另外考虑到接受方的接收能力，发送窗口可能小于拥塞窗口。 慢开始算法的思路就是，不要一开始就发送大量的数据，先探测一下网络的拥塞程度，也就是说由小到大逐渐增加拥塞窗口的大小。在一开始发送方先设置cwnd=1，每经过一次传输轮次之后拥塞窗口就加倍（2的指数倍增加）。 为了防止窗口cwnd增长过大引起网络拥塞，还需设置一个慢开始门限ssthresh状态变量。ssthresh的用法如下： 当cwnd&lt;ssthresh时，使用慢开始算法。 当cwnd&gt;ssthresh时，改用拥塞避免算法。 当cwnd=ssthresh时，既可使用慢开始算法，也可使用拥塞算法。 拥塞避免算法让拥塞窗口缓慢增长，即每经过一个往返时间RTT就把发送方的拥塞窗口cwnd加1，而不是加倍。这样拥塞窗口按线性规律缓慢增长，比慢开始算法的拥塞窗口增长速率缓慢得多。 无论是在慢开始阶段还是在拥塞避免阶段，只要发送方判断网络出现拥塞（其根据就是没有收到确认，虽然没有收到确认可能是其他原因的分组丢失，但是因为无法判定，所以都当做拥塞来处理），就把慢开始门限设置为出现拥塞时的发送窗口大小的一半。然后把拥塞窗口设置为1，执行慢开始算法。 快重传和快恢复快重传要求接收方在收到一个失序的报文段后就立即发出重复确认（为的是使发送方及早知道有报文段没有到达对方）而不要等到自己发送数据时捎带确认。快重传算法规定，发送方只要一连收到三个重复确认就应当立即重传对方尚未收到的报文段，而不必继续等待设置的重传计时器时间到期。由于发送方能 尽早重传未被确认的报文段，因此采用快重传算法后可以使整个网络的吞吐量提高约20%。 快恢复算法，有以下两个要点: 当发送方连续收到三个重复确认时，就执行“乘法减小”算法，把ssthresh门限减半。但是接下去并不执行慢开始算法。 考虑到如果网络出现拥塞的话就不会收到好几个重复的确认，所以发送方现在认为网络可能没有出现拥塞。所以此时不执行慢开始算法，而是将cwnd设置为ssthresh的大小，然后执行拥塞避免算法。 TCP和UDP的区别 TCP面向连接，UDP面向非连接即发送数据前不需要建立链接。 面向连接的服务，通信双方在进行通信之前，要先在双方建立起一个完整的可以彼此沟通的通道，在通信过程中，整个连接的情况一直可以被实时地监控和管理。 非面向连接的服务，不需要预先建立一个联络两个通信节点的连接，需要通信的时候，发送节点就可以往网络上发送信息，让信息自主地在网络上去传，一般在传输的过程中不再加以监控。 TCP提供可靠的服务（数据传输），UDP无法保证。 TCP面向字节流，UDP面向报文。 TCP数据传输慢，UDP数据传输快。 注意： TCP 并不能保证数据一定会被对方接收到，因为这是不可能的。 TCP的可靠性应该是相对于UDP不可靠传输来说的，因为UDP提供的是不可靠的数据报服务，不保证数据报能到达接收端，可能会有丢失；另外处于传输层之下的IP层也是不可靠的，仅提供尽力而为的端到端数据传输服务（best-effort delivery service），不作任何保证。 所以TCP的可靠性是指基于不可靠的IP层在传输层提供可靠的数据传输服务，如果有可能，就把数据递送到接收方，否则就（通过放弃重传并且中断连接这一手段）通知用户。因此准确说 TCP 也不是 100% 可靠的协议，它所能提供的是数据的可靠递送或故障的可靠通知。 实现TCP的可靠传输有以下机制： 校验和（校验数据是否损坏） 定时器（分组丢失则重传） 序号（用于检测丢失的分组和冗余的分组） 确认（接收方告知发送方正确接收分组以及期望的下一个分组） 否定确认（接收方通知发送方未被正确接收的分组） 窗口和流水线（用于增加信道的吞吐量）。 至于数据是否在中途被修改或者被窃听，这应该是属于安全性问题。提高安全性最根本的办法就是加密数据，比如远程登录用ssh而非telnet。 TCP、UDP对应的协议TCP对应的协议 FTP：定义了文件传输协议，使用21端口。 Telnet：一种用于远程登陆的端口，使用23端口，用户可以以自己的身份远程连接到计算机上，可提供基于DOS模式下的通信服务。 SMTP：邮件传送协议，用于发送邮件。服务器开放的是25号端口。 POP3：它是和SMTP对应，POP3用于接收邮件。POP3协议所用的是110端口。 HTTP：是从Web服务器传输超文本到本地浏览器的传送协议。 UDP对应的协议 DNS：用于域名解析服务，将域名地址转换为IP地址。DNS用的是53号端口。 SNMP：简单网络管理协议，使用161号端口，是用来管理网络设备的。由于网络设备很多，无连接的服务就体现出其优势。 TFTP(Trival File Transfer Protocal)，简单文件传输协议，该协议在熟知端口69上使用UDP服务。 SYN攻击什么是 SYN 攻击（SYN Flood）？在三次握手过程中，服务器发送 SYN-ACK 之后，收到客户端的 ACK 之前的 TCP 连接称为半连接(half-open connect)。此时服务器处于 SYN_RCVD 状态。当收到 ACK 后，服务器才能转入 ESTABLISHED 状态. SYN攻击指的是，攻击客户端在短时间内伪造大量不存在的IP地址，向服务器不断地发送SYN包，服务器回复确认包，并等待客户的确认。由于源地址是不存在的，服务器需要不断的重发直至超时，这些伪造的SYN包将长时间占用未连接队列，正常的SYN请求被丢弃，导致目标系统运行缓慢，严重者会引起网络堵塞甚至系统瘫痪。 SYN 攻击是一种典型的 DoS/DDoS 攻击。 如何检测 SYN 攻击？检测 SYN 攻击非常的方便，当你在服务器上看到大量的半连接状态时，特别是源IP地址是随机的，基本上可以断定这是一次SYN攻击。在 Linux/Unix 上可以使用系统自带的 netstats 命令来检测 SYN 攻击。 如何防御 SYN 攻击？SYN攻击不能完全被阻止，除非将TCP协议重新设计。我们所做的是尽可能的减轻SYN攻击的危害，常见的防御SYN 攻击的方法有如下几种： 缩短超时（SYN Timeout）时间 增加最大半连接数 过滤网关防护 SYN cookies技术]]></content>
      <categories>
        <category>网络</category>
      </categories>
      <tags>
        <tag>面试</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[SpringMVC 知识整理]]></title>
    <url>%2FSpringMVC-Knowledge-finishing%2F</url>
    <content type="text"><![CDATA[SpringMVC架构设计MVC是一种架构模式，它把业务的实现和展示相分离。 SpringMVC与struts2的区别 Struts2是类级别的拦截， 一个类对应一个request上下文，SpringMVC是方法级别的拦截，一个方法对应一个request上下文，而方法同时又跟一个url对应,所以说从架构本身上SpringMVC就容易实现restful url,而struts2的架构实现起来要费劲，因为Struts2中Action的一个方法可以对应一个url，而其类属性却被所有方法共享，这也就无法用注解或其他方式标识其所属方法了。 springmvc可以进行单例开发，并且建议使用单例开发，struts2通过类的成员变量接收参数，无法使用单例，只能使用多例。 由于Struts2需要针对每个request进行封装，把request，session等servlet生命周期的变量封装成一个一个Map，供给每个Action使用，并保证线程安全，所以在原则上，是比较耗费内存的。 拦截器实现机制上，Struts2有以自己的interceptor机制，SpringMVC用的是独立的AOP方式，这样导致Struts2的配置文件量还是比SpringMVC大。 servlet和filter的区别了。 SpringMVC集成了Ajax，使用非常方便，只需一个注解@ResponseBody就可以实现，然后直接返回响应文本即可，而Struts2拦截器集成了Ajax，在Action中处理时一般必须安装插件或者自己写代码集成进去，使用起来也相对不方便。 SpringMVC验证支持JSR303，处理起来相对更加灵活方便，而Struts2验证比较繁琐，感觉太烦乱。 spring MVC和Spring是无缝的。从这个项目的管理和安全上也比Struts2高（当然Struts2也可以通过不同的目录结构和相关配置做到SpringMVC一样的效果，但是需要xml配置的地方不少）。 设计思想上，Struts2更加符合OOP的编程思想， SpringMVC就比较谨慎，在servlet上扩展。 SpringMVC开发效率和性能高于Struts2。 SpringMVC可以认为已经100%零配置。 SpringAOP整合SpringMVCspring容器不注册controller层组件，controller组件由springMVC容器单独注册。 // applicationContext.xml&lt;context:component-scan base-package="com.shuyun.channel"&gt; &lt;context:exclude-filter type="annotation" expression="org.springframework.stereotype.Controller" /&gt; &lt;context:exclude-filter type="annotation" expression="org.springframework.web.bind.annotation.RestController" /&gt; &lt;context:exclude-filter type="annotation" expression="org.springframework.web.bind.annotation.ControllerAdvice" /&gt;&lt;/context:component-scan&gt;// springmvc-servlet.xml&lt;context:component-scan base-package="com.shuyun.channel" use-default-filters="false"&gt; &lt;context:include-filter type="annotation" expression="org.springframework.stereotype.Controller" /&gt; &lt;context:include-filter type="annotation" expression="org.springframework.web.bind.annotation.RestController" /&gt; &lt;context:include-filter type="annotation" expression="org.springframework.web.bind.annotation.ControllerAdvice" /&gt;&lt;/context:component-scan&gt; 注意： 关于&lt;context:annotation-config /&gt;和 &lt;context:component-scan /&gt;: component-scan会自动加上annotation-config功能，有了component-scan不用再写annotation-config了。参见spring官方reference 配置层次化Spring容器参考 配置层次化Spring容器 我们知道，在开发基于spring的Web应用时，通常使用两个IoC容器，一个是由DispatchServlet初始化的WebApplicationContext,一个是由ContextLoaderListener初始化的ApplicationContext。对于Spring容器，Spring的官方参考手册详细地讲解了依赖注入的配置方式，对于容器本身的配置和多个容器之间的关系却不曾提及。于是，很多人以为在一个应用中只有一个全局的Spring容器，或者不了解MVC使用的WebApplicationContext和根ApplicationContext的关系。 通过查看Spring的源码和API,发现Spring可以配置为多个容器，容器之间可以配置为层级关系，一个根容器可以配置许多子容器，子容器还可以配置子容器，从而形成一个单根的层次化结构。对于该容器结构中的每个容器，在其中查找特定的bean时，会首先在本容器内查找，如果找到对应的bean,就返回该bean；如果没有找到，就会从直接父容器中去查找，依此类推，直到根容器为止。 从上面的示例中可以看到，从子容器中可以取得父容器中配置的bean，而父容器中不能够取得子容器中的bean。 在Spring MVC中WebApplicationContext配置为根ApplicationContext的子容器，所以，MVC使用的容器中能够取得根ApplicationContext中的bean。在一个web程序中可以配置多个DispatchSerlvet，每个Servlet对应一个容器，所有这些容器都作为根容器的子容器，这样，我们就可以把通用的bean放在根容器中，而针对特定DispatchServlet的bean，可以放在各自的子容器中。 文件上传 Controller的方法中需要接受一个Spring MVC提供的MultipartFile接口作为方法的参数，该参数接收前台表单type为file提交的对象，使用@RequestParam注解指明参数，那么Spring就会自动将表单传递过来的对象的类型转换为MultipartFile类型。 MultipartFile中提供了getName()、getSize()、getByte()getContentType()、isEmpty()、getInputStream()、getOriginalFilename()方法来访问文件。getOriginalFilename()方法是获取最初文件名，即本地文件名。 在Controller方法中使用FileUtils下的copyInputStreamToFile(InputStream in,File file)方法来完成文件的拷贝.第一个参数是文件拷贝源的输入流,直接使用MultipartFile下的getInputStream()方法.第二个参数是文件将要保存的位置. @RequestMapping("/doUpload")public Result doUpload(@RequestParam("file") MultipartFile file) throws IOException &#123; if (!file.isEmpty()) &#123; FileUtils.copyInputStreamToFile(file.getInputStream(), new File("E://", file.getOriginalFilename())); &#125; return ResultUtil.SUCCESS_RESULT;&#125; Jackson How to enable pretty print JSON output (Jackson) Jackson 2 – Convert Java Object to / from JSON SpringMVC关于json、xml自动转换的原理研究(附带源码分析) SpringMVC拦截器拦截器好比你要去取经，那么，你就必须经过九九八十一关，主要用来解决请求的共性问题，如：乱码问题、权限验证问题等 实现SpringMVC拦截器的三个步骤 创建一个实现HandlerInterceptor接口，并实现接口的方法的类 将创建的拦截器注册到SpringMVC的配置文件中实现注册 &lt;mvc：interceptors&gt;&lt;bean class="路径下的类"&gt;&lt;/mvc：interceptors&gt; 配置拦截器的拦截规则： &lt;mvc：interceptors&gt; &lt;mvc：interceptor&gt; &lt;mvc:mapping path="拦截的action"&gt; &lt;bean class="路径下的类"&gt; &lt;/mvc：interceptor&gt;&lt;/mvc：interceptors&gt; 拦截器中三个方法的介绍： preHandle()方法是否将当前请求拦截下来。（返回true请求继续运行，返回false请求终止（包括action层也会终止），Object arg代表被拦截的目标对象。） postHandle()方法的ModelAndView对象可以改变发往的视图或修改发往视图的信息。 afterCompletion()方法表示视图显示之后在执行该方法。（一般用于资源的销毁） 拦截器和过滤器共同：他们都是用来检查程序的共同场景，只不过拦截器是面向Action的，过滤器是面向整个web应用的。 解决权限验证问题 解决乱码问题 拦截器和过滤器的区别： 拦截器是基于java的反射机制的，而过滤器是基于函数回调。 拦截器不依赖与servlet容器，过滤器依赖与servlet容器。 拦截器只能对action请求起作用，而过滤器则可以对几乎所有的请求起作用。 拦截器可以访问action上下文、值栈 里的对象，而过滤器不能访问。 在action的生命周期中，拦截器可以多次被调用，而过滤器只能在容器初始化时被调用一次。 拦截器可以获取IOC容器中的各个bean，而过滤器就不行，这点很重要，在拦截器里注入一个service，可以调用业务逻辑。 拦截器方法的作用顺序 拦截器的其它实现方式： 拦截器的类还可以通过实现WebRequestInterceptor（HandlerInterceptor）接口来编写。 向SpringMVC框架注册的写法不变。 弊端：preHandler方法没有返回值，不能终止请求。 Ps：建议使用功能更强大的实现方式，实现HandlerInterceptor接口。 Spring4增加功能Spring4主要在Web服务方面有下面两个方面提升： 控制器使用@ResponseBody和 @RestController。 异步调用。 Spring整合Struts2Spring默认是单例，Struts2默认是多实例的。 如果是spring配置文件中的 bean的名字的话就是spring创建，那么单实例还是多实例就由spring的action Bean中的业务逻辑控制器类是否配置为scope=”prototype”，有就是多实例的，没有就是单实例的，顺序是先从spring中找，找不到再从struts配置文件中找。 对于无Spring插件（Struts2-spring-plugin-XXX.jar）的整合方式，需要在spring的action Bean中加业务逻辑控制器类配scope=”prototype”。 &lt;bean id="user" class="modle.User" scope="prototype"/&gt; 对于有Spring插件（Struts2-spring-plugin-XXX.jar）的整合方式：反编译StrutsSpringObjectFactory以及相关的代码才发现，如果在struts action的配置文件 &lt;action name=&quot;..&quot; class=&quot;..&quot;/&gt; 中class写的如果是完整的包名和类名的话就是struts创建action对象，也就是多实例的； 参考文档： 史上最全最强SpringMVC详细示例实战教程 Spring MVC快速入门 Web MVC framework - Part VI. The Web Spring 注解学习手札（七） 补遗——@ResponseBody，@RequestBody，@PathVariable SpringMVC4.1之Controller层最佳实践]]></content>
      <categories>
        <category>SpringMVC</category>
      </categories>
  </entry>
  <entry>
    <title><![CDATA[SpringMVC 参数绑定注解解析]]></title>
    <url>%2FSpringMVC-Annotation%2F</url>
    <content type="text"><![CDATA[本文介绍了用于参数绑定的相关注解。 绑定：将请求中的字段按照名字匹配的原则填入模型对象。 SpringMVC就跟Struts2一样，通过拦截器进行参数匹配。 代码在 https://github.com/morethink/MySpringMVC URI模板变量这里指uri template中variable(路径变量)，不含queryString部分 @PathVariable当使用@RequestMapping URI template 样式映射时， 即 someUrl/{paramId}, 这时的paramId可通过 @Pathvariable注解绑定它传过来的值到方法的参数上。 示例代码：@RestController@RequestMapping("/users")public class UserAction &#123; @GetMapping("/&#123;id&#125;") public Result getUser(@PathVariable int id) &#123; return ResultUtil.successResult("123456"); &#125;&#125; 上面代码把URI template 中变量 ownerId的值和petId的值，绑定到方法的参数上。若方法参数名称和需要绑定的uri template中变量名称不一致，需要在@PathVariable(“name”)指定uri template中的名称。 请求头@RequestHeader@RequestHeader 注解，可以把Request请求header部分的值绑定到方法的参数上。 示例代码： 这是一个Request 的header部分： Accept:text/html,application/xhtml+xml,application/xml;q=0.9,image/webp,image/apng,*/*;q=0.8Accept-Encoding:gzip, deflate, brAccept-Language:zh-CN,zh;q=0.9,en-US;q=0.8,en;q=0.7Cache-Control:max-age=0Connection:keep-aliveHost:localhost:8080Upgrade-Insecure-Requests:1User-Agent:Mozilla/5.0 (Windows NT 10.0; WOW64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/62.0.3202.62 Safari/537.36 @GetMapping("/getRequestHeader")public Result getRequestHeader(@RequestHeader("Accept-Encoding") String encoding) &#123; return ResultUtil.successResult(encoding);&#125; 上面的代码，把request header部分的 Accept-Encoding的值，绑定到参数encoding上。 @CookieValue可以把Request header中关于cookie的值绑定到方法的参数上。 例如有如下Cookie值：JSESSIONID=588DC770E582A3189B7E6210102EAE02参数绑定的代码： @RequestMapping("/getCookie")public Result getCookie(@CookieValue("JSESSIONID") String cookie) &#123; return ResultUtil.successResult(cookie);&#125; 即把JSESSIONID的值绑定到参数cookie上。 请求体@RequestParam 常用来处理简单类型的绑定，通过Request.getParameter() 获取的String可直接转换为简单类型的情况（ String–&gt; 简单类型的转换操作由ConversionService配置的转换器来完成）；因为使用request.getParameter()方式获取参数，所以可以处理get 方式中queryString的值，也可以处理post方式中 body data的值； 用来处理Content-Type: 为 application/x-www-form-urlencoded编码的内容，提交方式GET、POST； 该注解有两个属性： value、required； value用来指定要传入值的id名称，required用来指示参数是否必须绑定； 示例代码： @GetMapping("/tesRequestParam")public Result tesRequestParam(@RequestParam("username") String username) &#123; return ResultUtil.successResult(username);&#125; @RequestBody该注解常用来处理Content-Type: 不是application/x-www-form-urlencoded编码的内容，例如application/json, application/xml等； 它是通过使用HandlerAdapter 配置的HttpMessageConverters来解析post data body，然后绑定到相应的bean上的。 因为配置有FormHttpMessageConverter，所以也可以用来处理 application/x-www-form-urlencoded的内容，处理完的结果放在一个MultiValueMap&lt;String, String&gt;里，这种情况在某些特殊需求下使用，详情查看FormHttpMessageConverter api; 示例代码： @PostMapping(&quot;/tesRequestBody&quot;)public Result tesRequestBody(@RequestBody User user) &#123; return ResultUtil.successResult(user);&#125; 结果截图： @RequestBody通过list接收对象数组在我们传递对象的时候，无论Content-Type是x-www-form-urlencoded还是application/json其实没有多大的关系，可是当我们需要传递对象数组的时候，表单编码就不行了，这时我们是可以采用json传递，然后后台使用@RequestBody注解，通过list接收来对象数组。 前端代码： index.html//打开页面时运行$(document).ready(function () &#123; var users = []; var user1 = &#123;"username": "dd", "password": "123"&#125;; var user2 = &#123;"username": "gg", "password": "123"&#125;; users.push(user1); users.push(user2); $.ajax(&#123; type: "POST", url: "users/saveUsers", timeout: 30000, dataType: "json", contentType: "application/json", data: JSON.stringify(users), success: function (data) &#123; //将返回的数据展示成table showTable(data); &#125;, error: function () &#123; //请求出错的处理 $("#result").text("请求出错"); &#125; &#125;);&#125;); 后台代码： @PostMapping("saveUsers")public Result saveUsers(@RequestBody List&lt;User&gt; users) &#123; return ResultUtil.successResult(users);&#125; 结果截图： @SessionAttribute该注解用来绑定HttpSession中的attribute对象的值，便于在方法中的参数里使用。该注解有value、types两个属性，可以通过名字和类型指定要使用的attribute 对象 示例代码： @PostMapping("/setSessionAttribute")public Result setSessionAttribute(HttpSession session, String attribute) &#123; session.setAttribute("attribute", attribute); return ResultUtil.SUCCESS_RESULT;&#125;@GetMapping("/getSessionAttribute")public Result getSessionAttribute(@SessionAttribute("attribute") String attribute) &#123; return ResultUtil.successResult(attribute);&#125; 我们首先给session添加一个attribute，然后再取出这个attribute。 @ModelAttribute@ModelAttribute标注可被应用在方法或方法参数上。 方法使用@ModelAttribute标注标注在方法上的@ModelAttribute说明方法是用于添加一个或多个属性到model上。这样的方法能接受与@RequestMapping标注相同的参数类型，只不过不能直接被映射到具体的请求上。 在同一个控制器中，标注了@ModelAttribute的方法实际上会在@RequestMapping方法之前被调用。 以下是示例： // Add one attribute// The return value of the method is added to the model under the name "account"// You can customize the name via @ModelAttribute("myAccount")@ModelAttributepublic Account addAccount(@RequestParam String number) &#123; return accountManager.findAccount(number);&#125;// Add multiple attributes@ModelAttributepublic void populateModel(@RequestParam String number, Model model) &#123; model.addAttribute(accountManager.findAccount(number)); // add more ...&#125; @ModelAttribute方法通常被用来填充一些公共需要的属性或数据，比如一个下拉列表所预设的几种状态，或者宠物的几种类型，或者去取得一个HTML表单渲染所需要的命令对象，比如Account等。 @ModelAttribute标注方法有两种风格： 在第一种写法中，方法通过返回值的方式默认地将添加一个属性； 在第二种写法中，方法接收一个Model对象，然后可以向其中添加任意数量的属性。 可以在根据需要，在两种风格中选择合适的一种。 一个控制器可以拥有多个@ModelAttribute方法。同个控制器内的所有这些方法，都会在@RequestMapping方法之前被调用。 @ModelAttribute方法也可以定义在@ControllerAdvice标注的类中，并且这些@ModelAttribute可以同时对许多控制器生效。 属性名没有被显式指定的时候又当如何呢？在这种情况下，框架将根据属性的类型给予一个默认名称。举个例子，若方法返回一个Account类型的对象，则默认的属性名为”account”。可以通过设置@ModelAttribute标注的值来改变默认值。当向Model中直接添加属性时，请使用合适的重载方法addAttribute(..)-即带或不带属性名的方法。 @ModelAttribute标注也可以被用在@RequestMapping方法上。这种情况下，@RequestMapping方法的返回值将会被解释为model的一个属性，而非一个视图名，此时视图名将以视图命名约定来方式来确定。 方法参数使用@ModelAttribute标注标注在方法参数上的@ModelAttribute说明了该方法参数的值将由model中取得。如果model中找不到，那么该参数会先被实例化，然后被添加到model中。在model中存在以后，请求中所有名称匹配的参数都会填充到该参数中。 这在Spring MVC中被称为数据绑定，一个非常有用的特性，我们不用每次都手动从表格数据中转换这些字段数据。 @PostMappingpublic Result saveUser(@ModelAttribute User user) &#123; return ResultUtil.successResult(user);&#125; 以上面的代码为例，这个User类型的实例可能来自哪里呢？有几种可能: 它可能因为@SessionAttributes标注的使用已经存在于model中 它可能因为在同个控制器中使用了@ModelAttribute方法已经存在于model中，正如上一小节所叙述的 它可能是由URI模板变量和类型转换中取得的 它可能是调用了自身的默认构造器被实例化出来的 @ModelAttribute方法常用于从数据库中取一个属性值，该值可能通过@SessionAttributes标注在请求中间传递。在一些情况下，使用URI模板变量和类型转换的方式来取得一个属性是更方便的方式。 在不给定注解的情况下，参数是怎样绑定的？通过分析AnnotationMethodHandlerAdapter和RequestMappingHandlerAdapter的源代码发现，方法的参数在不给定参数的情况下： 若要绑定的对象时简单类型：调用@RequestParam来处理的。这里的简单类型指Java的原始类型(boolean, int 等)、原始类型对象（Boolean, Int等）、String、Date等ConversionService里可以直接String转换成目标对象的类型。也就是说没有特别需求，不推荐使用@RequestParam。 若要绑定的对象时复杂类型：调用@ModelAttribute来处理的。也就是说如果不需要从model或者session中得到数据，@ModelAttribute可以不使用。 @RequestMapping支持的方法参数下面这些参数Spring在调用请求方法的时候会自动给它们赋值，所以当在请求方法中需要使用到这些对象的时候，可以直接在方法上给定一个方法参数的申明，然后在方法体里面直接用就可以了。 HttpServlet 对象，主要包括HttpServletRequest 、HttpServletResponse 和HttpSession 对象。 但是有一点需要注意的是在使用HttpSession 对象的时候，如果此时HttpSession 对象还没有建立起来的话就会有问题。 Spring 自己的WebRequest 对象。 使用该对象可以访问到存放在HttpServletRequest 和HttpSession 中的属性值。 InputStream 、OutputStream 、Reader 和Writer 。 InputStream 和Reader 是针对HttpServletRequest 而言的，可以从里面取数据；OutputStream 和Writer 是针对HttpServletResponse 而言的，可以往里面写数据。 使用@PathVariable 、@RequestParam 、@CookieValue 和 @RequestHeader 标记的参数。 使用@ModelAttribute 标记的参数。 java.util.Map 、Spring 封装的Model 和ModelMap 。 这些都可以用来封装模型数据，用来给视图做展示。 实体类。 可以用来接收上传的参数。 Spring 封装的MultipartFile 。 用来接收上传文件的。 Spring 封装的Errors 和BindingResult 对象。 这两个对象参数必须紧接在需要验证的实体对象参数之后，它里面包含了实体对象的验证结果。 一个参数传多个值在浏览器输入此URLhttp://localhost:8080/admin/login.action?username=geek&amp;password=geek&amp;password=geek 结果得到的对象为 ： Manager{username=&#39;geek&#39;, password=&#39;geek,geek&#39;} 参考文档: @RequestParam @RequestBody @PathVariable 等参数绑定注解详解 SpringMVC之Controller常用注解功能全解析 @ModelAttribute使用详解]]></content>
      <categories>
        <category>SpringMVC</category>
      </categories>
  </entry>
  <entry>
    <title><![CDATA[Redis和Memcached区别]]></title>
    <url>%2FRedis-and-Memcached-difference%2F</url>
    <content type="text"><![CDATA[本文参考 Redis与Memcached的区别。 如果简单地比较Redis与Memcached的区别，大多数都会得到以下观点： Redis不仅仅支持简单的k/v类型的数据，同时还提供list，set，zset，hash等数据结构的存储。 Redis支持数据的备份，即master-slave模式的数据备份。 Redis支持数据的持久化，可以将内存中的数据保持在磁盘中，重启的时候可以再次加载进行使用。 抛开这些，可以深入到Redis内部构造去观察更加本质的区别，理解Redis的设计。 网络IO模型Memcached网络IO模型Memcached是 多线程非阻塞IO复用 的网络模型，分为监听主线程和worker子线程，监听线程监听网络连接，接受请求后，将连接描述字pipe传递给worker线程，进行读写IO, 网络层使用libevent封装的事件库，多线程模型可以发挥多核作用，但是引入了cache coherency和锁的问题，比如，Memcached最常用的stats 命令，实际Memcached所有操作都要对这个全局变量加锁，进行计数等工作，带来了性能损耗。 Redis网络IO模型Redis使用 单线程非阻塞IO复用 模型，自己封装了一个简单的AeEvent事件处理框架，主要实现了epoll、kqueue和select，对于单纯只有IO操作来说，单线程可以将速度优势发挥到最大，但是 Redis也提供了一些简单的计算功能，比如排序、聚合等，对于这些操作，单线程模型实际会严重影响整体吞吐量，CPU计算过程中，整个IO调度都是被阻塞住的。 内存管理方面 Memcached使用预分配的内存池的方式，使用slab和大小不同的chunk来管理内存，Item根据大小选择合适的chunk存储，内存池的方式可以省去申请/释放内存的开销，并且能减小内存碎片产生，但这种方式也会带来一定程度上的空间浪费，并且在内存仍然有很大空间时，新的数据也可能会被剔除，原因可以参考Timyang的文章： http://timyang.net/data/Memcached-lru-evictions/ Redis使用现场申请内存的方式来存储数据，并且很少使用free-list等方式来优化内存分配，会在一定程度上存在内存碎片，Redis跟据存储命令参数，会把带过期时间的数据单独存放在一起，并把它们称为临时数据，非临时数据是永远不会被剔除的，即便物理内存不够，导致swap也不会剔除任何非临时数据(但会尝试剔除部分临时数据)，这点上 Redis更适合作为存储而不是cache。 数据一致性问题 Memcached提供了cas命令，可以保证多个并发访问操作同一份数据的一致性问题。 Redis没有提供cas命令，并不能保证这点，不过Redis提供了事务的功能，可以保证一串命令的原子性，中间不会被任何操作打断。 集群管理的不同Memcached是全内存的数据缓冲系统，Redis虽然支持数据的持久化，但是全内存毕竟才是其高性能的本质。作为基于内存的存储系统来说，机器物理内存的大小就是系统能够容纳的最大数据量。如果需要处理的数据量超过了单台机器的物理内存大小，就需要构建分布式集群来扩展存储能力。 分布式MemcachedMemcached本身并不支持分布式，因此只能在客户端通过像一致性哈希这样的分布式算法来实现Memcached的分布式存储。下图给出了Memcached的分布式存储实现架构。当客户端向Memcached集群发送数据之前，首先会通过内置的分布式算法计算出该条数据的目标节点，然后数据会直接发送到该节点上存储。但客户端查询数据时，同样要计算出查询数据所在的节点，然后直接向该节点发送查询请求以获取数据。 Redis Cluster相较于Memcached只能采用客户端实现分布式存储，Redis更偏向于在服务器端构建分布式存储。最新版本的Redis已经支持了分布式存储功能。Redis Cluster是一个实现了分布式且允许单点故障的Redis高级版本，它没有中心节点，具有线性可伸缩的功能。下图给出Redis Cluster的分布式存储架构，其中节点与节点之间通过二进制协议进行通信，节点与客户端之间通过ascii协议进行通信。在数据的放置策略上，Redis Cluster将整个key的数值域分成4096个哈希槽，每个节点上可以存储一个或多个哈希槽，也就是说当前Redis Cluster支持的最大节点数就是4096。Redis Cluster使用的分布式算法也很简单：crc16( key ) %HASH_SLOTS_NUMBER。 为了保证单点故障下的数据可用性，Redis Cluster引入了Master节点和Slave节点。在Redis Cluster中，每个Master节点都会有对应的两个用于冗余的Slave节点。这样在整个集群中，任意两个节点的宕机都不会导致数据的不可用。当Master节点退出后，集群会自动选择一个Slave节点成为新的Master节点。 存储方式及其它方面 Memcached基本只支持简单的key-value存储，不支持枚举，不支持持久化和复制等功能 Redis除key/value之外，还支持list,set,sorted set,hash等众多数据结构，提供了KEYS进行枚举操作，但不能在线上使用，如果需要枚举线上数据，Redis提供了工具可以直接扫描其dump文件，枚举出所有数据，Redis还同时提供了持久化和复制等功能。 根据以上比较不难看出，当我们不希望数据被踢出，或者需要除key/value之外的更多数据类型时，或者需要落地功能时，使用Redis比使用Memcached更合适。 单线程的Redis为什么这么高效单线程模型Redis客户端对服务端的每次调用都经历了发送命令，执行命令，返回结果三个过程。其中执行命令阶段，由于Redis是单线程来处理命令的，所有每一条到达服务端的命令不会立刻执行，所有的命令都会进入一个队列中，然后逐个被执行。并且多个客户端发送的命令的执行顺序是不确定的。但是可以确定的是不会有两条命令被同时执行，不会产生并发问题，这就是Redis的单线程基本模型。 单线程模型每秒万级别处理能力的原因 纯内存访问。数据存放在内存中，内存的响应时间大约是100纳秒，这是Redis每秒万亿级别访问的重要基础。 非阻塞I/O，Redis采用epoll做为I/O多路复用技术的实现，再加上Redis自身的事件处理模型将epoll中的连接，读写，关闭都转换为了时间，不在I/O上浪费过多的时间。 单线程避免了线程切换和竞态产生的消耗。 Redis采用单线程模型，每条命令执行如果占用大量时间，会造成其他线程阻塞，对于Redis这种高性能服务是致命的，所以Redis是面向高速执行的数据库。 总结： 1. Redis使用最佳方式是全部数据in-memory。 2. Redis更多场景是作为Memcached的替代者来使用。 3. 当需要除key/value之外的更多数据类型支持时，使用Redis更合适。 4. 当存储的数据不能被剔除时，使用Redis更合适。 5. 需要分布式部署时，使用Redis更合适。 参考文档： Redis与Memcached的区别 Redis和Memcached的区别]]></content>
      <categories>
        <category>数据库</category>
      </categories>
      <tags>
        <tag>Redis</tag>
        <tag>Memcached</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[常用编程工具快捷键整理]]></title>
    <url>%2FCommon-programming-tools-shortcut-keys%2F</url>
    <content type="text"><![CDATA[本文主要整理常用编程工具的快捷键。 Navicat for mysql 快捷键 Ctrl+Q 打开查询窗口 Ctrl+/ 注释sql语句 Ctrl+Shift +/ 解除注释 Ctrl+R 运行查询窗口的sql语句 Ctrl+Shift+R 只运行选中的sql语句 F6 打开一个mysql命令行窗口 Ctrl+L 删除一行 Ctrl+N 打开一个新的查询窗口 Ctrl+W 关闭一个查询窗口]]></content>
      <categories>
        <category>工具</category>
      </categories>
      <tags>
        <tag>工具</tag>
        <tag>效率</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[SpringMVC数据验证(AOP处理Errors和方法验证)]]></title>
    <url>%2FSpringMVC-Bean-Validation%2F</url>
    <content type="text"><![CDATA[什么是JSR303？ JSR 303 – Bean Validation 是一个数据验证的规范，2009 年 11 月确定最终方案。Hibernate Validator 是 Bean Validation 的最佳实践。 为什么使用JSR，松耦合，让业务代码的职责更加清晰。 松耦合就是职责更加清晰，每个人都有自己的职责，如果你的代码进行改动，我不用改动或者仅仅少量改动就可以发布和部署。 准备工作maven 配置&lt;!-- JSR 303 --&gt;&lt;dependency&gt; &lt;groupId&gt;javax.validation&lt;/groupId&gt; &lt;artifactId&gt;validation-api&lt;/artifactId&gt; &lt;version&gt;1.1.0.Final&lt;/version&gt;&lt;/dependency&gt;&lt;!-- Hibernate validator--&gt;&lt;dependency&gt; &lt;groupId&gt;org.hibernate&lt;/groupId&gt; &lt;artifactId&gt;hibernate-validator&lt;/artifactId&gt; &lt;version&gt;5.2.0.Final&lt;/version&gt;&lt;/dependency&gt; SpringMVC 配置&lt;mvc:annotation-driven validator="validator"&gt;&lt;/mvc:annotation-driven&gt;&lt;!-- 配置校验器 --&gt;&lt;bean id="validator" class="org.springframework.validation.beanvalidation.LocalValidatorFactoryBean"&gt; &lt;!-- 校验器，使用Hibernate校验器 --&gt; &lt;property name="providerClass" value="org.hibernate.validator.HibernateValidator"/&gt; &lt;!-- 指定校验使用的资源文件，在文件中配置校验错误信息，如果不指定则默认使用classpath下面的ValidationMessages.properties文件， --&gt; &lt;property name="validationMessageSource" ref="messageSource"/&gt;&lt;/bean&gt;&lt;!-- 校验错误信息配置文件，也可以不配置，直接使用注解中的message即可 --&gt;&lt;bean id="messageSource" class="org.springframework.context.support.ReloadableResourceBundleMessageSource"&gt; &lt;!-- 资源文件名 --&gt; &lt;property name="basenames"&gt; &lt;list&gt; &lt;value&gt;classpath:messageSource&lt;/value&gt; &lt;/list&gt; &lt;/property&gt; &lt;!-- 资源文件编码格式 --&gt; &lt;property name="fileEncodings" value="utf-8"/&gt; &lt;!-- 对资源文件内容缓存时间，单位秒 --&gt; &lt;property name="cacheSeconds" value="120"/&gt;&lt;/bean&gt; 常用校验注解 注解 运行时检查 @AssertFalse 被注解的元素必须为false @AssertTrue 被注解的元素必须为true @DecimalMax(value) 被注解的元素必须为一个数字，其值必须小于等于指定的最大值 @DecimalMin(Value) 被注解的元素必须为一个数字，其值必须大于等于指定的最小值 @Digits(integer=, fraction=) 被注解的元素必须为一个数字，其值必须在可接受的范围内 @Future 被注解的元素必须是日期，检查给定的日期是否比现在晚 @Max(value) 被注解的元素必须为一个数字，其值必须小于等于指定的最大值 @Min(value) 被注解的元素必须为一个数字，其值必须大于等于指定的最小值 @NotNull 被注解的元素必须不为null @Null 被注解的元素必须为null @Past(java.util.Date/Calendar) 被注解的元素必须过去的日期，检查标注对象中的值表示的日期比当前早 @Pattern(regex=, flag=) 被注解的元素必须符合正则表达式，检查该字符串是否能够在match指定的情况下被regex定义的正则表达式匹配 @Size(min=, max=) 被注解的元素必须在制定的范围(数据类型:String, Collection, Map and arrays) @Valid 递归的对关联对象进行校验, 如果关联对象是个集合或者数组, 那么对其中的元素进行递归校验,如果是一个map,则对其中的值部分进行校验 @CreditCardNumber 对信用卡号进行一个大致的验证 @Email 被注释的元素必须是电子邮箱地址 @Length(min=, max=) 被注解的对象必须是字符串的大小必须在制定的范围内 @NotBlank 被注解的对象必须为字符串，不能为空，检查时会将空格忽略 @NotEmpty 被注释的对象必须不为空(数据:String,Collection,Map,arrays) @Range(min=, max=) 被注释的元素必须在合适的范围内 (数据：BigDecimal, BigInteger, String, byte, short, int, long and 原始类型的包装类 ) @URL(protocol=, host=, port=, regexp=, flags=) 被注解的对象必须是字符串，检查是否是一个有效的URL，如果提供了protocol，host等，则该URL还需满足提供的条件 Bean验证首先向我们的bean中添加注解。 public class User &#123; @NotEmpty(message = "用户名不为空") private String username; @NotEmpty(message = "密码不为空") private String password; // getter 和 setter&#125; Controller中配置: @RestControllerpublic class UserAction &#123; @PostMapping("/login") public Result login(@Validated User user, Errors errors) &#123; System.out.println(user); if (errors.hasErrors()) &#123; return ResultUtil.messageResult(errors); &#125; return ResultUtil.SUCCESS_RESULT; &#125;&#125; 我们只需要在要校验的bean前面添加@Validated，在需要校验的bean后面添加Errors对象来接收校验出错信息即可，然后根据错误信息进行判断和返回错误信息给前端。 注意：@Validated 和 Errors errors 是成对出现的，并且形参顺序是固定的（一前一后）。也就是所每一个@Validated后面必须跟一个Errors，需要验证多个bean，后面就跟多个Errors。 AOP处理Errors如果我们通过JSR来验证bean对象，那么在每个需要验证的方法中都需要处理Error对象，很容易想到可以通过AOP的方式来统一处理错误对象，并且组织错误信息，返回给前端。 通过一个环绕通知对所有的action方法尽心拦截，如果发现有Errors对象存在，就获取所有的错误信息，封装为一个list返回前端。 import org.aspectj.lang.ProceedingJoinPoint;import org.springframework.validation.BindingResult;import org.springframework.validation.Errors;import org.springframework.validation.ObjectError;import java.util.ArrayList;import java.util.List;/** * @author 李文浩 * @version 2017/10/8. */public class ValidationAdvice &#123; /** * 切点处理 * * @param pjp * @return * @throws Throwable */ public Object aroundMethod(ProceedingJoinPoint pjp) throws Throwable &#123; Errors errors = null; Object[] args = pjp.getArgs(); if (null != args &amp;&amp; args.length != 0) &#123; for (Object object : args) &#123; if (object instanceof BindingResult) &#123; errors = (BindingResult) object; break; &#125; &#125; &#125; if (errors != null &amp;&amp; errors.hasErrors()) &#123; List&lt;ObjectError&gt; allErrors = errors.getAllErrors(); List&lt;String&gt; messages = new ArrayList&lt;String&gt;(); for (ObjectError error : allErrors) &#123; messages.add(error.getDefaultMessage()); &#125; return ResultUtil.messageResult(messages); &#125; return pjp.proceed(); &#125;&#125; Spring配置： &lt;bean id="validationAdvice" class="studio.jikewang.util.ValidationAdvice" /&gt;&lt;aop:config&gt; &lt;aop:pointcut id="validation1" expression="execution(public * studio.jikewang.action.*.*(..))" /&gt; &lt;aop:aspect id="validationAspect" ref="validationAdvice"&gt; &lt;aop:around method="aroundMethod" pointcut-ref="validation1" /&gt; &lt;/aop:aspect&gt;&lt;/aop:config&gt; @Validated 和 @Valid @Valid是使用Hibernate Validation的时候使用。Java的JSR303声明了这类接口，然后hibernate－validator对其进行了实现。 @Validated是只用Spring Validator校验机制使用。 方法参数验证Spring提供了MethodValidationPostProcessor类，用于对方法的校验。 Controller中配置: @RestController@Validatedpublic class UserAction &#123; @PostMapping("/login") public Result login(@NotEmpty(message = "用户名不为空") String username, @NotEmpty(message = "密码不为空") String password) &#123; return ResultUtil.SUCCESS_RESULT; &#125;&#125; xml配置(最好是配置在&lt;mvc:annotation-driven validator=&quot;validator&quot; /&gt;上面，不然会有未知错误)如下： &lt;bean class="org.springframework.validation.beanvalidation.MethodValidationPostProcessor"&gt;&lt;/bean&gt; 在校验遇到非法的参数时会抛出ConstraintViolationException，可以通过getConstraintViolations获得所有没有通过的校验ConstraintViolation集合，可以通过它们来获得对应的消息。 我们同样使用 @ExceptionHandler 捕捉ConstraintViolationException异常处理全局异常信息。 然后将所有的错误信息封装好返回给前端。 @RestControllerAdvicepublic class MyExceptionHandler &#123; @ExceptionHandler(ConstraintViolationException.class) public Result handleConstraintViolationException(ConstraintViolationException e) &#123; List&lt;String&gt; list = new ArrayList&lt;String&gt;(); for (ConstraintViolation&lt;?&gt; s : e.getConstraintViolations()) &#123; System.out.println(s.getInvalidValue() + ": " + s.getMessage()); list.add(s.getMessage()); &#125; Result result = new Result(); result.setStatus("0"); result.setMessage(list); return result; &#125;&#125; 使用@Validated验证list现在我遇到一个新的需求，我需要前端给我传递一个对象数组，于是我使用一个list去接收，但是无法获得验证信息。 于是将list重新包装一下。 public class ValidList&lt;E&gt; &#123; @Valid private List&lt;E&gt; list; public List&lt;E&gt; getList() &#123; return list; &#125; public void setList(List&lt;E&gt; list) &#123; this.list = list; &#125;&#125; Controller中配置: @RestControllerpublic class UserAction &#123; @PostMapping("/login") public Result login(@Validated ValidList&lt;User&gt; users, Errors errors) &#123; System.out.println(users.getList()); if (errors.hasErrors()) &#123; return ResultUtil.messageResult(errors); &#125; return ResultUtil.SUCCESS_RESULT; &#125;&#125; 然后为了只返回第一个验证失败的信息(如果不更改，就会将所有的出错信息返回给前端)，更改ValidationAdvice如下： public class ValidationAdvice &#123; /** * 切点处理 * * @param pjp * @return * @throws Throwable */ public Object aroundMethod(ProceedingJoinPoint pjp) throws Throwable &#123; Object[] args = pjp.getArgs(); boolean isValidList = false; Errors errors = null; if (null != args &amp;&amp; args.length != 0) &#123; for (Object object : args) &#123; if (object instanceof ValidList) &#123; isValidList = true; &#125; if (object instanceof BindingResult) &#123; errors = (BindingResult) object; break; &#125; &#125; &#125; if (errors != null &amp;&amp; errors.hasErrors()) &#123; List&lt;ObjectError&gt; allErrors = errors.getAllErrors(); List&lt;String&gt; messages = new ArrayList&lt;String&gt;(); for (ObjectError error : allErrors) &#123; if (isValidList) &#123; messages.add(error.getDefaultMessage()); break; &#125; else &#123; messages.add(error.getDefaultMessage()); &#125; &#125; return ResultUtil.messageResult(messages); &#125; return pjp.proceed(); &#125;&#125; 这样即可验证list。 总结AOP的思想是贯穿我们的开发的，使用AOP的思想可以大大提高我们的开发效率，减少重复代码。 参考文档： springmvc参数校验-JSR303(Bean Validation） Java Bean Validation 最佳实践]]></content>
      <categories>
        <category>SpringMVC</category>
      </categories>
  </entry>
  <entry>
    <title><![CDATA[查询在一张表不在另外一张表的记录及效率探究]]></title>
    <url>%2Frecords-in-a-table-but-not-in-another-table%2F</url>
    <content type="text"><![CDATA[在我做项目的时候遇到一个需求，要将存在于表ta而不存在于表tb中的数据查询出来。 记录使用的方法和探讨效率。 数据准备创建表ta，并且使用存储过程插入13000条数据，在我的机器上运行时间: 346.719s。如果觉得插入的速度比较慢,可以直接导入我建好的表，百度云地址 http://pan.baidu.com/s/1dFtovg1 ，里面已经有数据了，直接导入sql执行即可，这样比用存储过程要快很多。 DROP TABLE IF EXISTS ta;CREATE TABLE `ta` ( `id` int(11) unsigned NOT NULL AUTO_INCREMENT, PRIMARY KEY (`id`)) ENGINE=InnoDB AUTO_INCREMENT=13000 DEFAULT CHARSET=utf8;DROP PROCEDURE IF EXISTS ta_insert;DELIMITER $$CREATE PROCEDURE ta_insert() MODIFIES SQL DATA BEGIN SET @i=1; SET @max=13000; WHILE @i&lt;@max DO INSERT INTO `ta` VALUES (); SET @i = @i + 1;END WHILE;end $$CALL ta_insert(); 创建表tb，并且使用存储过程插入10000条数据，在我的机器上运行时间: 224.102s。 DROP TABLE IF EXISTS tb;CREATE TABLE `tb` ( `id` int(11) unsigned NOT NULL AUTO_INCREMENT, PRIMARY KEY (`id`)) ENGINE=InnoDB AUTO_INCREMENT=8000 DEFAULT CHARSET=utf8;DROP PROCEDURE IF EXISTS tb_insert;DELIMITER $$CREATE PROCEDURE tb_insert() MODIFIES SQL DATA BEGIN SET @i=1; SET @max=8000; WHILE @i&lt;@max DO INSERT INTO `tb` VALUES (); SET @i = @i + 1;END WHILE;end $$CALL tb_insert(); 子查询使用NOT IN，ta表中的每一个id值都要去与tb表中的id匹配，匹配到就停止，也就是说，存在于ta表而不存在于tb表的id值需要与所有tb表中的id值进行匹配。 执行子查询时，MYSQL需要创建临时表，查询完毕后再删除这些临时表，所以，子查询的速度会受到一定的影响，这里多了一个创建和销毁临时表的过程。 平均时间为 0.04s。SELECT ta.id FROM ta WHERE ta.id IN (SELECT id FROM tb) 左连接使用 LEFT JOIN，ta表左连接tb表，而存在于ta表不存在于tb表中的字段为NULL，于是我们可以通过判断WHERE tb.id IS NULL来找到存在于ta表而不存在于tb表的id值。 平均时间是 0.06s。SELECT ta.id FROM ta LEFT JOIN tb ON ta.id = tb.id WHERE tb.id IS NULL 效率之谜版本问题？按理来说，连接应该比子查询要快，但是在我进行试验的时候发现却不是这样的，子查询居然还比连接要快。 搜索了解到 对于类似NOT IN这样的子查询，也能受益于subquery materialize，将子查询的结果集cache到临时表里，使用hashindex来进行检索；物化的子查询可以看到select_type字段为SUBQUERY，而在MySQL5.5里为DEPENDENT SUBQUERY 可能是版本原因，我用的是mysql5.7，可能做了优化。 于是使用mysql5.5再次测试。 发现子查询和左连接的查询时间都在0.12s附近，还是不能说明连接比子查询高效。进一步猜测，我的数据组织格式是否出现了问题，于是使用上面百度云盘连接中的mm_member表和mm_log表(这是 mysql（4）—— 表连接查询与where后使用子查询的性能分析。 提供的数据) 。 猜测验证子查询 SELECT mm_member.id FROM mm_member WHERE mm_member.id NOT IN (SELECT DISTINCT mm_log.member_id FROM mm_log) 左连接 SELECT mm_member.id FROM mm_member LEFT JOIN (SELECT DISTINCT mm_log.member_id FROM mm_log ) AS mmON mm.member_id = mm_member.id WHERE mm.member_id IS NULL mysql5.7 子查询为1.1s左右，左连接为1.55s，子查询依然速度较快。 mysql5.5 子查询为48s左右，左连接为1.4s，将近34倍的差距，由此印证上面引用的那部分，mysql5.7确实已经多子查询做了优化，使其达到了逼近左连接的效率。 那为什么我自己所建立的表无法体现版本的这种性能差别？ 猜测应该是数据类型的原因，可能int类型的查询效率已经都优化好了。 网传最高效不太清楚其中的原理，并且在我的测试中性能跟连接差不多。 SELECT id FROM ta WHERE (SELECT COUNT(1) AS num FROM tb WHERE ta.id = tb.id) = 0 而且网上流传的版本((数据库篇) SQL查询~ 存在一个表而不在另一个表中的数据)为 select from B where (select count(1) as num from A where A.ID = B.ID) = 0应该是select from A where (select count(1) as num from B where A.ID = B.ID) = 0大表在前，小表在后。 注意： 存储过程循环插入比普通方式插入数据慢很多倍。 索引可以有效提高搜索效率。 不是所有的子查询都比连接慢的。 参考文档： MySQL 5.6的优化器改进 mysql（4）—— 表连接查询与where后使用子查询的性能分析。]]></content>
      <categories>
        <category>数据库</category>
      </categories>
      <tags>
        <tag>MySQL</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[get请求中文乱码及get,post编码探究]]></title>
    <url>%2Fget-request-Chinese-garbled%2F</url>
    <content type="text"><![CDATA[在我使用get请求进行查询的时候遇到一个问题：当我的请求参数中有中文时，出现乱码。可是即使我设置了Spring的characterEncodingFilter，也还是出现乱码。原因：tomcat默认使用ISO8859-1编码来解析get中的url参数，导致乱码。而characterEncodingFilter或者request.setCharacterEncoding(&quot;UTF-8&quot;);都只针对post请求体有效。下面对Http中get方法编码到tomcat的解码过程进行探究。 解决方法 更改tomcat中get方法默认ISO8859-1编码为utf-8编码。找到conf/server.xml,在 &lt;Connector port=&quot;8082&quot; protocol=&quot;HTTP/1.1&quot; 中加入 URIEncoding=&quot;utf-8&quot;。 将参数以iso8859-1编码转化为字节数组，然后再以UTF-8将字节数组转化为字符串。userName = new String(userName.getBytes(&quot;ISO8859-1&quot;), &quot;UTF-8&quot;); URL是怎么编码的？参考 关于URL编码 一般来说，URL只能使用英文字母、阿拉伯数字和某些标点符号，不能使用其他文字和符号。比如，世界上有英文字母的网址”http://www.abc.com&quot;，但是没有希腊字母的网址&quot;http://www.aβγ.com&quot;（读作阿尔法-贝塔-伽玛.com）。这是因为网络标准RFC 1738做了硬性规定。 这意味着，如果URL中有汉字，就必须编码后使用。但是麻烦的是，RFC 1738没有规定具体的编码方法，而是交给应用程序（浏览器）自己决定。这导致”URL编码”成为了一个混乱的领域。 不同的操作系统、不同的浏览器、不同的网页字符集，将导致完全不同的编码结果。经过测试，现在的浏览器大部分都是utf-8编码。但是为了兼容所有的浏览器，可以使用Javascript函数：encodeURI()。 encodeURI()是Javascript中真正用来对URL编码的函数。它着眼于对整个URL进行编码，因此除了常见的符号以外，对其他一些在网址中有特殊含义的符号”; / ? : @ &amp; = + $ , #“，也不进行编码。编码后，它输出符号的utf-8形式，并且在每个字节前加上%。 它对应的解码函数是decodeURI()。 tomcat是怎么解码的？get请求是使用url编码方式，而post请求基于请求体自身的编码。 推荐 get请求含有url参数时，使用js自带的编码函数进行编码。 post请求在content-type中设置charset=utf-8，否则使用页面默认编码。 get方法的编码查看tomcat源码中，org.apache.catalina.connector.CoyoteAdapter的方法：使用在conf/server.xml中 &lt;Connector port=&quot;8082&quot; protocol=&quot;HTTP/1.1&quot;&gt;配置的URIEncoding作为将前端传过来的参数转化为字符数组的编码，缺省为ISO8859-1。 protected void convertURI(MessageBytes uri, Request request) throws Exception &#123; ByteChunk bc = uri.getByteChunk(); int length = bc.getLength(); CharChunk cc = uri.getCharChunk(); cc.allocate(length, -1); // 使用默认编码 ISO8859-1 将字节数组编程字符 String enc = connector.getURIEncoding(); if (enc != null) &#123; B2CConverter conv = request.getURIConverter(); try &#123; if (conv == null) &#123; conv = new B2CConverter(enc, true); request.setURIConverter(conv); &#125; else &#123; conv.recycle(); &#125; &#125; catch (IOException e) &#123; log.error("Invalid URI encoding; using HTTP default"); connector.setURIEncoding(null); &#125; if (conv != null) &#123; try &#123; conv.convert(bc, cc, true); uri.setChars(cc.getBuffer(), cc.getStart(), cc.getLength()); return; &#125; catch (IOException ioe) &#123; // Should never happen as B2CConverter should replace // problematic characters request.getResponse().sendError( HttpServletResponse.SC_BAD_REQUEST); &#125; &#125; &#125; // Default encoding: fast conversion for ISO-8859-1 byte[] bbuf = bc.getBuffer(); char[] cbuf = cc.getBuffer(); int start = bc.getStart(); for (int i = 0; i &lt; length; i++) &#123; cbuf[i] = (char) (bbuf[i + start] &amp; 0xff); &#125; uri.setChars(cbuf, 0, length);&#125; post方法的字符编码 如果在servlet的doPost方法中或者filter中设置了request的字符编码，那么就以设置的为准。 request设置编码 public void doPost(HttpServletRequestrequest,HttpServletResponse response) throws IOException,ServletException&#123;//必须在getParameter,getParameterNames,//getParameterValues方法调用之前进行设置request.setContentType("UTF-8");&#125; web.xml中配置filter &lt;filter&gt; &lt;filter-name&gt;SetCharacterEncoding&lt;/filter-name&gt; &lt;filter-class&gt;org.apache.catalina.filters.SetCharacterEncodingFilter&lt;/filter-class&gt; &lt;init-param&gt; &lt;param-name&gt;encoding&lt;/param-name&gt; &lt;param-value&gt;UTF-8&lt;/param-value&gt; &lt;/init-param&gt;&lt;/filter&gt; 如果没有进行上面的配置，那么从http header中取出content-type,然后从content-type的值中取出charset的值，charset的值作为post的字符编码。如 content-type=application/x-www-form-urlencoded;charset=utf-8那么，post的字符编码就是utf-8。如果从http header中没有取到content-type中的charset，那么，就使用缺省的ISO-8859-1。 参考文档 关于URL编码 get请求中url传参中文乱码问题–集锦]]></content>
      <categories>
        <category>网络</category>
      </categories>
      <tags>
        <tag>Http</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[spring使用redis做缓存]]></title>
    <url>%2Fspring-redis-cache%2F</url>
    <content type="text"><![CDATA[缓存什么是缓存？在高并发下，为了提高访问的性能，需要将数据库中 一些经常展现和不会频繁变更的数据，存放在存取速率更快的内存中。这样可以 降低数据的获取时间，带来更好的体验 减轻数据库的压力 缓存适用于读多写少的场合，查询时缓存命中率很低、写操作很频繁等场景不适宜用缓存。 MySQL有自己的查询缓存，为什么还要使用 Redis 等缓存应用？ 当只有一台 MySQL服务器时，可以将缓存放置在本地。这样当有相同的 SQL 查询到达时，可以直接从缓存中取到查询结果，不需要进行 SQL 的解析和执行。MySQL 提供了服务器层面的缓存支持。 如果有多台 MySQL 服务器，请求会随机分发给多台中的一台，我们无法保证相同的请求会到达同一台服务器，本地缓存命中率较低。所以基于本机的缓存就没有什么意义，此时采用的策略应该是将查询结果缓存在 Redis 或者 Memcache 中。 而Redis是一个高性能的 key-value 内存数据库，恰恰可以作为缓存使用。GitHub 地址：https://github.com/antirez/redis 。Github 是这么描述的：Redis is an in-memory database that persists on disk. The data model is key-value, but many different kind of values are supported: Strings, Lists, Sets, Sorted Sets, Hashes, HyperLogLogs, Bitmaps. 但是mysql自己本身有查询缓存，memcached也是一个优秀的内存数据库，为什么一定要选择redis 缓存更新查看缓存更新的套路，缓存更新的模式有四种： Cache aside Read through Write through Write behind cachin。 这里我们使用的是 Cache Aside 策略，从三个维度： 命中：应用程序从cache中取数据，取到后返回。执行图中1,2步 失效：应用程序先从cache取数据，没有得到，则从数据库中取数据，成功后，放到缓存中。执行图中1,2,3，4,1，2步 更新：先把数据存到数据库中，成功后，再让缓存失效。执行图中1，2步 spring配置redis缓存接下来讲解一下spring的配置。 依赖配置pom.xml中添加 &lt;!-- redis cache--&gt;&lt;dependency&gt; &lt;groupId&gt;redis.clients&lt;/groupId&gt; &lt;artifactId&gt;jedis&lt;/artifactId&gt; &lt;version&gt;2.8.1&lt;/version&gt;&lt;/dependency&gt;&lt;dependency&gt; &lt;groupId&gt;org.springframework.data&lt;/groupId&gt; &lt;artifactId&gt;spring-data-redis&lt;/artifactId&gt; &lt;version&gt;1.7.2.RELEASE&lt;/version&gt;&lt;/dependency&gt; RedisConfig现在我们使用的是java config 配置，因此需要将本RedisConfig放在可以被&lt;context:component-scan base-package=&quot;&quot;/&gt;扫描的包下。 import com.fasterxml.jackson.annotation.JsonAutoDetect;import com.fasterxml.jackson.annotation.PropertyAccessor;import com.fasterxml.jackson.databind.ObjectMapper;import org.springframework.cache.CacheManager;import org.springframework.cache.annotation.CachingConfigurerSupport;import org.springframework.cache.annotation.EnableCaching;import org.springframework.cache.interceptor.KeyGenerator;import org.springframework.context.annotation.Bean;import org.springframework.context.annotation.Configuration;import org.springframework.data.redis.cache.RedisCacheManager;import org.springframework.data.redis.connection.RedisConnectionFactory;import org.springframework.data.redis.connection.jedis.JedisConnectionFactory;import org.springframework.data.redis.core.RedisTemplate;import org.springframework.data.redis.serializer.Jackson2JsonRedisSerializer;import org.springframework.data.redis.serializer.StringRedisSerializer;import java.lang.reflect.Method;/** * @author 李文浩 * @version 2017/11/5. */@Configuration@EnableCachingpublic class RedisConfig extends CachingConfigurerSupport &#123; @Bean public JedisConnectionFactory redisConnectionFactory() &#123; JedisConnectionFactory redisConnectionFactory = new JedisConnectionFactory(); // Defaults redisConnectionFactory.setHostName("127.0.0.1"); redisConnectionFactory.setPort(6379); return redisConnectionFactory; &#125; @Bean public RedisTemplate&lt;String, String&gt; redisTemplate(RedisConnectionFactory factory) &#123; RedisTemplate&lt;String, String&gt; redisTemplate = new RedisTemplate&lt;String, String&gt;(); redisTemplate.setConnectionFactory(factory); redisTemplate.afterPropertiesSet(); setSerializer(redisTemplate); return redisTemplate; &#125; @Bean public CacheManager cacheManager(RedisTemplate redisTemplate) &#123; RedisCacheManager rcm = new RedisCacheManager(redisTemplate); // 设置缓存过期时间，秒 rcm.setDefaultExpiration(600); return rcm; &#125; private void setSerializer(RedisTemplate&lt;String, String&gt; template) &#123; Jackson2JsonRedisSerializer jackson2JsonRedisSerializer = new Jackson2JsonRedisSerializer(Object.class); ObjectMapper om = new ObjectMapper(); om.setVisibility(PropertyAccessor.ALL, JsonAutoDetect.Visibility.ANY); om.enableDefaultTyping(ObjectMapper.DefaultTyping.NON_FINAL); jackson2JsonRedisSerializer.setObjectMapper(om); template.setKeySerializer(new StringRedisSerializer()); template.setValueSerializer(jackson2JsonRedisSerializer); &#125; @Override @Bean public KeyGenerator keyGenerator() &#123; return new KeyGenerator() &#123; @Override public Object generate(Object target, Method method, Object... params) &#123; StringBuilder sb = new StringBuilder(); sb.append(target.getClass().getName()); sb.append(":" + method.getName()); for (Object obj : params) &#123; sb.append(":" + null == obj ? "null" : obj.toString()); &#125; return sb.toString(); &#125; &#125;; &#125;&#125; 如果我们不配置重写keyGenerator()方法的话，默认的key生成策略是 Cacheable.java/** * Spring Expression Language (SpEL) expression for computing the key dynamically. * &lt;p&gt;Default is &#123;@code ""&#125;, meaning all method parameters are considered as a key, * unless a custom &#123;@link #keyGenerator&#125; has been configured. * &lt;p&gt;The SpEL expression evaluates against a dedicated context that provides the * following meta-data: * &lt;ul&gt; * &lt;li&gt;&#123;@code #root.method&#125;, &#123;@code #root.target&#125;, and &#123;@code #root.caches&#125; for * references to the &#123;@link java.lang.reflect.Method method&#125;, target object, and * affected cache(s) respectively.&lt;/li&gt; * &lt;li&gt;Shortcuts for the method name (&#123;@code #root.methodName&#125;) and target class * (&#123;@code #root.targetClass&#125;) are also available. * &lt;li&gt;Method arguments can be accessed by index. For instance the second argument * can be accessed via &#123;@code #root.args[1]&#125;, &#123;@code #p1&#125; or &#123;@code #a1&#125;. Arguments * can also be accessed by name if that information is available.&lt;/li&gt; * &lt;/ul&gt; */String key() default ""; 也就是把所有的方法参数作为一个key，但是这可能会重复。 缓存注解 @CacheConfig：主要用于配置该类中会用到的一些共用的缓存配置。在这里@CacheConfig(cacheNames = &quot;companies&quot;)，配置了该数据访问对象中返回的内容将存储于名为companies的缓存对象中，我们也可以不使用该注解，直接通过@Cacheable自己配置缓存集的名字来定义。 @Cacheable： 声明Spring在调用方法之前，首先应该在缓存中查找方法的返回值。如果这个值能够找到，就会返回存储的值，否则的话，这个方法就会被调用，返回值会放在缓存之中。该注解主要有下面几个参数： value、cacheNames：两个等同的参数（cacheNames为Spring4新增，作为value的别名），用于指定缓存存储的集合名。由于Spring4中新增了@CacheConfig，因此在Spring3中原本必须有的value属性，也成为非必需项了 key：缓存对象存储在Map集合中的key值，非必需，缺省按照函数的所有参数组合作为key值，若自己配置需使用SpEL表达式，比如：@Cacheable(key = “#p0”)：使用函数第一个参数作为缓存的key值，更多关于SpEL表达式的详细内容可参考官方文档 condition：缓存对象的条件，非必需，也需使用SpEL表达式，只有满足表达式条件的内容才会被缓存，比如：@Cacheable(key = “#p0”, condition = “#p0.length() &lt; 3”)，表示只有当第一个参数的长度小于3的时候才会被缓存，若做此配置上面的AAA用户就不会被缓存，读者可自行实验尝试。 unless：另外一个缓存条件参数，非必需，需使用SpEL表达式。它不同于condition参数的地方在于它的判断时机，该条件是在函数被调用之后才做判断的，所以它可以通过对result进行判断。 keyGenerator：用于指定key生成器，非必需。若需要指定一个自定义的key生成器，我们需要去实现org.springframework.cache.interceptor.KeyGenerator接口，并使用该参数来指定。需要注意的是：该参数与key是互斥的 cacheManager：用于指定使用哪个缓存管理器，非必需。只有当有多个时才需要使用 cacheResolver：用于指定使用那个缓存解析器，非必需。需通过org.springframework.cache.interceptor.CacheResolver接口来实现自己的缓存解析器，并用该参数指定。 除了这里用到的两个注解之外，还有下面几个核心注解： @CachePut： 表明Spring应该将方法的返回值放到缓存中，在方法的调用前并不会检查缓存，方法始终都会被调用。它的参数与@Cacheable类似，具体功能可参考上面对@Cacheable参数的解析。 @CacheEvict：配置于函数上，通常用在删除方法上，用来从缓存中移除相应数据。除了同@Cacheable一样的参数之外，它还有下面两个参数： allEntries：非必需，默认为false。当为true时，会移除所有数据 beforeInvocation：非必需，默认为false，会在调用方法之后移除数据。当为true时，会在调用方法之前移除数据。 缓存与数据库一致性 数据库处理要求强一致实时性的数据，例如金融数据、交易数据。 Redis处理不要求强一致实时性的数据，例如网站最热贴排行榜。 也就是说根据你的业务需求，设置你的过期时间，容许redis有一些不一致。 注意： 缓存java对象时必须实现Serilaizable接口，因为Spring会将对象先序列化之后再存入到Redis中。 缓存方法的 @Cacheable 最好使用方法名，避免不同的方法的 @Cacheable 值一致，然后再配以以上缓存策略。 在我将这个@Cacheable放置在SSM的dao层和service层时，redis缓存可以正常运行，但是当我将@Cacheable放在action层上时就会有NPE。 @Cacheable没有配置名字，改为@Cacheable(&quot;值&quot;)，否则会出现如下错误。 java.lang.IllegalStateException: No cache could be resolved for 'Builder[public abstract studio.jikewang.entity.TeacherClass studio.jikewang.dao.TeacherClassDao.getTeacherClass(int)] caches=[] | key='' | keyGenerator='' | cacheManager='' | cacheResolver='' | condition='' | unless='' | sync='false'' using resolver 'org.springframework.cache.interceptor.SimpleCacheResolver@4f8d471b'. At least one cache should be provided per cache operation. 参考文档： Redis 缓存 + Spring 的集成示例 Spring Boot 使用Redis缓存 MySQL缓存–服务器缓存query cache]]></content>
      <categories>
        <category>Spring</category>
      </categories>
      <tags>
        <tag>Redis</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[单点登录之tomcat中session在两个webapp中实现共享]]></title>
    <url>%2Ftomcat-webapps-session-sharing%2F</url>
    <content type="text"><![CDATA[现在遇到一个需求就是要求完成简单的单点登录，通过在一个tomcat实例中放置两个webapps应用ROOT应用和CEO应用来完成在ROOT应用登录后，在CEO可以直接使用，而未在ROOT应用登录时，不可以进去CEO应用。实际上问题就是session如何在两个webapp中实现共享，通过上网搜索发现一个方法 方法1ServletContextserver.xml文件修改如下：&lt;Host name="localhost" appBase="webapps"unpackWARs="true" autoDeploy="true"&gt; //WebappA为项目名，crossContext="true" &lt;Context path="/WebappA" debug="9" reloadable="true" crossContext="true"/&gt; &lt;Context path="/WebappB" debug="9" reloadable="true" crossContext="true"/&gt; &lt;/Host&gt; crossContext属性的意思是：如果设置为true，你可以通过ServletContext.getContext() 调用另外一个WEB应用程序，获得ServletContext 然后再调用其getAttribute() 得到你要的对象。Java代码如下：WebappA:HttpSession session = request.getSession(); session.setAttribute("userId", "test"); ServletContext ContextA =session .getServletContext(); ContextA.setAttribute("session", session ); WebappB:HttpSession sessionB = request.getSession(); ServletContext ContextB = sessionB.getServletContext(); ServletContext ContextA= ContextB.getContext("/WebappA");// 这里面传递的是 WebappA的虚拟路径 HttpSession sessionA =(HttpSession)ContextA.getAttribute("session"); System.out.println("userId: "+sessionA.getAttribute("userId")); 初看这个方法，好像是完成我们的目标，可是在我实际应用时发现一个问题，就是当user1在登录前不可以进入CEO应用，在user1登录后才可以进入CEO应用，但是当user1退出之后，未登录的用户依然可以进入CEO应用。后来仔细看了一下网上提供的方法，它只是在webappA的ServletContext存储了一个session值，然后传递给webAPPB，但是也仅仅只能传递一个session值，如果有两个用户的时候就会出现session覆盖。 于是探究其他解决方法。 方法2sessionCookiePath在tomcat conf/context.html中有如下配置 sessionCookieNameThe name to be used for all session cookies created for this context. If set, this overrides any name set by the web application. If not set, the value specified by the web application, if any, will be used, or the name JSESSIONID if the web application does not explicitly set one. sessionCookiePathThe path to be used for all session cookies created for this context. If set, this overrides any path set by the web application. If not set, the value specified by the web application will be used, or the context path used if the web application does not explicitly set one. To configure all web application to use an empty path (this can be useful for portlet specification implementations) set this attribute to / in the global CATALINA_BASE/conf/context.xml file.Note: Once one web application using sessionCookiePath=”/“ obtains a session, all subsequent sessions for any other web application in the same host also configured with sessionCookiePath=”/“ will always use the same session ID. This holds even if the session is invalidated and a new one created. This makes session fixation protection more difficult and requires custom, Tomcat specific code to change the session ID shared by the multiple applications. 也就是说我们可以通过sessionCookiePath属性使得一个tomcat实例下所有的webapps都共享一个session，通过sessionCookieName来指定sessionCookieName名字。 于是我就在tomcat conf/context.html中配置如下：&lt;Context sessionCookiePath="/" sessionCookieName="SESSIONID" &gt;&lt;/Context&gt; 然后在进行测试，发现在ROOT应用和CEO应用中确实sessionCookie是一样的。可是当我在ROOT中进行session.setAttribute();时，CEO应用不能从session中取得值，为null，也就是说，对CEO应用而言，ROOT应用在session所储存的值是不可见的。然后在CEO session中进行session.setAttribute();，ROOT应用总同样无法取得CEO存储在session中的数据，猜想可能是不同的webapps并不会共享相同的session内存，每一个webapps维护自己session HashTable，后来了解到 session管理器是和context容器关联的，也就说每个web应用都会有一个session管理器，所以CEO应用当然无法从ROOT应用存储的session中取值。 方法3redis session 共享以前曾经了解过Nginx+tomcat+redis做负载均衡的内容，知道可以把session数据存储到redis中，然后tomcat再去redis取值。而这次的tomcat中session在两个webapp中实现共享其实也可以通过这个方法进行处理。 在tomcat conf/context.html中配置如下： &lt;Context sessionCookiePath="/" sessionCookieName="SESSIONID" &gt;&lt;Valve className="com.orangefunction.tomcat.redissessions.RedisSessionHandlerValve" /&gt; &lt;Manager className="com.orangefunction.tomcat.redissessions.RedisSessionManager" host="172.22.4.16" port="6379" database="0" maxInactiveInterval="60" /&gt;&lt;/Context&gt; 然后即可实现tomcat中session在两个webapp中实现共享。 jar包和windows redis安装包http://pan.baidu.com/s/1eSITNnc session过期策略tomcat-session怎么实现的过期策略？首先如果没有使用redis做session缓存，tomcat服务器在启动的时候初始化了一个守护线程,定期6*10秒去检查有没有Session过期.过期则清除。而使用的tomcat-session-redis做缓存，那么session过期之后就由redis进行删除，redis通过惰性删除和定期删除来删除过期的sessionID值。 当然sessionID还存在于客户端，那么客户端的sessionID清理过程是什么？经过测试是当tomcat删除sessionID值之后，tomcat会重新生成一个sessionID值返回给客户端。 总结：其实我这个功能就是单点登录，也就是说在A应用登录的情况下可以访问B应用，但是即使设置了sessionCookiePath，session的Attribute并没有共享，于是想到了先把session序列化到redis中，然后取出来判断，这样就可以实现单点登录。核心是session在这两个应用中必须是一样的，通过设置sessionCookiePath。 注意： 当你使用自己的对象执行session.setAttribute();时，必须实现Serializable接口，不然无法进行序列化。 maxInactiveInterval 不起作用tomcat日志描述：警告: Manager.setMaxInactiveInterval() is deprecated and calls to this method are ignored. Session timeouts should be configured in web.xml or via Context.setSessionTimeout(int timeoutInMinutes).信息: Will expire sessions after 120 seconds(默认是30分钟)只能通过在ROOT下的web.xml或者全局的web.xml(CEO中的session-config无法生效)中配置才可生效。 &lt;!--设置session过期时间--&gt;&lt;session-config&gt; &lt;session-timeout&gt;20&lt;/session-timeout&gt;&lt;/session-config&gt; 配置成功后tomcat日志： 信息: Will expire sessions after 120 seconds 在context中配置host为静态ip 172.22.4.16报错如下:，Google发现原来redis和mysql一样都是已经默认绑定了localhost，只允许本机访问，于是更改redis.windows-service.conf如下 bind 127.0.0.1 172.22.4.16 之后就可以使用静态ip 172.22.4.16进行访问。 msi应用的安装，修复和卸载都是通过点击msi文件。 还有一个共享session的方案是spring-session。 spring-session中通过自己生成session并且存储到redis中，还是需要设置sessionCookiePath=&quot;/&quot;(在一个tomcat两个应用需要单点登录的情况)，其他session共享方案(在多个tomcat中实现单点登录可以参考 spring session无法实现共享（多web应用）)，但是spring-session不是服务器级别的，而是web 应用级别的，不受服务器如tomcat，jetty，jboss的限制。 参考文档 http://tomcat.apache.org/tomcat-7.0-doc/config/context.html 搭建Tomcat集群&amp;通过Redis缓存共享session的一种流行方案 Tomcat的Session过期处理策略 Tomcat中session的管理机制]]></content>
      <categories>
        <category>服务器</category>
      </categories>
      <tags>
        <tag>Tomcat</tag>
        <tag>Redis</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[JDK8 HashMap 源码解析]]></title>
    <url>%2FHashMap%2F</url>
    <content type="text"><![CDATA[HashMap中数据结构在jdk1.7中，HashMap采用数组+链表(拉链法)。因为数组是一组连续的内存空间，易查询，不易增删，而链表是不连续的内存空间，通过节点相互连接，易删除，不易查询。HashMap结合这两者的优秀之处来提高效率。 而在jdk1.8时，为了解决当hash碰撞过于频繁，而链表的查询效率(时间复杂度为O(n))过低时，当链表的长度达到一定值(默认是8)时，将链表转换成红黑树(时间复杂度为O(lg n))，极大的提高了查询效率。 如图所示： HashMap初始化以下代码未经特别声明，都是jdk1.8。 /** * The default initial capacity - MUST be a power of two. */static final int DEFAULT_INITIAL_CAPACITY = 1 &lt;&lt; 4; // aka 16 /** * Constructs an empty &lt;tt&gt;HashMap&lt;/tt&gt; with the default initial capacity * (16) and the default load factor (0.75). */public HashMap() &#123; this.loadFactor = DEFAULT_LOAD_FACTOR; // all other fields defaulted&#125; HashMap的默认大小是16。查看HashMap的构造方法，发现没有执行new操作，猜测可能跟ArrayList一样是在第一次add的时候开辟的内存，于是查看put方法。 put方法关于Node节点HashMap将hash，key，value，next已经封装到一个静态内部类Node上。它实现了Map.Entry&lt;K,V&gt;接口。 static class Node&lt;K,V&gt; implements Map.Entry&lt;K,V&gt; &#123; final int hash; final K key; V value; Node&lt;K,V&gt; next; Node(int hash, K key, V value, Node&lt;K,V&gt; next) &#123; this.hash = hash; this.key = key; this.value = value; this.next = next; &#125; public final K getKey() &#123; return key; &#125; public final V getValue() &#123; return value; &#125; public final String toString() &#123; return key + "=" + value; &#125; public final int hashCode() &#123; return Objects.hashCode(key) ^ Objects.hashCode(value); &#125; public final V setValue(V newValue) &#123; V oldValue = value; value = newValue; return oldValue; &#125; public final boolean equals(Object o) &#123; if (o == this) return true; if (o instanceof Map.Entry) &#123; Map.Entry&lt;?,?&gt; e = (Map.Entry&lt;?,?&gt;)o; if (Objects.equals(key, e.getKey()) &amp;&amp; Objects.equals(value, e.getValue())) return true; &#125; return false; &#125;&#125; 然后在定义一个Node数组table transient Node&lt;K,V&gt;[] table; hash实现当我们put的时候，首先计算 key的hash值，这里调用了 hash方法，hash方法实际是让key.hashCode()与key.hashCode()&gt;&gt;&gt;16进行异或操作，高16bit补0，一个数和0异或不变，所以 hash 函数大概的作用就是：高16bit不变，低16bit和高16bit做了一个异或，目的是减少碰撞。按照函数注释，因为bucket数组大小是2的幂，计算下标index = (table.length - 1) &amp; hash，如果不做 hash 处理，相当于散列生效的只有几个低 bit 位，为了减少散列的碰撞，设计者综合考虑了速度、作用、质量之后，使用高16bit和低16bit异或来简单处理减少碰撞，而且JDK8中用了复杂度 O（logn）的树结构来提升碰撞下的性能。 public V put(K key, V value) &#123; return putVal(hash(key), key, value, false, true);&#125;static final int hash(Object key) &#123; int h; return (key == null) ? 0 : (h = key.hashCode()) ^ (h &gt;&gt;&gt; 16);&#125; 实现方法 如果当前数组table为null，进行resize()初始化 否则计算数组索引i = (n - 1) &amp; hash 如果这个table[i]值为空，那么就将这个Node键值对放在这里 判断key是否与table[i]重复，重复则替换 不重复在判断table[i]是否连接了一个链表，链表为空则new 一个Node键值对，链表不为空就循环直到最后一个节点的next为null或者出现出现重复key值 final V putVal(int hash, K key, V value, boolean onlyIfAbsent, boolean evict) &#123; Node&lt;K,V&gt;[] tab; Node&lt;K,V&gt; p; int n, i; // 默认容量初始化 if ((tab = table) == null || (n = tab.length) == 0) n = (tab = resize()).length; //如果table[i]为空，那就把这个键值对放在table[i], i = (n - 1) &amp; hash 相等于 hash % n, //但是hash后按位与 n-1，比%模运算取余要快 if ((p = tab[i = (n - 1) &amp; hash]) == null) tab[i] = newNode(hash, key, value, null); //当另一个key的hash值已经存在时 else &#123; Node&lt;K,V&gt; e; K k; // table[i].key == key if (p.hash == hash &amp;&amp; ((k = p.key) == key || (key != null &amp;&amp; key.equals(k)))) e = p; //JDK8在哈希碰撞的链表长度达到TREEIFY_THRESHOLD（默认8)后， //会把该链表转变成树结构，提高了性能。 else if (p instanceof TreeNode) e = ((TreeNode&lt;K,V&gt;)p).putTreeVal(this, tab, hash, key, value); else &#123; //遍历table[i]所对应的链表，直到最后一个节点的next为null或者有重复的key值 for (int binCount = 0; ; ++binCount) &#123; if ((e = p.next) == null) &#123; p.next = newNode(hash, key, value, null); if (binCount &gt;= TREEIFY_THRESHOLD - 1) // -1 for 1st treeifyBin(tab, hash); break; &#125; if (e.hash == hash &amp;&amp; ((k = e.key) == key || (key != null &amp;&amp; key.equals(k)))) break; p = e; &#125; &#125; //key重复，替换value if (e != null) &#123; // existing mapping for key V oldValue = e.value; if (!onlyIfAbsent || oldValue == null) e.value = value; afterNodeAccess(e); return oldValue; &#125; &#125; ++modCount; if (++size &gt; threshold) resize(); afterNodeInsertion(evict); return null;&#125;// Callbacks to allow LinkedHashMap post-actionsvoid afterNodeAccess(Node&lt;K,V&gt; p) &#123; &#125;void afterNodeInsertion(boolean evict) &#123; &#125;void afterNodeRemoval(Node&lt;K,V&gt; p) &#123; &#125; get方法首先通过hash函数找到索引，然后判断map为null，再判断table[i]是否等于key，然后在找与table相连的链表的key是否相等。 public V get(Object key) &#123; Node&lt;K,V&gt; e; return (e = getNode(hash(key), key)) == null ? null : e.value;&#125;final Node&lt;K,V&gt; getNode(int hash, Object key) &#123; Node&lt;K,V&gt;[] tab; Node&lt;K,V&gt; first, e; int n; K k; if ((tab = table) != null &amp;&amp; (n = tab.length) &gt; 0 &amp;&amp; (first = tab[(n - 1) &amp; hash]) != null) &#123; if (first.hash == hash &amp;&amp; // always check first node ((k = first.key) == key || (key != null &amp;&amp; key.equals(k)))) return first; if ((e = first.next) != null) &#123; if (first instanceof TreeNode) return ((TreeNode&lt;K,V&gt;)first).getTreeNode(hash, key); do &#123; if (e.hash == hash &amp;&amp; ((k = e.key) == key || (key != null &amp;&amp; key.equals(k)))) return e; &#125; while ((e = e.next) != null); &#125; &#125; return null;&#125; jdk1.7中的线程安全问题(resize死循环)当HashMap的size超过Capacity*loadFactor时，需要对HashMap进行扩容。具体方法是，创建一个新的，长度为原来Capacity两倍的数组，保证新的Capacity仍为2的N次方，从而保证上述寻址方式仍适用。同时需要通过如下transfer方法将原来的所有数据全部重新插入（rehash）到新的数组中。 下列代码基于 jdk1.7.0_79 void transfer(Entry[] newTable, boolean rehash) &#123; int newCapacity = newTable.length; for (Entry&lt;K,V&gt; e : table) &#123; while(null != e) &#123; Entry&lt;K,V&gt; next = e.next; if (rehash) &#123; e.hash = null == e.key ? 0 : hash(e.key); &#125; int i = indexFor(e.hash, newCapacity); e.next = newTable[i]; newTable[i] = e; e = next; &#125; &#125;&#125; 该方法并不保证线程安全，而且在多线程并发调用时，可能出现死循环。其执行过程如下。从步骤2可见，转移时链表顺序反转。 遍历原数组中的元素 对链表上的每一个节点遍历：用next取得要转移那个元素的下一个，将e转移到新数组的头部，使用头插法插入节点 循环2，直到链表节点全部转移 循环1，直到所有元素全部转移 单线程rehash单线程情况下，rehash无问题。下图演示了单线程条件下的rehash过程 多线程并发下的rehash这里假设有两个线程同时执行了put操作并引发了rehash，执行了transfer方法，并假设线程一进入transfer方法并执行完next = e.next后，因为线程调度所分配时间片用完而“暂停”，此时线程二完成了transfer方法的执行。此时状态如下。 接着线程1被唤醒，继续执行第一轮循环的剩余部分 e.next = newTable[1] = nullnewTable[1] = e = key(5)e = next = key(9) 结果如下图所示 接着执行下一轮循环，结果状态图如下所示 此时循环链表形成，并且key(11)无法加入到线程1的新数组。在下一次访问该链表时会出现死循环。 jdk1.8中的扩容在jdk1.8中采用resize方法来对HashMap进行扩容。 resize方法final Node&lt;K,V&gt;[] resize() &#123; Node&lt;K,V&gt;[] oldTab = table; int oldCap = (oldTab == null) ? 0 : oldTab.length; int oldThr = threshold; int newCap, newThr = 0; if (oldCap &gt; 0) &#123; // 超过最大值就不再扩充了，就只好随你碰撞去吧 if (oldCap &gt;= MAXIMUM_CAPACITY) &#123; threshold = Integer.MAX_VALUE; return oldTab; &#125; // 没超过最大值，就扩充为原来的2倍 else if ((newCap = oldCap &lt;&lt; 1) &lt; MAXIMUM_CAPACITY &amp;&amp; oldCap &gt;= DEFAULT_INITIAL_CAPACITY) newThr = oldThr &lt;&lt; 1; // double threshold &#125; else if (oldThr &gt; 0) // initial capacity was placed in threshold newCap = oldThr; else &#123; // zero initial threshold signifies using defaults newCap = DEFAULT_INITIAL_CAPACITY; newThr = (int)(DEFAULT_LOAD_FACTOR * DEFAULT_INITIAL_CAPACITY); &#125; // 计算新的resize上限 if (newThr == 0) &#123; float ft = (float)newCap * loadFactor; newThr = (newCap &lt; MAXIMUM_CAPACITY &amp;&amp; ft &lt; (float)MAXIMUM_CAPACITY ? (int)ft : Integer.MAX_VALUE); &#125; threshold = newThr; @SuppressWarnings(&#123;"rawtypes","unchecked"&#125;) Node&lt;K,V&gt;[] newTab = (Node&lt;K,V&gt;[])new Node[newCap]; table = newTab; if (oldTab != null) &#123; // 把每个bucket都移动到新的buckets中 for (int j = 0; j &lt; oldCap; ++j) &#123; Node&lt;K,V&gt; e; if ((e = oldTab[j]) != null) &#123; // 清除原来table[i]中的值 oldTab[j] = null; if (e.next == null) newTab[e.hash &amp; (newCap - 1)] = e; else if (e instanceof TreeNode) ((TreeNode&lt;K,V&gt;)e).split(this, newTab, j, oldCap); else &#123; // 带有链表时优化重hash Node&lt;K,V&gt; loHead = null, loTail = null; Node&lt;K,V&gt; hiHead = null, hiTail = null; Node&lt;K,V&gt; next; do &#123; next = e.next; // 原索引，(e.hash &amp; oldCap) == 0 说明 在put操作通过 hash &amp; newThr //计算出的索引值等于现在的索引值。 if ((e.hash &amp; oldCap) == 0) &#123; if (loTail == null) loHead = e; else loTail.next = e; loTail = e; &#125; // 原索引+oldCap，不是原索引，就移动原来的长度 else &#123; if (hiTail == null) hiHead = e; else hiTail.next = e; hiTail = e; &#125; &#125; while ((e = next) != null); // 原索引放到bucket里 if (loTail != null) &#123; loTail.next = null; newTab[j] = loHead; &#125; // 原索引+oldCap放到bucket里 if (hiTail != null) &#123; hiTail.next = null; newTab[j + oldCap] = hiHead; &#125; &#125; &#125; &#125; &#125; return newTab;&#125; 声明两对指针，维护两个链表，依次在末端添加新的元素，在多线程操作的情况下，无非是第二个线程重复第一个线程一模一样的操作。 因此不会产生jdk1.7扩容时的resize死循环问题。 jdk1.8中hashmap的确不会因为多线程put导致死循环，但是依然有其他的弊端。因此多线程情况下还是建议使用concurrenthashmap。 面试问题 如果new HashMap(19)，bucket数组多大？HashMap的bucket 数组大小一定是2的幂，如果new的时候指定了容量且不是2的幂，实际容量会是最接近(大于)指定容量的2的幂，比如 new HashMap&lt;&gt;(19)，比19大且最接近的2的幂是32，实际容量就是32。 基础知识 简便方法： 如对 a 按位取反，则得到的结果为 -(a+1) 。 此条运算方式对正数负数和零都适用。 源码 /** * Returns a power of two size for the given target capacity. */static final int tableSizeFor(int cap) &#123; int n = cap - 1; n |= n &gt;&gt;&gt; 1; n |= n &gt;&gt;&gt; 2; n |= n &gt;&gt;&gt; 4; n |= n &gt;&gt;&gt; 8; n |= n &gt;&gt;&gt; 16; return (n &lt; 0) ? 1 : (n &gt;= MAXIMUM_CAPACITY) ? MAXIMUM_CAPACITY : n + 1;&#125; 解析 先来分析有关n位操作部分：先来假设n的二进制为01xxx…xxx。接着 对n右移1位：001xx…xxx，再位或：011xx…xxx 对n右移2为：00011…xxx，再位或：01111…xxx 此时前面已经有四个1了，再右移4位且位或可得8个1 同理，有8个1，右移8位肯定会让后八位也为1。 综上可得，该算法让最高位的1后面的位全变为1。 最后再让结果n+1，即得到了2的整数次幂的值了。 现在回来看看第一条语句： int n = cap - 1; 让cap-1再赋值给n的目的是另找到的目标值大于或等于原值。例如二进制1000，十进制数值为8。如果不对它减1而直接操作，将得到答案10000，即16。显然不是结果。减1后二进制为111，再进行操作则会得到原来的数值1000，即8。 这种方法的效率非常高，可见Java8对容器优化了很多，很强哈。其他之后再进行分析吧。 HashMap什么时候开辟bucket数组占用内存？HashMap在new 后并不会立即分配bucket数组，而是第一次put时初始化，类似ArrayList在第一次add时分配空间。 HashMap何时扩容？HashMap 在 put 的元素数量大于 Capacity LoadFactor（默认16 0.75） 之后会进行扩容。 当两个对象的hashcode相同会发生什么？碰撞 如果两个键的hashcode相同，你如何获取值对象？遍历与hashCode值相等时相连的链表，直到相等或者null 你了解重新调整HashMap大小存在什么问题吗？ 参考文档 HashMap实现原理分析 由阿里巴巴Java开发规约HashMap条目引发的故事 Java8 HashMap之tableSizeFor Java 8系列之重新认识HashMap Java进阶（六）从ConcurrentHashMap的演进看Java多线程核心技术]]></content>
      <categories>
        <category>Java</category>
      </categories>
      <tags>
        <tag>HashMap</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[windows Apache服务器配置]]></title>
    <url>%2FWindows-apache-Server%2F</url>
    <content type="text"><![CDATA[Apache64位可以而32位不可以 安装Apache服务注意： 如果没有自己设置Apache服务名，后面都可不跟-n “服务名”，即采用默认的服务名称。 必须用管理员提示符打开，直接用shift+F10打开命令行是不行的。 命令： 将apache注册为服务 httpd -k install将Apache注册为windows服务，可以指定的服务名为”apache”。 httpd -k install -n “服务名” 将Apache注册为windows服务，自己指定一个服务名字。 httpd -k install -n “服务名” -f “conf\my.conf”将Apache注册为windows服务，自己指定一个服务名字，并且使用特定配置文件。 卸载Apache服务 httpd.exe -k uninstall -n “服务名” 移除Apache服务，-n 后面跟自己取得Apache服务器名字 启动Apache服务 httpd.exe -k start -n “服务名” 停止Apache服务 httpd.exe -k stop -n “服务名” httpd.exe -k shutdown -n “服务名” 重启Apache服务 httpd.exe -k restart -n “服务名” 想要正确启动Apache 服务，还需要在httpd.conf中配置Define SRVROOT &quot;E:\Apache24&quot;为本地Apache位置。 Apache反向代理配置 需要开启apache代理的拓展，将httpd.conf中下列注释取消 LoadModule access_compat_module modules/mod_access_compat.soLoadModule proxy modules/proxy.soLoadModule proxy_connect modules/proxy_connect.soLoadModule proxy_http modules/proxy_http.soLoadModule proxy_html modules/proxy_html.so 添加配置 &lt;VirtualHost *:80&gt; ServerName localhost &lt;Proxy *&gt; Order deny,allow Allow from all &lt;/Proxy&gt;ProxyPass / http://127.0.0.1:8080/ProxyPassReverse / http://127.0.0.1:8080/&lt;/VirtualHost&gt; 即可反向代理成功到localhost:8080 参考文档： windows下apache最新下载、安装配置 Apache服务器的下载与安装 Apache24（window）]]></content>
      <categories>
        <category>服务器</category>
      </categories>
      <tags>
        <tag>反向代理</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[面试中的Java链表]]></title>
    <url>%2FlinkedList%2F</url>
    <content type="text"><![CDATA[链表作为常考的面试题，并且本身比较灵活，对指针的应用较多。本文对常见的链表面试题Java实现做了整理。 链表节点定义如下:static class Node &#123; int num; Node next;&#125; 1. 求单链表中结点的个数依次遍历链表 public static int size(Node head) &#123; int size = 0; while (head != null) &#123; size++; head = head.next; &#125; return size;&#125; 2. 将单链表反转构建一个新的链表，依次将本链表的节点插入到新链表的最前端，即可完成链表的反转。 public static Node reverse(Node head) &#123; Node p1 = head, p2; head = null; while (p1 != null) &#123; p2 = p1; p1 = p1.next; //头插法 p2.next = head; head = p2; &#125; return head;&#125; 3. 查找单链表中的倒数第K个结点（k &gt; 0）第一种解法是得到顺数的第 size+k-1 个节点，即为倒数的第K歌节点第二种解法是快慢指针,主要思路就是使用两个指针，先让前面的指针走到正向第k个结点，后面的指针才走，这样前后两个指针的距离差是k-1，之后前后两个指针一起向前走，前面的指针走到最后一个结点时，后面指针所指结点就是倒数第k个结点，下面采用这种解法。 public static Node getKNode(Node head, int k) &#123; if (k &lt; 0 || head == null) &#123; return null; &#125; Node p2 = head, p1 = head; while (k-- &gt; 1 &amp;&amp; p1 != null) &#123; p1 = p1.next; &#125; // 说明k&gt;size，因此返回null if (k &gt; 1 || p1 == null) &#123; return null; &#125; while (p1.next != null) &#123; p1 = p1.next; p2 = p2.next; &#125; return p2;&#125; 4. 查找单链表的中间结点采用快慢指针，p1每次走两步，p2每次走一步，奇数返回size/2+1，偶数返回size/2,注意链表为空，链表结点个数为1和2的情况。 public static Node getMidNode(Node head) &#123; if (head == null) &#123; return null; &#125; Node p1 = head, p2 = head; while (p1.next != null) &#123; if (p1 == null) &#123; break; &#125; p1 = p1.next.next; p2 = p2.next; &#125; return p2;&#125; 5. 从尾到头打印单链表用栈 public static void reversePrint(Node node) &#123; Stack&lt;Node&gt; stack = new Stack&lt;&gt;(); while (node != null) &#123; stack.push(node); node = node.next; &#125; while (!stack.isEmpty()) &#123; System.out.print(stack.pop().num + " "); &#125;&#125; 递归 public static void reversePrint2(Node node) &#123; if (node != null) &#123; reversePrint2(node.next); System.out.print(node.num + " "); &#125;&#125; 6. 已知两个单链表pHead1和pHead2各自有序，把它们合并成一个链表依然有序类似于归并排序 public static Node merge(Node head1, Node head2) &#123; Node p1 = head1, p2 = head2, head; if (head1.num &lt; head2.num) &#123; head = head1; p1 = p1.next; &#125; else &#123; head = head2; p2 = p2.next; &#125; Node p = head; while (p1 != null &amp;&amp; p2 != null) &#123; if (p1.num &lt;= p2.num) &#123; p.next = p1; p1 = p1.next; p = p.next; &#125; else &#123; p.next = p2; p2 = p2.next; p = p.next; &#125; &#125; if (p1 != null) &#123; p.next = p1; &#125; if (p2 != null) &#123; p.next = p2; &#125; return head;&#125; 7. 判断一个单链表中是否有环这里也是用到两个指针。如果一个链表中有环，也就是说用一个指针去遍历，是永远走不到头的。因此，我们可以用两个指针去遍历，一个指针一次走两步，一个指针一次走一步，如果有环，两个指针肯定会在环中相遇。时间复杂度为O（n）。public static boolean hasRing(Node head) &#123; Node p1 = head, p2 = head; while (p1 != null &amp;&amp; p1.next != null) &#123; p1 = p1.next.next; p2 = p2.next; if (p1 == p2) &#123; return true; &#125; &#125; return false;&#125; 8. 已知一个单链表中存在环，求进入环中的第一个节点解题思路： 由上题可知，按照 p1 每次两步，p2 每次一步的方式走，发现 p2 和 p1 重合，确定了单向链表有环路了。接下来，让 p1 回到链表的头部，重新走，每次步长不是走2了，而是走1，那么当 p1 和 p2 再次相遇的时候，就是环路的入口了。 为什么？ 假定起点到环入口点的距离为 a，p1 和 p2 的相交点M与环入口点的距离为b，环路的周长为L，当 p1 和 p2 第一次相遇的时候，假定 p2 走了 n 步。那么有： p2走的路径： a+b ＝ n；p1走的路径： a+b+kL = 2n； p1 比 p2 多走了k圈环路，总路程是p2的2倍 根据上述公式可以得到 k*L=a+b=n ，显然，如果从相遇点M开始，p2 再走 n 步的话，还可以再回到相遇点，同时p2从头开始走的话，经过n步，也会达到相遇点M。 显然在这个步骤当中 p1 和 p2 只有前 a 步走的路径不同，所以当 p1 和 p2 再次重合的时候，必然是在链表的环路入口点上。 public static Node getFirstRingNode(Node head) &#123; Node p1 = head, p2 = head; while (p1 != null &amp;&amp; p1.next != null) &#123; p1 = p1.next.next; p2 = p2.next; if (p1 == p2) &#123; p1 = head; while (p1 != p2) &#123; p1 = p1.next; p2 = p2.next; &#125; break; &#125; &#125; return p1;&#125; 9. 判断两个单链表是否相交如果两个链表相交，那么相交之后的节点应该相同，那么最后那个节点应该也相同 public static boolean isIntersect(Node head1, Node head2) &#123; Node p1 = head1, p2 = head2; while (p1.next != null) &#123; p1 = p1.next; &#125; while (p2.next != null) &#123; p2 = p2.next; &#125; return p1 == p2;&#125; 10. 求两个单链表相交的第一个节点采用对齐的思想。计算两个链表的长度 L1 , L2，分别用两个指针 p1 , p2 指向两个链表的头，然后将较长链表的 p1（假设为 p1）向后移动L2 - L1个节点，然后再同时向后移动p1 , p2，直到 p1 = p2。相遇的点就是相交的第一个节点。 public static Node firstIntersectNode(Node head1, Node head2) &#123; int len1 = size(head1); int len2 = size(head2); Node p1 = head1, p2 = head2; if (len1 &gt; len2) &#123; for (int i = 1; i &lt; len1 - len2; i++) &#123; p1 = p1.next; &#125; &#125; else &#123; for (int i = 1; i &lt; len2 - len1; i++) &#123; p2 = p2.next; &#125; &#125; while (p1 != p2) &#123; p1 = p1.next; p2 = p2.next; &#125; return p1;&#125; 11. 给出一单链表头指针 head 和一节点指针 deletedNode，O(1)时间复杂度删除节点deletedNode将deletedNode下一个节点的值复制给deletedNode节点，然后删除deletedNode节点，但是对于要删除的节点是最后一个节点的时候要做处理。 public static Node firstIntersectNode(Node head1, Node head2) &#123; int len1 = size(head1); int len2 = size(head2); Node p1 = head1, p2 = head2; if (len1 &gt; len2) &#123; for (int i = 1; i &lt; len1 - len2; i++) &#123; p1 = p1.next; &#125; &#125; else &#123; for (int i = 1; i &lt; len2 - len1; i++) &#123; p2 = p2.next; &#125; &#125; while (p1 != p2) &#123; p1 = p1.next; p2 = p2.next; &#125; return p1;&#125; 12. 链表的冒泡排序对于数组的冒泡排序是上层for循环控制次数，下次for循环控制距离，对于链表的冒泡排序而言，首先让tail指针为null，一次循环比较完之后，在等于最后一个节点，倒数第二个节点。。。就是通过tail指针控制循环比较的次数和距离。 public static void bubbleSort(Node head) &#123; Node tail = null; Node p1; while (head != tail) &#123; for (p1 = head; p1.next != tail; p1 = p1.next) &#123; if (p1.num &gt; p1.next.num) &#123; int temp = p1.num; p1.num = p1.next.num; p1.next.num = temp; &#125; &#125; tail = p1; &#125; show(head);&#125; 13. 单链表的双冒泡排序public static void doubleBubblesort(Node start, Node end) &#123; if (start != end) &#123; Node p1 = start; Node p2 = p1.next; while (p2 != end) &#123; if (p2.num &lt; start.num) &#123; p1 = p1.next; int temp = p1.num; p1.num = p2.num; p2.num = temp; &#125; p2 = p2.next; &#125; int temp = p1.num; p1.num = start.num; start.num = temp; doubleBubblesort(start, p1); doubleBubblesort(p1.next, null); &#125;&#125; 全部代码放在 https://github.com/morethink/algorithm/blob/master/src/main/java/algorithm/list/LinkedList.java 参考文档 轻松搞定面试中的链表题目 面试精选：链表问题集锦 链表面试题总结（一） 合并两个有序链表递归和迭代两种写法以及扩展问题：合并k个有序链表 java实现（leetcode21和23题）]]></content>
      <categories>
        <category>算法</category>
      </categories>
      <tags>
        <tag>链表</tag>
        <tag>面试</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[递归与变态跳台阶]]></title>
    <url>%2Fabnormal-jump-step%2F</url>
    <content type="text"><![CDATA[数学是算法的皇后，不懂数学，难学算法啊。 剑指offer第11题Java实现题目：一只青蛙一次可以跳上1级台阶，也可以跳上2级……它也可以跳上n级。求该青蛙跳上一个n级的台阶总共有多少种跳法。 /** * @author 李文浩 * @version 2017/8/13. * &lt;p&gt; * &lt;p&gt; * 一只青蛙一次可以跳上1级台阶，也可以跳上2级……它也可以跳上n级。 求该青蛙跳上一个n级的台阶总共有多少种跳法。 * &lt;p&gt; * 思路: * 因为n级台阶，第一步有n种跳法：跳1级、跳2级、到跳n级 * 跳1级，剩下n-1级，则剩下跳法是f(n-1) * 跳2级，剩下n-2级，则剩下跳法是f(n-2) * 所以f(n)=f(n-1)+f(n-2)+...+f(1) * 因为f(n-1)=f(n-2)+f(n-3)+...+f(1) * 所以f(n)=f(n-1)+f(n-1)=2*f(n-1) */public class Test11 &#123; public static void main(String[] args) &#123; Test11 test11 = new Test11(); System.out.println(test11.JumpFloorII(10)); System.out.println(test11.JumpFloorII2(10)); System.out.println(test11.JumpFloorII3(10)); &#125; /** * 递归 * * @param target * @return */ public int JumpFloorII2(int target) &#123; if (target &lt;= 0) &#123; return -1; &#125; else if (target == 1) &#123; return 1; &#125; else &#123; return 2 * JumpFloorII2(target - 1); &#125; &#125; /** * 迭代 * * @param target * @return */ public int JumpFloorII3(int target) &#123; int f = 1; for (int i = 1; i &lt; target; i++) &#123; f *= 2; &#125; return f; &#125; /** * 此种思路充分说明了数学是算法的皇后 * &lt;p&gt; * 每个台阶都有跳与不跳两种情况(第n阶台阶必须跳),所以总共有 2 ^ (n - 1)种跳法， * * @param target * @return */ public int JumpFloorII(int target) &#123; //使用Math类的方法// return (int) Math.pow(2, target - 1); //2^(n-1)可以用位移操作进行，更快 return 1 &lt;&lt; --target; &#125;&#125; 移位运算符左移运算是将一个二进制位的操作数按指定移动的位数向左移位，移出位被丢弃，右边的空位一律补0。右移运算是将一个二进制位的操作数按指定移动的位数向右移动，移出位被丢弃，左边移出的空位或者一律补0，或者补符号位，这由不同的机器而定。在使用补码作为机器数的机器中，正数的符号位为0，负数的符号位为1。 &lt;&lt; 左移 需要移位的数字 &lt;&lt; 移位的次数例如：3 &lt;&lt; 2，则是将数字3左移2位 计算过程： 首先把3转换为二进制数字0000 0000 0000 0000 0000 0000 0000 0011，然后把该数字高位(左侧)的两个零移出，其他的数字都朝左平移2位，最后在低位(右侧)的两个空位补零。则得到的最终结果是0000 0000 0000 0000 0000 0000 0000 1100，则转换为十进制是12。 数学意义： 在数字没有溢出的前提下，对于正数和负数，左移一位都相当于乘以2的1次方，左移n位就相当于乘以2的n次方。 &gt;&gt; 右移运算规则：按二进制形式把所有的数字向右移动对应位移位数，低位移出(舍弃)，高位的空位补符号位，即正数补零，负数补1。语法格式：需要移位的数字 &gt;&gt; 移位的次数 例如11 &gt;&gt; 2，则是将数字11右移2位 计算过程：11的二进制形式为：0000 0000 0000 0000 0000 0000 0000 1011，然后把低位的最后两个数字移出，因为该数字是正数，所以在高位补零。则得到的最终结果是0000 0000 0000 0000 0000 0000 0000 0010。转换为十进制是2。 数学意义：右移一位相当于除2，右移n位相当于除以2的n次方。 &gt;&gt;&gt; 无符号右移 运算规则：按二进制形式把所有的数字向右移动对应位数，低位移出(舍弃)，高位的空位补零。对于正数来说和带符号右移相同，对于负数来说不同。 其他结构和&gt;&gt;相似。有的时候，你希望将一个数的二进制值向右或向左移位。执行左移时，在一个数的二进制形式中，所有位都向左移动由移位运算符右侧的操作数指定的位数。 移位后在右边留下的空位将由零来填充。右移位运算符的原理相似，只是朝相反的方向移位。然而，如果数是负数，那么在左侧填充的值就是1而不是0。两个移位 运算符是&gt;&gt;和&lt;&lt;，它们分别是右移位和左移位运算符。除此之外，还有复合移位和赋值运算符&lt;&lt;=和&gt;&gt;=。 参考文档 移位运算符-百度百科]]></content>
      <categories>
        <category>算法</category>
      </categories>
      <tags>
        <tag>面试</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Java迭代实现斐波那契数列]]></title>
    <url>%2FJava%E8%BF%AD%E4%BB%A3%E5%AE%9E%E7%8E%B0%E6%96%90%E6%B3%A2%E9%82%A3%E5%A5%91%E6%95%B0%E5%88%97%2F</url>
    <content type="text"><![CDATA[剑指offer第九题Java实现题目：大家都知道斐波那契数列，现在要求输入一个整数n，请你输出斐波那契数列的第n项。 public class Test9 &#123; public static void main(String[] args) &#123; Test9 test9 = new Test9(); System.out.println(test9.Fibonacci(390000)); System.out.println(test9.Fibonacci3(39)); &#125; /** * 为什么不采用递归？因为递归实际是大量调用自身，当数量足够大的时候，需要同时保存成千上百个调用记录，容易发生内存溢出。 * 怎么优化？ * 1. 采用尾递归，但是Java并没有基于尾递归进行优化，也就是说Java中采用递归还是无法避免很容易发生"栈溢出"错误（stack overflow）。 * 因为尾递归都是位于调用函数的最后一行，此时可以删除以前所保存的函数内变量，想当于每次只调用了一个函数。 * 2. 采用迭代 * * @param n * @return */ public int Fibonacci(int n) &#123; if (n &lt;= 0) &#123; return 0; &#125; int f1 = 0, f2 = 1; for (int i = 1; i &lt;= n; i++) &#123; f1 = f1 + f2; f2 = f1 - f2; &#125; // 下面这种写法更为巧妙// while (n-- &gt; 0) &#123;// f1 = f1 + f2;// f2 = f1 - f2;// &#125; return f1; &#125; /** * 采用递归的方式 * * @param n * @return */ public int Fibonacci3(int n) &#123; if (n &lt;= 0) &#123; return 0; &#125; if (n == 1 || n == 2) &#123; return 1; &#125; else &#123; return Fibonacci3(n - 1) + Fibonacci3(n - 2); &#125; &#125;&#125; 参考文档 尾调用优化]]></content>
      <categories>
        <category>算法</category>
      </categories>
      <tags>
        <tag>递归</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[罔]]></title>
    <url>%2F%E8%AF%BB%E4%B9%A6%2Fwang%2F</url>
    <content type="text"><![CDATA[尼采说，我爱以血书者。所思所感，皆可成诗，诗者，书真情也。 诗中有大道，我手写我心。 罔 惘时伤悲望月明，群星闪闪似笑我。姣姣月光人已变，所思所感无不同。]]></content>
      <categories>
        <category>读书</category>
      </categories>
      <tags>
        <tag>诗</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Java实现二叉树的先序、中序、后序、层序遍历（递归和非递归）]]></title>
    <url>%2Fbinary-tree%2F</url>
    <content type="text"><![CDATA[二叉树是一种非常重要的数据结构，很多其它数据结构都是基于二叉树的基础演变而来的。对于二叉树，有前序、中序以及后序三种遍历方法。因为树的定义本身就是递归定义，因此采用递归的方法去实现树的三种遍历不仅容易理解而且代码很简洁。而对于树的遍历若采用非递归的方法，就要采用栈去模拟实现。在三种遍历中，前序和中序遍历的非递归算法都很容易实现，非递归后序遍历实现起来相对来说要难一点。 节点分布如下: import java.util.LinkedList;import java.util.Queue;import java.util.Stack;/** * @author 李文浩 * @version 2017/7/30. */public class BinaryTree &#123; /** * 节点定义 */ static class TreeNode &#123; int val; TreeNode left; TreeNode right; TreeNode(int x) &#123; val = x; &#125; &#125; /** * 高度，左右子树中的较大值 * * @param node * @return */ public static int height(TreeNode node) &#123; if (node == null) &#123; return 0; &#125; int leftHeight = height(node.left); int rightHeight = height(node.right); return leftHeight &gt; rightHeight ? leftHeight + 1 : rightHeight + 1; &#125; /** * 层序遍历一颗二叉树，用广度优先搜索的思想，使用一个队列来按照层的顺序存放节点 * 先将根节点入队列，只要队列不为空，然后出队列，并访问，接着讲访问节点的左右子树依次入队列 * * @param node */ public static void levelTraversal(TreeNode node) &#123; if (node == null) return; Queue&lt;TreeNode&gt; queue = new LinkedList&lt;TreeNode&gt;(); queue.offer(node); TreeNode treeNode; while (!queue.isEmpty()) &#123; treeNode = queue.poll(); System.out.print(treeNode.val + " "); if (treeNode.left != null) &#123; queue.offer(treeNode.left); &#125; if (treeNode.right != null) &#123; queue.offer(treeNode.right); &#125; &#125; &#125; /** * 先序递归 * * @param treeNode */ public static void preOrder(TreeNode treeNode) &#123; if (treeNode != null) &#123; System.out.print(treeNode.val + " "); preOrder(treeNode.left); preOrder(treeNode.right); &#125; &#125; /** * 中序递归 * * @param treeNode */ public static void inOrder(TreeNode treeNode) &#123; if (treeNode != null) &#123; inOrder(treeNode.left); System.out.print(treeNode.val + " "); inOrder(treeNode.right); &#125; &#125; /** * 后序递归 * * @param treeNode */ public static void postOrder(TreeNode treeNode) &#123; if (treeNode != null) &#123; postOrder(treeNode.left); postOrder(treeNode.right); System.out.print(treeNode.val + " "); &#125; &#125; /** * 先序非递归： * 这种实现类似于图的深度优先遍历（DFS）。 * 维护一个栈，将根节点入栈，然后只要栈不为空，出栈并访问， * 接着依次将访问节点的右节点、左节点入栈。 * 这种方式应该是对先序遍历的一种特殊实现（看上去简单明了）， * 但是不具备很好的扩展性，在中序和后序方式中不适用 * * @param root */ public static void preOrderStack(TreeNode root) &#123; Stack&lt;TreeNode&gt; stack = new Stack&lt;&gt;(); stack.push(root); while (!stack.isEmpty()) &#123; TreeNode treeNode = stack.pop(); System.out.print(treeNode.val + " "); if (treeNode.right != null) &#123; stack.push(treeNode.right); &#125; if (treeNode.left != null) &#123; stack.push(treeNode.left); &#125; &#125; &#125; /** * 先序非递归2： * 利用栈模拟递归过程实现循环先序遍历二叉树。 * 这种方式具备扩展性，它模拟递归的过程，将左子树点不断的压入栈，直到null， * 然后处理栈顶节点的右子树。 * * @param root */ public static void preOrderStack2(TreeNode root) &#123; Stack&lt;TreeNode&gt; stack = new Stack&lt;&gt;(); TreeNode treeNode = root; while (treeNode != null || !stack.isEmpty()) &#123; //将左子树点不断的压入栈 while (treeNode != null) &#123; //先访问再入栈 System.out.print(treeNode.val + " "); stack.push(treeNode); treeNode = treeNode.left; &#125; //出栈并处理右子树 if (!stack.isEmpty()) &#123; treeNode = stack.pop(); treeNode = treeNode.right; &#125; &#125; &#125; /** * 中序非递归： * 利用栈模拟递归过程实现循环中序遍历二叉树。 * 思想和上面的先序非递归2相同， * 只是访问的时间是在左子树都处理完直到null的时候出栈并访问。 * * @param treeNode */ public static void inOrderStack(TreeNode treeNode) &#123; Stack&lt;TreeNode&gt; stack = new Stack&lt;&gt;(); while (treeNode != null || !stack.isEmpty()) &#123; while (treeNode != null) &#123; stack.push(treeNode); treeNode = treeNode.left; &#125; //左子树进栈完毕 if (!stack.isEmpty()) &#123; treeNode = stack.pop(); System.out.print(treeNode.val + " "); treeNode = treeNode.right; &#125; &#125; &#125; public static class TagNode &#123; TreeNode treeNode; boolean isFirst; &#125; /** * 后序非递归： * 后序遍历不同于先序和中序，它是要先处理完左右子树， * 然后再处理根(回溯)。 * &lt;p&gt; * &lt;p&gt; * 对于任一结点P，将其入栈，然后沿其左子树一直往下搜索，直到搜索到没有左孩子的结点， * 此时该结点出现在栈顶，但是此时不能将其出栈并访问，因此其右孩子还为被访问。 * 所以接下来按照相同的规则对其右子树进行相同的处理，当访问完其右孩子时，该结点又出现在栈顶， * 此时可以将其出栈并访问。这样就保证了正确的访问顺序。 * 可以看出，在这个过程中，每个结点都两次出现在栈顶，只有在第二次出现在栈顶时，才能访问它。 * 因此需要多设置一个变量标识该结点是否是第一次出现在栈顶，这里是在树结构里面加一个标记，然后合成一个新的TagNode。 * * @param treeNode */ public static void postOrderStack(TreeNode treeNode) &#123; Stack&lt;TagNode&gt; stack = new Stack&lt;&gt;(); TagNode tagNode; while (treeNode != null || !stack.isEmpty()) &#123; //沿左子树一直往下搜索，直至出现没有左子树的结点 while (treeNode != null) &#123; tagNode = new TagNode(); tagNode.treeNode = treeNode; tagNode.isFirst = true; stack.push(tagNode); treeNode = treeNode.left; &#125; if (!stack.isEmpty()) &#123; tagNode = stack.pop(); //表示是第一次出现在栈顶 if (tagNode.isFirst == true) &#123; tagNode.isFirst = false; stack.push(tagNode); treeNode = tagNode.treeNode.right; &#125; else &#123; //第二次出现在栈顶 System.out.print(tagNode.treeNode.val + " "); treeNode = null; &#125; &#125; &#125; &#125; /** * 后序非递归2： * 要保证根结点在左孩子和右孩子访问之后才能访问，因此对于任一结点P，先将其入栈。如果P不存在左孩子和右孩子，则可以直接访问它； * 或者P存在左孩子或者右孩子，但是其左孩子和右孩子都已被访问过了，则同样可以直接访问该结点。 * 若非上述两种情况，则将P的右孩子和左孩子依次入栈，这样就保证了每次取栈顶元素的时候，左孩子在右孩子前面被访问， * 左孩子和右孩子都在根结点前面被访问。 * * @param treeNode */ public static void postOrderStack2(TreeNode treeNode) &#123; Stack&lt;TreeNode&gt; stack = new Stack&lt;&gt;(); TreeNode currentTreeNode; TreeNode preTreeNode = null; stack.push(treeNode); while (!stack.isEmpty()) &#123; currentTreeNode = stack.peek(); //如果当前结点没有孩子结点或者孩子节点都已被访问过 if ((currentTreeNode.left == null &amp;&amp; currentTreeNode.right == null) || (preTreeNode != null &amp;&amp; (preTreeNode == currentTreeNode.left || preTreeNode == currentTreeNode.right))) &#123; System.out.print(currentTreeNode.val + " "); stack.pop(); preTreeNode = currentTreeNode; &#125; else &#123; if (currentTreeNode.right != null) &#123; stack.push(currentTreeNode.right); &#125; if (currentTreeNode.left != null) &#123; stack.push(currentTreeNode.left); &#125; &#125; &#125; &#125;&#125; 参考文档 二叉树的非递归遍历 JAVA下实现二叉树的先序、中序、后序、层序遍历（递归和循环）]]></content>
      <categories>
        <category>算法</category>
      </categories>
      <tags>
        <tag>树</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Linux常用命令]]></title>
    <url>%2FLinux%E5%B8%B8%E7%94%A8%E5%91%BD%E4%BB%A4%2F</url>
    <content type="text"><![CDATA[本文记录一些在Linux系统下比较常用的命令。 首先介绍下Linux下命令生效的顺序： 第一顺位：执行绝对路径或者相对路径的命令 第二顺位：执行别名 第三顺位：执行Bash的内部命令 第四顺位：执行按照$PATH环境变量设置定义的目录顺序的第一个命令 cp该命令用于复制文件，copy之意，它还可以把多个文件一次性地复制到一个目录下，它的常用参数如下： -a：将文件的特性一起复制 -p：连同文件的属性一起复制，而非使用默认方式，与-a相似，常用于备份 -i：若目标文件已经存在时，在覆盖时会先询问操作的进行 -r：递归持续复制，用于目录的复制行为 -u：目标文件与源文件有差异时才会复制 例： cp source dest 复制文件 cp -r sourceFolder targetFolder 递归复制整个文件夹 mv该命令用于移动文件、目录或更名，move之意，它的常用参数如下： -f：force强制的意思，如果目标文件已经存在，不会询问而直接覆盖 -i：若目标文件已经存在，就会询问是否覆盖 -u：若目标文件已经存在，且比目标文件新，才会更新 注：该命令可以把一个文件或多个文件一次移动一个文件夹中，但是最后一个目标文件一定要是“目录”。 例： mv file1 file2 file3 dir 把文件file1、file2、file3移动到目录dir中 mv file1 file2 把文件file1重命名为file2 rm该命令用于删除文件或目录，remove之间，它的常用参数如下： -f ：就是force的意思，忽略不存在的文件，不会出现警告消息 -i ：互动模式，在删除前会询问用户是否操作 -r ：递归删除，最常用于目录删除，它是一个非常危险的参数 例如： rm -i file 删除文件file，在删除之前会询问是否进行该操作 rm -fr dir 强制删除目录dir中的所有文件 tartar是linux上常用的打包、压缩、解压缩工具，它的参数很多，这里列举常用的压缩与解压缩参数。 参数： -c： 建立压缩档案的参数 -x： 解压缩压缩档案的参数 -z： 用gzip压缩 -j： 用bzip2解压文件 -Z： 用compress解压文件 -v： 压缩的过程中显示档案 -f： 置顶文档名，在-f后面立即接文件名，不能再加参数 打包与解包 将整个/home/www/images 目录下的文件全部打包 tar -cvf images.tar images ← 仅打包，不压缩 解包到指定的目录 tar -xvf filename.tar -C 指定目录 压缩与解压缩gzip压缩 打包后，以gzip压缩，在参数f后面的压缩文件名是自己取的，习惯上用tar来做，如果加z参数，则以tar.gz 或tgz来代表gzip压缩过的tar文件 tar -zcvf images.tar.gz images 将 images.tar.gz 解压 tar -zxvf images.tar.gz 解压缩到指定的目录 tar -zxvf images.tar.gz -C 指定的目录 bzip2压缩 打包后，以bzip2压缩 tar -jcvf images.tar.bz2 images 将 images.tar.bz2 解压 tar -jxvf images.tar.bz2 解压缩到指定的目录 tar -jxvf images.tar.bz2 -C 指定的目录 不同压缩方式的压缩率比较 tar -zcvf test.tar.gz test tar -jcvf test.tar.bz2 test zip -ry test.zip test zip命令要加上两个选项 -r：表示递归目录，不然只压出来一个空目录。 -y：表示保持符号链接，而不用把符号链接指向的文件也压进来。 结果比较：ll -h test.*-rw-r--r-- 1 liwenhao staff 45M 8 6 20:12 test.tar.bz2-rw-r--r-- 1 liwenhao staff 47M 8 6 20:12 test.tar.gz-rw-r--r-- 1 liwenhao staff 50M 8 6 20:13 test.zip 可以看到：压缩率：bzip2 &gt; gzip &gt; zip tar打包绝对路径备份mysql时，想把之前的文件压缩，但是用tar czvf /data/backup/test.tar.gz /data/a/b/directory 打开一看，里面的内容是把整个绝对路径都放进去了。 如果想实现相对路径，这样写就可以解决了 tar czvf /data/backup/test.tar.gz /data/a/b/directory改成tar czvf /data/backup/test.tar.gz -C /data/a/b directory C是临时切换工作目录，-P是绝对路径，在这里只用到-C参数就行了。 ps该命令用于将某个时间点的进程运行情况选取下来并输出，process之意，它的常用参数如下： -A ：所有的进程均显示出来 -a ：显示 同一终端下 的所有程序 -u ：有效用户的相关进程 -x ：一般与a参数一起使用，可列出较完整的信息 -l ：较长，较详细地将PID的信息列出 -e ：等于&quot;-A&quot; -f ：显示程序间的关系 其实我们只要记住ps一般使用的命令参数搭配即可，它们并不多，如下： ps aux 查看系统所有的进程数据 ps ax 查看不与terminal有关的所有进程 ps -lA 查看系统所有的进程数据 ps axjf 查看连同一部分进程树状态 查看某个进程： ps –ef|grep tomcat 查看所有有关tomcat的进程 ps -ef|grep --color java 高亮要查询的关键字 kill该命令用于向某个工作（jobnumber）或者是某个PID（数字）传送一个信号，它通常与ps和jobs命令一起使用，它的基本语法如下：kill -signal PIDsignal的常用参数(最前面的数字为信号的代号，使用时可以用代号代替相应的信号)如下： 1：SIGHUP，启动被终止的进程 2：SIGINT，相当于输入ctrl+c，中断一个程序的进行 9：SIGKILL，强制中断一个进程的进行 15：SIGTERM，以正常的结束进程方式来终止进程 17：SIGSTOP，相当于输入ctrl+z，暂停一个进程的进行 例如： 以正常的结束进程方式来终于第一个后台工作，可用jobs命令查看后台中的第一个工作进程kill -SIGTERM %1 或者 kill -15 %1 重新改动进程ID为PID的进程，PID可用ps命令通过管道命令加上grep命令进行筛选获得kill -SIGHUP PID 或者 kill -1 PID 终止线程号位19979的进程kill -9 19979 killall该命令用于向一个命令启动的进程发送一个信号，它的一般语法如下： killall [-iIe] [command name]它的参数如下： -i ：交互式的意思，若需要删除时，会询问用户 -e ：表示后面接的command name要一致，但command name不能超过15个字符 -I ：命令名称忽略大小写 例如：killall -SIGHUP syslogd 重新启动syslogd file该命令用于判断接在file命令后的文件的基本数据，因为在Linux下文件的类型并不是以后缀为分的，所以这个命令对我们来说就很有用了，它的用法非常简单，基本语法如下：file filename例如：file ./test lnln是linux中又一个非常重要命令，它的功能是为某一个文件在另外一个位置建立一个同步的链接.当我们需要在不同的目录，用到相同的文件时，我们不需要在每一个需要的目录下都放一个必须相同的文件，我们只要在某个固定的目录，放上该文件，然后在 其它的目录下用ln命令链接（link）它就可以，不必重复的占用磁盘空间。 命令格式：ln [参数][源文件或目录][目标文件或目录] 命令功能：Linux文件系统中，有所谓的链接(link)，我们可以将其视为档案的别名，而链接又可分为两种 : 硬链接(hard link)与软链接(symbolic link)，硬链接的意思是一个档案可以有多个名称，而软链接的方式则是产生一个特殊的档案，该档案的内容是指向另一个档案的位置。硬链接是存在同一个文件系统中，而软链接却可以跨越不同的文件系统。 软链接： 软链接，以路径的形式存在。类似于Windows操作系统中的快捷方式 软链接可以 跨文件系统 ，硬链接不可以 软链接可以对一个不存在的文件名进行链接 软链接可以对目录进行链接 硬链接: 硬链接，以文件副本的形式存在。但不占用实际空间。 不允许给目录创建硬链接 硬链接只有在同一个文件系统中才能创建 这里有两点要注意： ln命令会保持每一处链接文件的同步性，也就是说，不论你改动了哪一处，其它的文件都会发生相同的变化； ln的链接又分软链接和硬链接两种，软链接就是ln –s 源文件 目标文件，它只会在你选定的位置上生成一个文件的镜像，不会占用磁盘空间，硬链接 ln 源文件 目标文件，没有参数-s， 它会在你选定的位置上生成一个和源文件大小相同的文件，无论是软链接还是硬链接，文件都保持同步变化。ln指令用在链接文件或目录，如同时指定两个以上的文件或目录，且最后的目的地是一个已经存在的目录，则会把前面指定的所有文件或目录复制到该目录中。若同时指定多个文件或目录，且最后的目的地并非是一个已存在的目录，则会出现错误信息。 命令参数： 必要参数:-b 删除，覆盖以前建立的链接-d 允许超级用户制作目录的硬链接-f 强制执行-i 交互模式，文件存在则提示用户是否覆盖-n 把符号链接视为一般目录-s 软链接(符号链接)-v 显示详细的处理过程选择参数:-S &quot;-S&lt;字尾备份字符串&gt; &quot;或 &quot;--suffix=&lt;字尾备份字符串&gt;”-V &quot;-V&lt;备份方式&gt;&quot;或&quot;--version-control=&lt;备份方式&gt;&quot;--help 显示帮助信息--version 显示版本信息 使用实例实例1：给文件创建软链接 命令： ln -s log2013.log link2013 输出： [root@localhost test]# ll-rw-r--r-- 1 root bin 61 11-13 06:03 log2013.log[root@localhost test]# ln -s log2013.log link2013[root@localhost test]# lllrwxrwxrwx 1 root root 11 12-07 16:01 link2013 -&gt; log2013.log-rw-r--r-- 1 root bin 61 11-13 06:03 log2013.log 说明： 为log2013.log文件创建软链接link2013，如果log2013.log丢失，link2013将失效 实例2：给文件创建硬链接 命令：ln log2013.log ln2013输出： [root@localhost test]# lllrwxrwxrwx 1 root root 11 12-07 16:01 link2013 -&gt; log2013.log-rw-r--r-- 1 root bin 61 11-13 06:03 log2013.log[root@localhost test]# ln log2013.log ln2013[root@localhost test]# lllrwxrwxrwx 1 root root 11 12-07 16:01 link2013 -&gt; log2013.log-rw-r--r-- 2 root bin 61 11-13 06:03 ln2013-rw-r--r-- 2 root bin 61 11-13 06:03 log2013.log 说明： 为log2013.log创建硬链接ln2013，log2013.log与ln2013的各项属性相同 删除软连接 目录的软连接要谨慎删除，有可能把原文件目录下的文件给删除了。 先建立一个软连接 [root@rekfan.com test]# ls -il总计 01491138 -rw-r–r– 1 root root 48 07-14 14:17 file11491139 -rw-r–r– 2 root root 0 07-14 14:17 file21491139 -rw-r–r– 2 root root 0 07-14 14:17 file2hand#建立file1和file1soft软连接[root@rekfan.com test]# ln -s file1 file1soft[root@rekfan.com test]# ls -il总计 01491138 -rw-r–r– 1 root root 48 07-14 14:17 file11491140 lrwxrwxrwx 1 root root 5 07-14 14:24 file1soft -&gt; file11491139 -rw-r–r– 2 root root 0 07-14 14:17 file21491139 -rw-r–r– 2 root root 0 07-14 14:17 file2hand 删除上面建立的软连接 [root@rekfan.com test]# ls -il总计 01491138 -rw-r–r– 1 root root 0 07-14 14:17 file11491140 lrwxrwxrwx 1 root root 5 07-14 14:24 file1soft -&gt; file11491139 -rw-r–r– 2 root root 0 07-14 14:17 file21491139 -rw-r–r– 2 root root 0 07-14 14:17 file2hand#删除软连接[root@rekfan.com test]# rm -rf file1soft[root@rekfan.com test]# ls -il总计 01491138 -rw-r–r– 1 root root 0 07-14 14:17 file11491139 -rw-r–r– 2 root root 0 07-14 14:17 file21491139 -rw-r–r– 2 root root 0 07-14 14:17 file2hand 删除软链接 确实是用rm 但是!!!rm -fr xxxx/ 加了个/ 这个是删除文件夹rm -fr xxxx 没有/ 这个是删除软链接 cat命令该命令用于查看文本文件的内容，后接要查看的文件名，通常可用管道与more和less一起使用，从而可以一页页地查看数据。例如： cat text | less 查看text文件中的内容注：这条命令也可以使用less text来代替 chgrp命令该命令用于改变文件所属用户组，它的使用非常简单，它的基本用法如下： chgrp [选项] [组] [文件] -R：处理指定目录以及其子目录下的所有文件 -v：运行时显示详细的处理信息 --dereference：作用于符号链接的指向，而不是符号链接本身 --no-dereference：作用于符号链接本身 例如：chgrp -vR users ./dir 递归地把dir目录下中的所有文件和子目录下所有文件的用户组修改为users chown命令该命令用于改变文件的所有者，与chgrp命令的使用方法相同，只是修改的文件属性不同。 chmod命令该命令用于改变文件的权限，一般的用法如下： chmod [-R] xyz 文件或目录-R：进行递归的持续更改，即连同子目录下的所有文件都会更改同时，chmod还可以使用u（user）、g（group）、o（other）、a（all）和+（加入）、-（删除）、=（设置）跟rwx搭配来对文件的权限进行更改。 例如：chmod 755 file 把file的文件权限改变为-rxwr-xr-x，r表示读、w表示写、x表示可执行。chmod g+w file 向file的文件权限中加入用户组可写权限。chmod +x file 让文件可执行。 权限代号： r：读权限，用数字4表示 w：写权限，用数字2表示 x：执行权限，用数字1表示 关于搜索的命令1 find find是最常见和最强大的查找命令，你可以用它找到任何你想找的文件。（避免大范围的搜索，会非常浪费系统资源，建议不在直接在”/“目录下搜索） 格式：find [搜索范围] [搜索条件] 例： find /home -name 文件名 find /home -iname 文件名 (不区分大小写) find /home -user 文件名 (所有者文件) find /home -nouser 文件名(没有所属者的文件，liunx中，每个文件都有所属者，如果没有，那么一般都是垃圾文件，但还是有特例的，比如内核产生的文件，就没有所属者，一般在proc和sys目录下；还有外来文件，也就是U盘拷入的文件也会忽略所有者。) find . -type f -mmin -10 搜索当前目录中，所有过去10分钟中更新过的普通文件。如果不加-type f参数，则搜索普通文件+特殊文件+目录。 find 目录 -size 文件大小 (注意：文件大小用小写k和大写M。) 如果什么参数也不加，find默认搜索当前目录及其子目录，并且不过滤任何结果（也就是返回所有文件），将它们全都显示在屏幕上。 2 locatelocate命令其实是”find -name”的另一种写法，但是要比后者快得多，原因在于它不搜索具体目录，而是搜索一个数据库（/var/lib/locatedb），这个数据库中含有本地所有文件信息。Linux系统自动创建这个数据库，并且每天自动更新一次，所以使用locate命令查不到最新变动过的文件。为了避免这种情况，可以在使用locate之前，先使用updatedb命令，手动更新数据库。 locate命令的使用实例： locate /etc/sh,搜索etc目录下所有以sh开头的文件。 locate ~/m,搜索用户主目录下，所有以m开头的文件。 locate -i ~/m,搜索用户主目录下，所有以m开头的文件，并且忽略大小写。 3 whereiswhereis命令只能用于程序名的搜索，而且只搜索二进制文件（参数-b）、man说明文件（参数-m）和源代码文件（参数-s）。如果省略参数，则返回所有信息。 whereis命令的使用实例： whereis grep 4 whichwhich命令的作用是，在PATH变量指定的路径中，搜索某个系统命令的位置，并且返回第一个搜索结果。也就是说，使用which命令，就可以看到某个系统命令是否存在，以及执行的到底是哪一个位置的命令。 which命令的使用实例： which grep grep搜索文件中的信息，一般为字符串，与正则表达式结合使用 grep [选项] 字符串 文件名 （字符串使用 “” 包围，结果为行记录） -i 忽略大小写 -v 排除指定字符串 find与grep的区别： find：在 系统 中搜索符合条件的 文件名，使用 通配符（完全）匹配 grep：在 文件 当中搜索符合条件的 字符串，使用 正则表达式 （包含）匹配 timetime 可以统计命令执行的时间，包括程序的实际运行时间(real time)，以及程序运行在用户态的时间(user time)和内核态的时间(sys time)。例如：time git pull 则是统计更新代码花费多长时间 aliasalias命令用来设置指令的别名。我们可以使用该命令可以将一些较长的命令进行简化。使用alias时，用户必须使用单引号’’将原来的命令引起来，防止特殊字符导致错误。 alias命令的作用只局限于该次登入的操作。若要每次登入都能够使用这些命令别名，则可将相应的alias命令存放到bash的初始化文件/etc/bashrc中。 alias 的基本使用方法为：alias 新的命令=&#39;原命令 -选项/参数&#39;例如：alias l=‘ls -lsh&#39;将重新定义ls命令，现在只需输入l就可以列目录了。直接输入 alias 命令会列出当前系统中所有已经定义的命令别名。 要删除一个别名，可以使用 unalias 命令，如 unalias l。 查看系统已经设置的别名： alias egrep='egrep --color'alias fgrep='fgrep --color'alias grep='grep --color'alias hpush='bash /Users/liwenhao/Desktop/github/hpush.sh'alias ll='ls -l'alias ls='ls -F --show-control-chars --color=auto'alias vi='vim' tailtail 命令从指定点开始将文件写到标准输出。使用tail命令的-f选项可以方便的查阅正在改变的日志文件，tail -f filename会把filename里最尾部的内容显示在屏幕上，并且不但刷新，使你看到最新的文件内容. 命令格式; tail[必要参数][选择参数][文件] 命令功能：用于显示指定文件末尾内容，不指定文件时，作为输入信息进行处理。常用查看日志文件。 命令参数： -f 循环读取-q 不显示处理信息-v 显示详细的处理信息-c &lt;数目&gt; 显示的字节数-n &lt;行数&gt; 显示行数--pid=PID 与-f合用,表示在进程ID,PID死掉之后结束.-q, --quiet, --silent 从不输出给出文件名的首部-s, --sleep-interval=S 与-f合用,表示在每次反复的间隔休眠S秒 举例 # 写到1.log，且后台运行ping www.baidu.com &gt; 1.log 2&gt;&amp;1 &amp;# 循环查看尾部日志tail -f 1.log# 杀死 ping进程killall ping 执行多个命令 后一个命令依赖于前一个命令的输出，可以是用管道(|)ls | wc -l :当前目录文件个数 后一个命令必须等前一个命令运行成功后在运行，可以使用双与号(&amp;&amp;)aa &amp;&amp; ls ：只运行aa，ls不运行 后一个命令必须等前一个命令运行完，不关心是否成功，使用单与号(&amp;)aa &amp; ls ：aa和ls都运行，但是ls必须等aa运行完。 并行执行多个命令，使用两个竖号(||)aa || ls：aa和ls并行执行，互不影响。 执行shell脚本当我们写完一个脚本的时候，它是不可以被直接运行的。我们可以通过： 通过Bash调用执行脚本 ：bash hello.sh 首先赋予执行权限：chmod 755 hello.sh，然后就可以通过相对路径 ./hello.sh 或者通过绝对路径 /root/hello.sh来执行。 Shell特殊变量：$0, $#, $*, $@, $?, $$和命令行参数变量名只能包含数字、字母和下划线，因为某些包含其他字符的变量有特殊含义，这样的变量被称为特殊变量。 例如，$$ 表示当前Shell进程的ID，即pid，看下面的代码：$echo $$ 运行结果29949 特殊变量列表： 变量 含义 $0 当前脚本的文件名 $n 传递给脚本或函数的参数。n 是一个数字，表示第几个参数。 $# 传递给脚本或函数的参数个数。 $* 传递给脚本或函数的所有参数。 $@ 传递给脚本或函数的所有参数。被双引号(“ “)包含时，与 $* 稍有不同，下面将会讲到。 $? 上个命令的退出状态，或函数的返回值。 $$ 当前Shell进程ID。对于 Shell 脚本，就是这些脚本所在的进程ID。 命令行参数运行脚本时传递给脚本的参数称为命令行参数。命令行参数用 $n 表示，例如，$1 表示第一个参数，$2 表示第二个参数，依次类推。 请看下面的脚本：#!/bin/bashecho "File Name: $0"echo "First Parameter : $1"echo "First Parameter : $2"echo "Quoted Values: $@"echo "Quoted Values: $*"echo "Total Number of Parameters : $#" 运行结果： $./test.sh Zara Ali File Name : ./test.shFirst Parameter : ZaraSecond Parameter : AliQuoted Values: Zara AliQuoted Values: Zara AliTotal Number of Parameters : 2 $* 和 $@ 的区别$*和 $@ 都表示传递给函数或脚本的所有参数，不被双引号(“ “)包含时，都以&quot;$1&quot; &quot;$2&quot; … &quot;$n&quot; 的形式输出所有参数。 但是当它们被双引号(“ “)包含时，&quot;$*&quot; 会将所有的参数作为一个整体，以”$1 $2 … $n“的形式输出所有参数；&quot;$@&quot; 会将各个参数分开，以&quot;$1&quot; &quot;$2&quot; … &quot;$n&quot; 的形式输出所有参数。 下面的例子可以清楚的看到 $* 和 $@的区别：#!/bin/bashecho "\$*=" $*echo "\"\$*\"=" "$*"echo "\$@=" $@echo "\"\$@\"=" "$@"echo "print each param from \$*"for var in $*do echo "$var"doneecho "print each param from \$@"for var in $@do echo "$var"doneecho "print each param from \"\$*\""for var in "$*"do echo "$var"doneecho "print each param from \"\$@\""for var in "$@"do echo "$var"done 执行 ./test.sh “a” “b” “c” “d”，看到下面的结果：$*= a b c d&quot;$*&quot;= a b c d$@= a b c d&quot;$@&quot;= a b c dprint each param from $*abcdprint each param from $@abcdprint each param from &quot;$*&quot;a b c dprint each param from &quot;$@&quot;abcd 退出状态$? 可以获取上一个命令的退出状态。所谓退出状态，就是上一个命令执行后的返回结果。 退出状态是一个数字，一般情况下，大部分命令执行成功会返回 0，失败返回 1。 不过，也有一些命令返回其他值，表示不同类型的错误。 下面例子中，命令$./test.sh Zara Ali成功执行输出：File Name : ./test.shFirst Parameter : ZaraSecond Parameter : AliQuoted Values: Zara AliQuoted Values: Zara AliTotal Number of Parameters : 2$echo $?0$ $? 也可以表示函数的返回值。 PATH环境变量下面的第一种方式关机后会失效，相当于临时环境变量，后面两种都是写进了系统配置，因此会永久生效。 直接用export命令 export PATH=$PATH:/usr/local/src/node-v0.10.24/node_modules/node-sass/bin 修改profile文件： vi /etc/profile 然后在文件里面加入export PATH=$PATH:/usr/local/src/node-v0.10.24/node_modules/node-sass/bin 执行命令 source /etc/profile 生效 修改.bashrc文件 vi /root/.bashrc 在里面加入 export PATH=$PATH:/usr/local/src/node-v0.10.24/node_modules/node-sass/bin 执行命令source /root/.bashrc 生效 如果修改了/etc/profile，那么编辑结束后执行source profile(source /etc/profile) 或 执行点命令 . ./profile，PATH的值才会立即生效，不然就只有等下一次开机。 这个方法的原理就是再执行一次/etc/profile shell脚本，注意如果用sh /etc/profile是不行的，因为sh是在子shell进程中执行的，即使PATH改变了也不会反应到当前环境中，但是source是在当前 shell进程中执行的，所以我们能看到PATH的改变。 有时我们可能不小心将PATH环境变量设置错误导致命令失效，可以参考下面两种方式解决。 如果是通过命令行设置的，可以通过重启或者离开本次登录(退出本次shell) 如果是因为修改配置文件导致无法使用，可以执行export PATH=/usr/bin:/usr/sbin:/bin:/sbin:/usr/X11R6/bin命令暂时使用这些目录下的命令，如 vi,ls,cd，从新修改配置文件。 命令行快捷键 光标移动 Ctrl + f – 向右移动一个字符，当然多数人用→ Ctrl + b – 向左移动一个字符， 多数人用← ESC + f – 向右移动一个单词，MAC下建议用ALT + → ESC + b – 向左移动一个单词，MAC下建议用ALT + ← Ctrl + a – 跳到行首 Ctrl + e – 跳到行尾 删除 Ctrl + d – 向右删除一个字符 Ctrl + h – 向左删除一个字符 Ctrl + u – 删除当前位置字符至行首（输入密码错误的时候多用下这个） Ctrl + k – 删除当前位置字符至行尾 Ctrl + w – 删除从光标到当前单词开头 命令切换 Ctrl + p – 上一个命令，也可以用↑ Ctrl + n – 下一个命令，也可以用↓ 其他快捷键 Ctrl + y – 插入最近删除的单词 Ctrl + c – 终止操作 Ctrl + d – 当前操作转到后台 Ctrl + l – 清屏 （有时候为了好看） ctrl + z - 把命令放入后台，这个命令谨慎使用 ctrl + r - 历史命令搜索 Alt + f - 光标向前（Forward）移动到下一个单词 Alt + b - 光标往回（Backward）移动到前一个单词 Ctrl + w - 删除从光标位置前到当前所处单词（Word）的开头 Alt + d - 删除从光标位置到当前所处单词的末尾 Ctrl + y - 粘贴最后一次被删除的单词 参考文档： Linux的五个查找命令 初窥Linux 之 我最常用的20条命令]]></content>
      <categories>
        <category>Linux</category>
      </categories>
  </entry>
  <entry>
    <title><![CDATA[Windows、Linux及Mac查看端口和杀死进程]]></title>
    <url>%2FWindows-and-Linux-view-the-port-and-kill-the-process%2F</url>
    <content type="text"><![CDATA[本文介绍如何在Windows、Linux及Mac下查看端口和杀死进程。 Windows下查看端口和杀死进程 查看占用端口号的进程号：netstat –ano | findstr &quot;指定端口号&quot; 通过进程号杀死进程：taskkill /pid 进程号 通过进程号强制杀死进程：taskkill /f /pid 进程号 通过进程号查看进程 tasklist | findstr &quot;进程号&quot; Linux下查看端口和杀死进程 Linux下查看端口号所使用的进程号：netstat -anp|grep port 杀死进程 kill -9 PID Linux下还提供了一个killall命令，可以直接使用进程的名字而不是进程标识号，例如： killall -9 name 通过端口号查看进程号 查看程序对应进程号：ps -ef | grep 进程名 查看进程号所占用的端口号 REDHAT: netstat -nltp|grep pid ubuntu: netstat -anp|grep pid Mac下查看端口和杀死进程Mac下使用lsof（list open files）来查看端口占用情况，lsof 是一个列出当前系统打开文件的工具。 使用 -i 查看某个端口是否被占用，如：lsof -i:3000 如果端口被占用，则会返回相关信息，如果没被占用，则不返回任何信息。]]></content>
      <categories>
        <category>Linux</category>
      </categories>
  </entry>
  <entry>
    <title><![CDATA[我读东坡]]></title>
    <url>%2F%E8%AF%BB%E4%B9%A6%2F%E8%8B%8F%E4%B8%9C%E5%9D%A1%E4%BC%A0%2F</url>
    <content type="text"><![CDATA[每当我一提起苏东坡来，大家总会不自觉的微笑，这句话最能概括苏东坡的一切。有诗云：人生所处应何似，应是飞鸿踏雪泥。泥上偶然留指爪，鸿飞哪复计东西。 苏东坡的一生是一个乐观从政到乐观生活的过程，苏东坡以其拒不妥协的气质而为官场所不容，本来有治世之才的苏东坡先因王安石变法而惨遭排挤，后得到太皇太后的赏识而开始肃清变法流毒，不料随着太皇太后的去世，自己也被流放，究其原因，乃是因为皇位的更迭，一位不贤明的皇帝易受小人蒙蔽而不行益民之策。我们不能保证天才的后代仍然是天才，而把王朝的命运寄托于基因实非好事。 想起《明朝那些事儿》的徐阶，明里退让，暗里积蓄力量，秉持忠义之心，严嵩终倒。正所谓，贪官奸，你要比贪官更奸。]]></content>
      <categories>
        <category>读书</category>
      </categories>
  </entry>
  <entry>
    <title><![CDATA[给大一的学弟]]></title>
    <url>%2Fto-16-freshman%2F</url>
    <content type="text"><![CDATA[我应该编程吗？ 如何确定自己是否适合做程序员？ 如何学习编程？ 为什么不尝试一下呢？ 学会使用工具其实整个互联网就是一个工具，是我们获取信息、交流的工具，而作为”Write the code, Change the world”的程序员，更应该通过工具来提高效率。 Windows 没有鼠标的日子多亏了我精湛的手操，各种快捷键分享 Chrome Vimium~让您的Chrome起飞 IDEA IDEA 是一种编程工具，而编程工具是用来提高编程效率的，因此，怎么有效提高编程效率，才是我们使用工具所要考虑的。 IDEA 中文教程 IntelliJ下使用Code/Live Template加快编码速度：程序员的工作不是写程序，而是写程序解决问题 程序员素养说到底，程序员不过是一种职业，做一个优秀的程序员不仅需要素养，还需要良好的习惯，当然，一切都可以培养。 怎么解决问题 再谈“我是怎么招聘程序员的”（上） 什么是真正的程序员 Java源码阅读的真实体会 为什么中国的程序员总被称为码农？ JavaJava 后端技能树Java 学习过程 学习 JAVA，有什么书籍推荐？学习的方法和过程是怎样的？ Java工程师成神之路~ 我们如何学好Java？ Java工程师 学习平台 慕课网是一个很好学习平台，里面有很多不错的学习资料，比较系统和全面。 极客学院 实验楼 Java 博客 skywang的博客目录(持续更新中…) 【Java并发编程】并发编程大合集 《成神之路-基础篇》JVM——Java内存相关(已完结) http://cmsblogs.com/ 一个Java牛人的博客 暑假入门书籍《Java 核心技术：卷1 基础知识》 价值博客们 刘未鹏 | Mind Hacks - 里面讲了一些方法论，还可以买他的暗时间来看看 阮一峰的个人网站 - 科技与人文并重，他翻译的黑客与画家可以看看 High一下! 酷 壳 – COOLSHELL - 一个优秀的程序员，享受编程和技术所带来的快乐 技术 infoq APP：开发者头条，掘金，segmentfault，CSDN 最后，任何事情最拍专注且坚持，慢慢来。]]></content>
      <categories>
        <category>编程思考</category>
      </categories>
  </entry>
  <entry>
    <title><![CDATA[Spring事务不回滚原因分析]]></title>
    <url>%2FSpring-transaction-no-rollback%2F</url>
    <content type="text"><![CDATA[Synchronized用于线程间的数据共享，而ThreadLocal则用于线程间的数据隔离。 在我完成一个项目的时候，遇到了一个Spring事务不回滚的问题，通过aspectJ和@Transactional注解都无法完成对于事务的回滚，经过查看博客和文档 默认回滚RuntimeException Service内部方法调用 Spring父子容器覆盖 代码已经上传到 https://github.com/morethink/transactional 异常下面是@Transactional的注释文档，下面有个If no rules are relevant to the exception,it will be treated like {@link org.springframework.transaction.interceptor.DefaultTransactionAttribute} (rolling back on runtime exceptions).默认会使用RuntimeException,那为什么Spring默认回滚RuntimeException，因为Java把Exception分为两种。 checked Exception：Exception类本身，以及Exception的子类中除了”运行时异常”之外的其它子类都属于被检查异常。 unchecked Exception： RuntimeException和Error都属于未检查异常。 /** * Describes transaction attributes on a method or class. * * &lt;p&gt;This annotation type is generally directly comparable to Spring&apos;s * &#123;@link org.springframework.transaction.interceptor.RuleBasedTransactionAttribute&#125; * class, and in fact &#123;@link AnnotationTransactionAttributeSource&#125; will directly * convert the data to the latter class, so that Spring&apos;s transaction support code * does not have to know about annotations. If no rules are relevant to the exception, * it will be treated like * &#123;@link org.springframework.transaction.interceptor.DefaultTransactionAttribute&#125; * (rolling back on runtime exceptions). * * &lt;p&gt;For specific information about the semantics of this annotation&apos;s attributes, * consult the &#123;@link org.springframework.transaction.TransactionDefinition&#125; and * &#123;@link org.springframework.transaction.interceptor.TransactionAttribute&#125; javadocs. * * @author Colin Sampaleanu * @author Juergen Hoeller * @author Sam Brannen * @since 1.2 * @see org.springframework.transaction.interceptor.TransactionAttribute * @see org.springframework.transaction.interceptor.DefaultTransactionAttribute * @see org.springframework.transaction.interceptor.RuleBasedTransactionAttribute */ 因此，如果发生的不是RuntimeException，而你有没有配置rollback for ，那么，异常就不会回滚。 service 内部方法调用就是一个没有开启事务控制的方法调用一个开启了事务控制方法，不会事务回滚。 AccountService类@Servicepublic class AccountService &#123; @Autowired private AccountDao accountDao; /** * 完成转钱业务,transfer方法开启事务 * * @param out * @param in * @param money */ @Transactional(isolation = Isolation.DEFAULT, propagation = Propagation.REQUIRED) public void transfer(String out, String in, double money) &#123; Account account = new Account(); account.setName(out); account.setMoney(money); accountDao.out(account); int i = 1 / 0; account.setName(in); accountDao.in(account); &#125; /** * 完成转钱业务,transferProxy方法没有开启事务 * * @param out * @param in * @param money */ public void transferProxy(String out, String in, double money) &#123; System.out.println(&quot;调用transfer方法 开始&quot;); transfer(out, in, money); System.out.println(&quot;调用transfer方法 结束&quot;); &#125;&#125; AccountAction类@RestControllerpublic class AccountAction &#123; @Autowired private AccountService accountService; @RequestMapping(&quot;/transfer&quot;) public String transfer(String out, String in, double money) &#123; accountService.transfer(out, in, money); return &quot;transfer&quot;; &#125; @RequestMapping(&quot;/transferProxy&quot;) public String transferProxy(String out, String in, double money) &#123; accountService.transferProxy(out, in, money); return &quot;transfer&quot;; &#125;&#125; 通过transferProxy方法调用transfer方法时 Logging initialized using &apos;class org.apache.ibatis.logging.stdout.StdOutImpl&apos; adapter.调用transfer方法 开始Creating a new SqlSessionSqlSession [org.apache.ibatis.session.defaults.DefaultSqlSession@a0dbf4] was not registered for synchronization because synchronization is not activeJDBC Connection [com.mchange.v2.c3p0.impl.NewProxyConnection@6386ed [wrapping: com.mysql.jdbc.JDBC4Connection@9f2009]] will not be managed by Spring==&gt; Preparing: update account set money = money - ? where name = ?==&gt; Parameters: 100.0(Double), aaa(String)&lt;== Updates: 1Closing non transactional SqlSession [org.apache.ibatis.session.defaults.DefaultSqlSession@a0dbf4] 发现没有开启事务 直接调用transfer方法时 Logging initialized using &apos;class org.apache.ibatis.logging.stdout.StdOutImpl&apos; adapter.Creating a new SqlSessionRegistering transaction synchronization for SqlSession [org.apache.ibatis.session.defaults.DefaultSqlSession@238be2]JDBC Connection [com.mchange.v2.c3p0.impl.NewProxyConnection@a502e0 [wrapping: com.mysql.jdbc.JDBC4Connection@3dbe42]] will be managed by Spring==&gt; Preparing: update account set money = money - ? where name = ?==&gt; Parameters: 100.0(Double), aaa(String)&lt;== Updates: 1Releasing transactional SqlSession [org.apache.ibatis.session.defaults.DefaultSqlSession@238be2]Transaction synchronization deregistering SqlSession [org.apache.ibatis.session.defaults.DefaultSqlSession@238be2]Transaction synchronization closing SqlSession [org.apache.ibatis.session.defaults.DefaultSqlSession@238be2] 我们都知道Spring事务管理是通过AOP代理实现的，可是那么什么条件会使得AOP代理开启？通过查看Sprin官方文档，发现只有把整个Service设为事务控制时，才会进行AOP代理。如果我们通过一个没有事务的transferProxy方法去调用有事务的transfer方法，是通过this引用进行调用，没有开启事务，即使发生了RuntimeException也不会回滚。 然后 Spring父子容器覆盖Spring容器优先加载由ServletContextListener（对应applicationContext.xml）产生的父容器，而SpringMVC（对应mvc_dispatcher_servlet.xml）产生的是子容器。子容器Controller进行扫描装配时装配的@Service注解的实例是没有经过事务加强处理，即没有事务处理能力的Service，而父容器进行初始化的Service是保证事务的增强处理能力的。如果不在子容器中将Service exclude掉，此时得到的将是原样的无事务处理能力的Service，因为在多上下文的情况下，如果同一个bean被定义两次，后面一个优先。 当我们在applicationContext.xml,spring-mvc.xml都配置如下扫描包时，spring-mvc.xml中的service就会覆盖applicationContext.xml中的service。 context:component-scan base-package=&quot;net.morethink&quot;/&gt; 注意：当我们使用JUnit测试的时候，不会出现这种情况。JUnit配置如下@RunWith(SpringJUnit4ClassRunner.class)@ContextConfiguration(locations = &#123;&quot;classpath:applicationContext.xml&quot;, &quot;classpath:dispatcher-servlet.xml&quot;&#125;)@WebAppConfigurationpublic class AccountActionTest &#123; protected MockMvc mockMvc; @Autowired protected WebApplicationContext wac; @Before() //这个方法在每个方法执行之前都会执行一遍 public void setup() &#123; mockMvc = MockMvcBuilders.webAppContextSetup(wac).build(); //初始化MockMvc对象 &#125; @Test public void testTransfer() throws Exception &#123; String responseString = mockMvc.perform( get(&quot;/transfer&quot;) //请求的url,请求的方法是get .contentType(MediaType.APPLICATION_FORM_URLENCODED) //数据的格式 .param(&quot;out&quot;, &quot;aaa&quot;) .param(&quot;in&quot;, &quot;bbb&quot;) .param(&quot;money&quot;, &quot;100&quot;) ).andExpect(status().isOk()) //返回的状态是200// .andDo(print()) //打印出请求和相应的内容 .andReturn().getResponse().getContentAsString(); //将相应的数据转换为字符串 System.out.println(responseString); &#125; @Test public void testTransferProxy() throws Exception &#123; String responseString = mockMvc.perform( get(&quot;/transferProxy&quot;) //请求的url,请求的方法是get .contentType(MediaType.APPLICATION_FORM_URLENCODED) //数据的格式 .param(&quot;out&quot;, &quot;aaa&quot;) .param(&quot;in&quot;, &quot;bbb&quot;) .param(&quot;money&quot;, &quot;100&quot;) ).andExpect(status().isOk()) //返回的状态是200// .andDo(print()) //打印出请求和相应的内容 .andReturn().getResponse().getContentAsString(); //将相应的数据转换为字符串 System.out.println(responseString); &#125;&#125; 可能因为JUnit不会产生父子容器。 还有可能是其它配置文件出错，例如，连接池配置为多例 参考文档 Spring声明式事务为何不回滚 Spring中@Transactional事务回滚（含实例详细讲解，附源码） 深入研究java.lang.ThreadLocal类 Spring单实例、多线程安全、事务解析]]></content>
      <categories>
        <category>Spring</category>
      </categories>
      <tags>
        <tag>事务</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[怎么使用IDEA]]></title>
    <url>%2Fhow-to-use-IDEA%2F</url>
    <content type="text"><![CDATA[war 和 war exploded war部署首先通过IDEA生成.war工程文件，然后将WEB工程以包的形式上传到服务器，因此会替代服务器本来同名的web app项目。 war exploded模式直接将WEB工程以当前文件夹的位置关系上传到服务器。 使用： war模式这种可以称之为是发布模式，看名字也知道，这是先打成war包，再发布； war exploded模式是直接把文件夹、jsp页面 、classes等等移到IDEA生成的Tomcat部署文件夹里面，进行加载部署，不会替代本来tomcat中的同名web app项目。因此这种方式支持热部署，一般在开发的时候也是用这种方式。 在平时开发的时候，使用热部署的话，应该对Tomcat中进行相应的设置,更改图中两处标记为 update classes and resources:这样的话修改的html，jsp或者第三方模版框架例如 Freemarker热部署才可以生效。 热部署插件 JRebel 在 Java Web 开发中， 一般更新了 Java 文件后要手动重启 Tomcat 服务器， 才能生效， 浪费不少生命啊， 自从有了 JRebel 这神器的出现， 不论是更新 class 类还是更新 Spring 配置文件都能做到立马生效，大大提高开发效率。 IntelliJ IDEA 的 Java 热部署插件 JRebel 安装及使用 常用插件 阿里巴巴规约检测 翻译插件 wakatime]]></content>
      <categories>
        <category>工具</category>
      </categories>
      <tags>
        <tag>IDEA</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[有一种斜落下来的幽光]]></title>
    <url>%2F%E8%AF%BB%E4%B9%A6%2FThere-is-a-light-that-falls-down%2F</url>
    <content type="text"><![CDATA[By:Emily Dickinson译:余光中 冬日的下午往往有一种斜落下来的幽光，压迫着我们，那重量如同大教堂中的琴响。它给我们以神圣的创伤；我们找不到斑痕。只有内心所引起的变化，将它的意义蕴存。 没人能够使它感悟；它是绝望的烙印，一种无比美妙的痛苦，借大气传给我们。 当它来时，四野都倾听，阴影全屏住呼吸；当去时，远得像我们遥望死亡的距离。]]></content>
      <categories>
        <category>读书</category>
      </categories>
  </entry>
  <entry>
    <title><![CDATA[怎么解决问题]]></title>
    <url>%2F%E6%80%8E%E4%B9%88%E8%A7%A3%E5%86%B3%E9%97%AE%E9%A2%98%2F</url>
    <content type="text"><![CDATA[编程总是会遇到很多问题，你解决问题的、方式反映你的思维方式，你的思维方式决定你的未来，很多事情可以靠经验解决，思维却需要不断的锻炼。 程序员不仅仅是程序员，他更应该是一个高效的学习者。 以下主要来源于马士兵老师(很有参考意义)： 在编程中，每个人都会难免遇到问题，那么，遇到问题之后，环境配不通，程序调不过，运行不正常，遇见这些恼人的问题的时候，该怎么办呢？首先我要恭喜你，遇见问题，意味着你又有涨经验的机会了，每解决一个问题，你的JAVA经验值就应该上升几百点，问题遇到的越多，知识提升的越快。 但是总是解决不了也是很恼人的，怎么办呢？ 我一般要求我们的学生都是这样来进行的。 当你遇到一个问题的时候： 要仔细的观察错误的现象，是的，要仔细英语是成为一个牛逼程序员的必经之路 有不少同学的手非常快，在编译一个程序的时候，报了一大堆的错误，扫了一眼之后就开始盯着代码一行一行的找，看清什么错误了吗？没有！有的时候安装软件出问题了，一个对话框弹出来说出错了，马上举手问老师：“不得了了，出错了，出错了”。 “什么错误？” “还没看呢？” 这都是典型的不上心的方法！请记住，学习编程并不是一件很容易的事情，自己首先要重视，要用心才可以。在开发中，仔细观察出错信息，或者运行不正常的信息，是你要做的第一件事。读清楚了，才能在以后的步骤中有的放矢，哭了半天，总要知道哭的是谁才成。 这里又分三种情况： A:错误信息读懂了，那么请进入2步：要仔细思考问题会出在哪些环节 B:没读懂，愣是一点没看懂，进入第4步吧：google C:读了个半懂，有些眉目但是不太能确定，第2步和第4步结合着来。 要仔细思考问题会出在哪些环节（重要）当你读懂了一个问题之后，要好好的思考这个问题可能会在哪些环节上出错。 一辆汽车从总成品线上下来，车门子关不上！ 哪错了？你怎么查？ 当然是顺着生产线一站一站的查下来。 程序也是一样的，也是一系列语句完成后产生的结果。 写一个网络程序，总是发现服务器端打印不出接收的数据，有几个环节会出错？仔细分析这个现象的环节： 客户端产生数据-&gt;按“发送”按钮-&gt;发送到服务器-&gt;服务器接收到后打印 这几个环节都有可能会出错： 有可能客户端根本就没产生数据，有可能发送按钮按下去后根本就没发出去，或者发出去的不是你产生的东西，或者根本就没连接网络，或者发送出去服务器没有接收到，或者接收到之前就打印了等等等等。 学着去这样仔细的分析程序的环节和这些环节可能会产生的问题，你的经验值定然会大幅度快速的提升，这样做很累人，但是一件事情如果做下来一点都不累的话，这个东西还有价值吗？ 在网页A输入了一个人的名字，提交到B，首先存储到数据库，然后再读出来，发现乱码！怎么办？当然是分析环节： 客户输入-&gt;HTTP发送-&gt;B接收-&gt;存储到数据库-&gt;读出-&gt;展现到网页 每个环节都可能出问题，怎么才能知道哪里出的问题？继续往下读。 如何定位错误（重要）分析清楚有哪些环节之后，下一步就是定位到底什么环节出错了 定位有以下三种办法： A 打印输出，比如java的System.out.println()，比如js的alert()，这种办法常用，必须掌握 B Debug，可以参考我们的视频《坦克大战》，详细讲了Eclipse的调试。 C 删掉一部分调试一部分，也就是去掉一部分的功能，做简化，然后调试剩下的功能，JSP和JavaScript常用。 如果还不行，google吧还查不出来？恭喜你，你遇到的错误是值得认真对待的错误，是会影响你学习生涯的错误，问一下google或者百度吧。照着下面的方法查查看。 先精后粗，首先先进行尽量精确的查找，比如一个错误，SocketException，你怀疑它是在connect()方法出的问题，那么当然是选这样的关键词java connect SocketException 先中后英，本着以解决问题为主的想法，练习英文还是先放在一边吧，首先应该在中文网页中查询，还不行的话，搜索英文的吧。有很多东西就像一层窗户纸，远看灰蒙蒙怪唬人的，你壮着胆子一捅，它就破了。阅读英文的书籍就是如此，不是想象中的那么困难(宁可在沙场上战死，也不能被吓死不是吗) 信息筛选，搜索出来的结果不见得能够完全匹配，建议大家多阅读前几页的搜索结果，多打开几个网页看看，不过，我的经验是超过3页一般就没意义了，所以超过3页还没有找到合适的答案，或许应该调整一下关键词，或者放粗整个搜索的结果了。 经常的进行知识难点的查询，如果一个问题牵扯的面比较广，就干脆到网上搜索一些相关的专题，比如“java 乱码 mysql” “oracle 创建用户”等等，如果有必要，不要犯懒，勤动手写一些小小的测试程序，来弄明白知识点的细节。这也是涨知识的重要的途径。 什么？还不行？那么就BBS吧如果实在还不行，就到BBS上面问一问高手吧。 到哪个BBS上？ google或者百度不就行了么？关键词“java论坛”“java bbs” 然后在搜索结果里好好的看看那些活动频繁的论坛，以后就是你经常光顾的地方了。CSDN论坛首页就很不错。 向别人提问是非常需要技巧的！ 曾经有人问我这样的问题：“请问如何才能学好java呢？”这个要求太泛泛了。还有人给我一段代码甚至jar包也寄过来，然后说老师的我的代码中有个错误您帮我查查。 我没有办法去花大量的时间建立环境去调试一个还不知道存在不存在的错误！ 还有人在BBS上问这样的问题：“是否有人能帮我完成一个完整聊天的程序？请帮我写一个登陆模块吧！” 这个要求有些过分了，有人帮你做是你的运气，没有人帮你是正常反应。 向别人提问，应该首先确定你已经做了自己应该做的事，简单说是我前面列举的1，2，3，4步你都作过了，然后再求助于人。不要没有经过认真思考就草率的向别人提问，自己也不会有长足进步的。 那我该怎么样向别人提问呢？ 在google或百度搜索《提问的智慧》，你会找到答案。 在这里我给出一个链接！提问的智慧，磨刀不误砍柴功，先花点时间去阅读一下吧。 得到别人的回答，要懂得感恩。不需要去写信感谢，不需要支付费用，不需要那些花言巧语，做到下面这一点就够了： 当你逐步成为高手的时候，要回答别人在论坛上提出的问题，有时间有精力的前提之下！]]></content>
      <categories>
        <category>编程思考</category>
      </categories>
      <tags>
        <tag>方法论</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[如果你真的想做一件事，你一定会找到一个方法]]></title>
    <url>%2Fthere-is-away%2F</url>
    <content type="text"><![CDATA[写博客这件事情给我最大的体会就是，一件事情如果你能够坚持做8年，那么不管效率和频率多低，最终总能取得一些很可观的收益。而另一个体会就是，一件事情只要你坚持得足够久，“坚持”就会慢慢变成“习惯”。原本需要费力去驱动的事情便成了家常便饭，云淡风轻。–刘未鹏 分享一个我非常喜欢的故事，这篇文章对我影响很深，每当我想放弃我的计划时，就会响起这句话： Where there is a will，there is away。然后我的计划就又能继续下去了。这句话就是意志力最直接的体现，绝不放弃自己想做的事，不管发生什么事情，你一定会想到一个解决这件事的办法。 希望每个能看到这篇文章的人，要要记得下面的话： 如果你真的想做一件事，你一定会找到一个方法；如果你不想做一件事，你一定会找到一个借口。 &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;1953 年11月13日，丹麦首都哥本哈根。消防队的电话总机在清晨三点收到一个电话。二十二岁的年青消防员埃里希在值班。 「喂喂！这里是消防队」。电话的那端没人回答，可是埃里希听到一沉重的呼吸声。 后来一个十分激动的声音，说：「救命，救命啊！我站不起来！我的血在流！」 「别慌，太太」，埃里希回答，「我们马上就到，您在那里？」 「我不知道。」 「不在您的家里？」 「是的，我想是在家里。」 「家在哪里，哪条街？」 「我不知道，我的头晕，我在流血。」 「您至少要告诉我您叫什么名字！」 「我记不得了，我想我撞到了头。」 「请不要把电话挂掉。」 埃里希拿起第二具电话，拨到电话公司，回答他的是一个年老的男士。 「请您帮我找一下一个电话客户的号码，这客户现在正和消防总队通电话。」 「不，我不能，我是守夜的警卫，我不懂这些事。而且今天是星期六，没有任何人在。」 埃里希挂上电话。他有了另一个主意，于是问那女人：「你怎样找到消防队的电话号码的？」 「号码写在电话机上，我跌倒时把它给拖下来了。」 「那您看看电话机上是否也有您家的电话号码。」 「没有，没有别的任何号码。请你们快点来啊！」那女人的声音愈来愈弱。 「请您告诉我，您能看到什么东西？」 「我…我看到窗子，窗外，街上，有一盏路灯。」 好啊──埃里希想──她家面向大街，而且必定是在一层不太高的楼上，因为她看得见路灯。「窗户是怎样的？」他继续查问，「是正方形的吗？」 「不，是长方形的。」 那么，一定是在一个旧区内。 「您点了灯吗？」 「是的，灯亮着。」 埃里希还想问，但不再有声音回答了。 需要赶快采取行动！但是做什么？ 埃里希打电话给上司，向他陈述案情。 上司说：「一点办法也没有。不可能找到那个女人。而且，」他几乎生起气来，「那女人占了我们的一条电话线，要是哪里发生火警？」 但是埃里希不愿放弃。救命是消防队员的首要职责！他是这样被教导的。 突然，他兴起一个疯狂的念头。上司听了，吓坏了：「人们会以为原子战争爆发了！」他说。「在深夜，在哥本哈根这样一个大都会里 ……」 「我恳求您！」埃里希坚持，「我们必须赶快行动，否则全都徒劳无益！」 电话线的另一端静默了片刻，而后埃里希听到答复：「好的，我们就这么做。我马上来。」 十五分钟后，二十辆救火车在城中发出响亮的警笛声，每辆车在一个区域内四面八方的跑。 那女人已经不能再说话了，但埃里希仍听到她那急促的呼吸声。 十分钟后埃里希喊说：「我听到电话里传来警笛声！」 队长透过收发对讲机，下令：「一号车，熄灭警笛！」而后转问埃里希。 「我还听到警笛声！」他答说。 「二号车，熄灭警笛！」 「我还听得见… 。」 直到第十二辆车，埃里希喊说：「我现在听不见了。」 队长下令：「十二号车，再放警笛。」 埃里希告知：「我现在又听到了，但越走越远！」 「十二号车掉回头！」队长下令。 不久，埃里希喊道：「又逐渐地近了，现在声音非常刺耳，应该刚好到了正确的路上。」 「十二号车，你们找一个有灯光的窗户！」 「有上百盏的灯在亮着，人们出现在窗口为看发生了什么事！」 「利用扩音机！」队长下令。 埃里希经由电话听到扩音机的声音：「各位女士和先生，我们正在寻找一个生命有严重危险的妇女。我们知道她在一间有灯光的房间里，请你们关掉你们的灯。」 所有的窗户都变黑了，除了一个。 过了一会儿，埃里希听到消防队员闯入房间，而后一个男音向对讲机说：「这女人已失去知觉，但脉搏仍在跳动。我们立刻把她送到医院。我相信有救。」 海伦．索恩达──这是那女人的名字──真的获救了。她苏醒了，几个星期后，也恢复了记忆。 如果你真的想做一件事，你一定会找到一个方法；如果你不想做一件事，你一定会找到一个借口。「 Where there is a will，there is a way。」]]></content>
      <categories>
        <category>读书</category>
      </categories>
  </entry>
  <entry>
    <title><![CDATA[Spring 事务]]></title>
    <url>%2FSpring-Transaction%2F</url>
    <content type="text"><![CDATA[事务 事务：逻辑上的一组操作,这组操作要么全部成功,要么全部失败 事务四大特性 原子性: 事务是一个不可分割的工作单位,事务中的操作要么都发生,要么都不发生 一致性: 事务前后数据的完整性必须保持一致(例如:两个人转账,转账前后总金额的数目都是固定的) 隔离性: 多个用户并发访问数据库时,一个用户的事务不能被其他用户的事务所干扰,多个并发事务之间数据要相互隔离(例如:假设有两个事务同时在操作数据库,例如张三修改一个记录,同时李四也在修改这个记录,会导致该记录被重复修改,或者第一次修改的记录被第二次记录给覆盖掉) 持久性: 一个事务一旦被提交,它对数据库中数据的改变就是永久性的,即使数据库发生故障也不应该对其有任何影响 事务隔离通常来说，在一个应用中，会同时有多个用户访问数据库，在并发访问数据库的时候，可能会有，以下三个问题 脏读 一个事务读取了另一个事务改写但还未提交的数据，如果这些数据被回滚，则读到的数据是无效的。 不可重复读 在同一个事务中，多次读取同一数据返回的结果有所不同。 幻读(“幻读”是不可重复读的一种特殊场景) 一个事务读取了几行记录后，另一个事务插入一些记录，幻读就发生了。再后来的查询中，第一个事务就会发现有些原来没有的记录。 DBMS通过加锁来进行并发控制，不同的隔离级别拥有不同的持锁时间。 “C”-表示锁会持续到事务提交。 “S” –表示锁持续到当前语句执行完毕。如果锁在语句执行完毕就释放则另外一个事务就可以在这个事务提交前修改锁定的数据，从而造成混乱。 隔离级别l 写操作 读操作 范围操作 (…where…) 未提交读 S S S 提交读 C S S 可重复读 C C S 可序列化 C C C 隔离级别 未提交读(READ_UNCOMMITED);允许你读取还未提交的改变了的数据，可能导致脏，幻，不可重复读。 提交读(READ_COMMINTED):允许在并发事务已经提交后读取，可防止脏读，但幻读和不可重复读还是有可能发生。 可重复读(REPEATABLE_READ):对相同字段的多次读取是一致的，除非数据被事务本身改变，可防止脏读，不可重复读，但幻读仍有可能出现。 可序列化(SERILIZABLE):完全服从ACID的隔离级别，确保不发生脏读，幻读，不可重复读，这在所有的隔离级别中是最慢的，它是典型的完全通过锁定在事务中涉及的数据表来完成的。 如下表所示： 隔离级别 脏读 不可重复读 幻影读 未提交读 可能发生 可能发生 可能发生 提交读 - 可能发生 可能发生 可重复读 - - 可能发生 可序列化 - - - TransactionDefinition定义事务隔离级别 默认事务隔离级别 除了以上的数据库提供的事务隔离级别，spring提供了Default隔离级别，该级别表示spring使用后端数据库默认的隔离级别。 MySQL：REPATABLE_READ(可能出现幻读) Oracle：READ_COMMITTED(可能出现不可重复读和幻读) 并发操作怎么设置数据库隔离级别 这个要看你具体使用场景，例如你是并发的时候去写，还是去读，还是去读写，一般来说是读写，那么一般是 repeatable read，虽然他有幻读得可能性，但是一般去发生幻读得业务是不常见的。 事务传播行为事务的传播行为：解决业务层的方法之间的相互调用的问题(在调用方法的过程中,事务是如何传递的) 事务的传播行为有七种，又分为三类： 如果 A 方法中有事务，则调用 B 方法时就用该事务，即：A和B方法在同一个事务中。 PROPAGATION_REQUIRED：如果 A 方法中没有事务，则调用 B 方法时就创建一个新的事务，即：A和B方法在同一个事务中。 PROPAGATION_SUPPORTS：如果 A 方法中没有事务，则调用 B 方法时就不使用该事务。 PROPAGATION_MANDATORY：如果 A 方法中没有事务，则调用 B 方法时就抛出异常。 A 方法和 B 方法不在同一个事务里面。 PROPAGATION_REQUIRES_NEW：如果 A 方法中有事务，则挂起并新建一个事务给 B 方法。 PROPAGATION_NOT_SUPPORTED：如果 A 方法中有事务，则挂起。 PROPAGATION_NEVER：如果 A 方法中有事务，则报异常。 如果 A 方法有的事务执行完，设置一个保存点，如果 B 方法中事务执行失败，可以滚回保存点或初始状态。 PROPAGATION_NESTED ：如果当前事务存在，则嵌套事务执行 重点的三种： PROPAGATION_REQUIRED PROPAGATION_REQUIRES_NEW PROPAGATION_NESTED Spring事务管理Spring将事务管理分成了两类: 编程式事务管理 手动编写代码进行事务管理(很少使用) 声明式事务管理 基于TransactionProxyFactoryBean的方式(很少使用),需要为每个进行事务管理的类,配置一个TransactionProxyFactoryBean进行增强 基于AspectJ的xml方式(经常使用),一旦配置好,类上不需要添加任何东西 基于注解(经常使用),配置简单,需要在业务层类上添加一个@Transactionl的注解 编程式事务管理TransactionProxyFactoryBean&lt;bean id="accountServiceProxy" class="org.springframework.transaction.interceptor.TransactionProxyFactoryBean"&gt; &lt;property name="target" ref="accountService"/&gt; &lt;property name="transactionManager" ref="transactionManager"/&gt; &lt;property name="transactionAttributes"&gt; &lt;props&gt; &lt;!-- prop格式： * PROPAGATION：事务的传播行为 * ISOLATION ：事务的隔离级别 * readOnly ：只读(不可以进行修改,插入,删除的操作) * -Exception ：发生哪些异常回滚事务 * +Exception ：发生哪些异常事务不回滚 --&gt; &lt;prop key="transfer"&gt;PROPAGATION_REQUIRED,readOnly&lt;/prop&gt; &lt;/props&gt; &lt;/property&gt; &lt;/bean&gt; &lt;bean id="transactionManager" class="org.springframework.jdbc.datasource.DataSourceTransactionManager"&gt; &lt;property name="dataSource" ref="dataSource"/&gt; &lt;/bean&gt; AspectJ&lt;bean id="transactionManager" class="org.springframework.jdbc.datasource.DataSourceTransactionManager"&gt; &lt;property name="dataSource" ref="dataSource"/&gt; &lt;/bean&gt; &lt;!-- 配置事务的通知（事务的增强） --&gt; &lt;tx:advice id="txAdvice" transaction-manager="transactionManager"&gt; &lt;tx:attributes&gt; &lt;!-- propagation ：传播行为 isolation ：隔离级别 read-only ：只读 rollback-for ：发生哪些异常时回滚 no-rollback-for ：发生哪些异常时不回滚 timeout ：过期信息 --&gt; &lt;tx:method name="transfer" propagation="REQUIRED"/&gt; &lt;/tx:attributes&gt; &lt;/tx:advice&gt; &lt;!-- 配置aop切面 --&gt; &lt;aop:config&gt; &lt;!-- 配置切入点 --&gt; &lt;aop:pointcut expression="execution(* net.morethink.service.AccountService+.*(..))" id="pointcut1"/&gt; &lt;!-- 配置切面 --&gt; &lt;aop:advisor advice-ref="txAdvice" pointcut-ref="pointcut1"/&gt; &lt;/aop:config&gt; @Transactionl&lt;bean id="transactionManager" class="org.springframework.jdbc.datasource.DataSourceTransactionManager"&gt; &lt;property name="dataSource" ref="dataSource"/&gt;&lt;/bean&gt;&lt;tx:annotation-driven transaction-manager="transactionManager"/&gt; 参考文档 事务隔离-维基百科 全面分析 Spring 的编程式事务管理及声明式事务管理-IBM MyBatis6：MyBatis集成Spring事物管理（下篇）-博客园]]></content>
      <categories>
        <category>Spring</category>
      </categories>
      <tags>
        <tag>事务</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[CentOS7下 Java、Tomcat、MySQL、Maven热部署]]></title>
    <url>%2FLinux-Java%2F</url>
    <content type="text"><![CDATA[本文介绍了CentOS7 64位下Java、Tomcat、MySQL、Maven热部署等服务器环境的搭建和调试过程。 学生服务器资源获取方法： 云+校园计划 - 腾讯云 阿里云云翼计划 github 学生包，里面有Digital Ocean 50美元的VPS可用 已经将所需要的工具(Xshell,Xftp、FileZilla等sftp上传工具，jdk-8u101-linux-x64.tar.gz和apache-tomcat-9.0.0.M10.tar.gz)上传至百度云 http://pan.baidu.com/s/1qYRms8G Java环境配置环境准备通过uname -r判断系统是多少位 64位 ： 出现x86_64 32位 ： 出现i686或i386 安装Java JDK8.0 建立Java目录，存放Java和Tomcat cd /usr/local/ mkdir Java cd Java 使用FileZilla将下载好的jdk-8u101-linux-x64.tar.gz 和 apache-tomcat-9.0.0.M10.tar.gz上传至Java目录下(传送的国外服务器很慢,国内几乎是国外的十倍，但是也只有两三百KB，也可能是电脑问题) 将上传的jdk解压，然后重命名为jdk tar -zxv -f jdk-8u101-linux-x64.tar.gz mv jdk1.8.0_101 jdk cd jdk 配置环境变量Environment=JAVA_HOME=/usr/local/Java/jdk vim /etc/profile 打开之后按键盘（i）进入编辑模式,将下面的内容复制到底部 JAVA_HOME=/usr/local/Java/jdkPATH=$JAVA_HOME/bin:$PATHCLASSPATH=$JAVA_HOME/jre/lib/ext:$JAVA_HOME/lib/tools.jarexport PATH JAVA_HOME CLASSPATH 写完之后我们按键盘（ESC）按钮退出，然后按（:wq）保存并且关闭Vim。 使用 source /etc/profile命令使其立即生效 通过java -version验证Java是否配置成功。 安装Tomcat9 在Java目录下解压上面一步已经上传上去的Tomcat9.0 tar -zxv -f apache-tomcat-9.0.0.M10.tar.gz mv apache-tomcat-9.0.0.M10 tomcat cd tomcat 启动命令为 /usr/local/Java/tomcat/bin/startup.sh 启动完成后还需开放8080端口(CentOS7这个版本的防火墙默认使用的是firewall，与之前的版本使用iptables不一样。 关于防火墙端口可以查看后面的参考文档) firewall-cmd –zone=public –add-port=8080/tcp –permanent出现success表明添加成功 更新防火墙规则即可： firewall-cmd –reload 重启防火墙 systemctl restart firewalld.service 然后再次在浏览器中输入http://ip:8080，如果看到tomcat系统界面，说明安装成功。 Tomcat 8080 端口无法访问 查看8080端口被那个程序占用(应该是Java) netstat -anp 然后再杀死占用进程。 可能是你的服务器提供商有安全组来控制端口，你需要去提供商那里开启端口(PS：我的阿里云服务器就是必须要设置端口安全组才可以访问端口) 关闭命令为 /usr/local/Java/tomcat/bin/shutdown.sh MySQL安装MySQLCentOS 7的yum源中貌似没有正常安装mysql时的mysql-sever文件，需要去官网上下载 # wget http://dev.mysql.com/get/mysql-community-release-el7-5.noarch.rpm# rpm -ivh mysql-community-release-el7-5.noarch.rpm# yum install mysql-community-server 成功安装之后重启mysql服务。初次安装mysql是root账户是没有密码的设置密码的方法 # systemctl start mysqld.service# mysql -uroot# mysql&gt;set password = password(&quot;你的密码&quot;);# mysql&gt;flush privileges;# mysql&gt; exit 远程连接进入MySQL后通过mysql&gt; GRANT ALL PRIVILEGES ON *.* TO &apos;root&apos;@&apos;%&apos; IDENTIFIED BY &apos;你的密码&apos; WITH GRANT OPTION; 可以允许任意主机访问你在服务器搭建的MySQL，可以吧%更换为你需要允许的IP和主机。 但是，在我进行远程连接的时候发生如下错误可以根据下面进行排错 服务器是否可以访问，是否能ping 通 安全组及端口号是否打开或者被占用 服务器是否运行或者关闭只允许本机访问 重启试试 当我排除上面3中情况后，发现属于第四中，可能是因为还没有立即生效。 中文乱码问题在我建立数据时发现中文无法插入，于是查看使用show variables like &#39;character%&#39;;如图所示，发现默认不是utf-8，于是通过在CentOS7中修改文件/usr/share/mysql/my-default.cnf，在[mysqld]，[mysql]，[client]下分别添加如下内容[mysqld]collation-server = utf8_unicode_ciinit-connect=&apos;SET NAMES utf8&apos;character_set_server = utf8[mysql]default-character-set=utf8[client]default-character-set=utf8 修改完成后，重启mysql服务，systemctl restart mysql，然后进入mysql，再次使用show variables like &#39;character%&#39;;命令查看，如图，发现一样，说明utf-8可以使用，但是无法插入中文，于是猜测可能是系统原生不支持utf-8，更改系统编码后发现果然如此。# cat /etc/locale.confLANG=&quot;en_US.UTF-8&quot;# vim /etc/locale.conf# cat /etc/locale.confLANG=&quot;zh_CN.UTF-8&quot; 于是更改为 LANG=&quot;zh_CN.UTF-8&quot;LANGUAGE=&quot;zh_CN.UTF-8&quot;SUPPORTED=&quot;zh_CN.UTF-8:zh_CN:zh:en_US.UTF-8:en_US:en&quot;SYSFONT=&quot;lat0-sun16&quot; 重启系统之后，再次重启MySQL服务。 如图，发现可以插入中文。 解决MySQL5.5忘记密码通过跳过权限安全检查设置新密码。 首先检查mysql服务是否启动，若已启动则先将其停止服务，可在开始菜单的运行，使用命令： net stop mysql，然后打开第一个cmd1窗口，切换到mysql的bin目录，运行命令：mysqld --defaults-file=&quot;C:\Program Files\MySQL\MySQL Server 5.5\my.ini&quot; --console --skip-grant-tables，将命令中的MySql版本更换你的版本。该命令通过跳过权限安全检查，开启mysql服务，这样连接mysql时，可以不用输入用户密码。此时已经开启了mysql服务了！这个窗口保留不关闭。 打开第二个cmd2窗口，连接mysql 输入命令：mysql -u root -p出现： Enter password: ，在这里直接回车，不用输入密码。 然后就就会出现登录成功的信息。 使用命令切换到mysql数据库：use mysql; 使用命令更改root密码：UPDATE user SET Password=PASSWORD(&#39;newpassword&#39;) where USER=&#39;root&#39;; 刷新权限：FLUSH PRIVILEGES; 然后退出，重新登录：quit 重新登录： 可以关掉之前的cmd1 窗口了。 然后用net start mysql 启动服务 登录：mysql -u root -p 出现输入密码提示，输入新的密码即可登录：Enter password: *********** 显示登录信息： 成功 就一切ok了 Maven 热部署Maven 热部署可以通过一行命令部署到本地服务器，没有问题的话就一行命令部署到正式服务器。及其方便了开发和部署。因为我的Tomcat9遇到很多问题。可以参考 maven自动部署到远程tomcat教程 进行部署和测试。 下面是我遇到的一个错误，因为没有配置IDEA的make 导致出错。[ERROR] Failed to execute goal org.apache.tomcat.maven:tomcat7-maven-plugin:2.2:deploy (default-cli) on project liwenhao: Cannot invoke Tomcat manager: Connection reset by peer: socket write error -&gt; [Help 1][ERROR][ERROR] To see the full stack trace of the errors, re-run Maven with the -e switch.[ERROR] Re-run Maven using the -X switch to enable full debug logging.[ERROR][ERROR] For more information about the errors and possible solutions, please read the following articles:[ERROR] [Help 1] 可以通过将make如下配置即可成功 war包部署在服务器乱码 可以通过配置如下属性，解决中文war包服务器乱码。&lt;properties&gt; &lt;project.build.sourceEncoding&gt;UTF-8&lt;/project.build.sourceEncoding&gt; &lt;project.reporting.outputEncoding&gt;UTF-8&lt;/project.reporting.outputEncoding&gt;&lt;/properties&gt; 配置完图。 在我通过mvn tomcat7:deploy命令热部署时，会出现mysql无法连接的情况，后来在我重新进行热部署的时候，没有出现这个问题。猜测应该是我的配置文件的问题 参考文档 centos 7 开放 80端口 centos7 设置中文 CentOS 7下彻底卸载MySQL数据库 CentOS7 远程访问MySQL Centos 7 mysql 5.7 给root开启远程访问权限，修改root密码 连接Mysql提示Can’t connect to local MySQL server through socket的解决方法 How To Install Apache Tomcat 8 on CentOS 7 windows环境中mysql忘记root密码的解决办法]]></content>
      <categories>
        <category>Linux</category>
      </categories>
      <tags>
        <tag>CentOS7</tag>
        <tag>Tomcat</tag>
        <tag>MySQL</tag>
        <tag>Maven</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[前端通过Nginx反向代理解决跨域问题]]></title>
    <url>%2FNginx-Cross-Domain%2F</url>
    <content type="text"><![CDATA[在前面写的一篇文章SpringMVC解决跨域问题，我们探讨了什么是跨域问题以及SpringMVC怎么解决跨域问题，解决方式主要有如下三种方式: JSONP CORS WebSocket 可是这几种方式都是基于服务器配置的，即对于自己的网站是可以通过这几种方式解决的，可是现在遇到另一个需求(前面提到过，写扇贝插件，我们不能更改扇贝的服务器配置，也不能发短信叫他们给我配置一下)。 本文探讨了前端如何通过Nginx反向代理的方式解决跨域问题。 跨域再次重申： 跨域是浏览器行为，不是服务器行为。 实际上，请求已经到达服务器了，只不过在回来的时候被浏览器限制了。就像Python他可以进行抓取数据一样，不经过浏览器而发起请求是可以得到数据，想到通过Nginx的反向代理来解决跨域问题。 代理所谓代理就是在我们和真实的服务器之间有一台代理服务器，我们所有的请求都是通过它来进行转接的。 正向代理正向代理就是我们访问不了Google，但是我在国外有一台vps，它可以访问Google，我访问它，叫它访问Google后，把数据传给我。 如图： 反向代理 大家都有过这样的经历，拨打10086客服电话，可能一个地区的10086客服有几个或者几十个，你永远都不需要关心在电话那头的是哪一个，叫什么，男的，还是女的，漂亮的还是帅气的，你都不关心，你关心的是你的问题能不能得到专业的解答，你只需要拨通了10086的总机号码，电话那头总会有人会回答你，只是有时慢有时快而已。那么这里的10086总机号码就是我们说的反向代理。客户不知道真正提供服务人的是谁。 反向代理隐藏了真实的服务端，当我们请求 www.baidu.com 的时候，就像拨打10086一样，背后可能有成千上万台服务器为我们服务，但具体是哪一台，你不知道，也不需要知道，你只需要知道反向代理服务器是谁就好了，www.baidu.com 就是我们的反向代理服务器，反向代理服务器会帮我们把请求转发到真实的服务器那里去。Nginx就是性能非常好的反向代理服务器，用来做负载均衡。 如图： 总结 正向代理隐藏了真实的客户端。 反向代理隐藏了真实的服务器。 Nginx 就是一个很好的反向代理服务器，当然apache也可以实现此功能。 windows下Apache配置参考这篇文章： Windows Apache服务器配置 NginxNginx（发音同engine x）是一个 Web服务器，也可以用作反向代理，负载平衡器和 HTTP缓存。该软件由 Igor Sysoev 创建，并于2004年首次公开发布。同名公司成立于2011年，以提供支持。 我在Windows下实现Nginx负载均衡提到过Windows下Nginx命令使用。 Nginx 反向代理模块 proxy_passproxy_pass 后面跟着一个 URL，用来将请求反向代理到 URL 参数指定的服务器上。例如我们上面例子中的 proxy_pass https://api.shanbay.com，则将匹配的请求反向代理到 https://api.shanbay.com。 通过在配置文件中增加proxy_pass 你的服务器ip,例如这里的扇贝服务器地址，就可以完成反向代理。 server &#123; listen 80; server_name localhost; ## 用户访问 localhost，则反向代理到https://api.shanbay.com location / &#123; root html; index index.html index.htm; proxy_pass https://api.shanbay.com; &#125;&#125; 配置html以文件方式打开一般的情况下，我们的HTML文件时放置在Nginx服务器上面的，即通过输入 http://localhost/index.html ，但是在前端进行调试的时候，我们可能是通过 使用 file:///E:/nginx/html/index.html 来打开HTML。服务器打开不是特别方便。 而我们之所以要部署在服务器上，是想要使用浏览器自带的CORS头来解决跨域问题，如果不想把HTML放置在Nginx中，而想通过本地打开的方式来调试HTML，可以通过自己添加Access-Control-Allow-Origin等http头，但是我们的AJAX请求一定要加上http://127.0.0.1/request，而不能直接是 /request，于是将nginx.conf作如下配置： location / &#123; root html; index index.html index.htm; # 配置html以文件方式打开 if ($request_method = &apos;POST&apos;) &#123; add_header &apos;Access-Control-Allow-Origin&apos; *; add_header &apos;Access-Control-Allow-Credentials&apos; &apos;true&apos;; add_header &apos;Access-Control-Allow-Methods&apos; &apos;GET, POST, OPTIONS&apos;; add_header &apos;Access-Control-Allow-Headers&apos; &apos;DNT,X-Mx-ReqToken,Keep-Alive,User-Agent,X-Requested-With,If-Modified-Since,Cache-Control,Content-Type&apos;; &#125; if ($request_method = &apos;GET&apos;) &#123; add_header &apos;Access-Control-Allow-Origin&apos; *; add_header &apos;Access-Control-Allow-Credentials&apos; &apos;true&apos;; add_header &apos;Access-Control-Allow-Methods&apos; &apos;GET, POST, OPTIONS&apos;; add_header &apos;Access-Control-Allow-Headers&apos; &apos;DNT,X-Mx-ReqToken,Keep-Alive,User-Agent,X-Requested-With,If-Modified-Since,Cache-Control,Content-Type&apos;; &#125; # 代理到8080端口 proxy_pass http://127.0.0.1:8080;&#125; 处理DELETE和PUT跨域请求而现在我的后台是restful风格的接口，采用了delete和put方法，而上面的配置就无能为力了。 可以通过增加对非简单请求的判断来解决DELETE和PUT跨域请求。 非简单请求是那种对服务器有特殊要求的请求，比如请求方法是PUT或DELETE，或者Content-Type字段的类型是application/json。 非简单请求的CORS请求，会在正式通信之前，增加一次HTTP查询请求，称为”预检”请求（preflight）。 服务器收到”预检”请求以后，检查了Origin、Access-Control-Request-Method和Access-Control-Request-Headers字段以后，确认允许跨源请求，就可以做出回应。 因此，为了使Nginx可以处理delete等非简单请求，Nginx需要作出相应的改变，更改配置如下 location / &#123; # 完成浏览器的&quot;预检&quot;请求 if ($request_method = &apos;OPTIONS&apos;) &#123; add_header Access-Control-Allow-Origin *; add_header Access-Control-Allow-Credentials true; add_header Access-Control-Allow-Methods &apos;GET, POST, PUT, DELETE, OPTIONS&apos;; add_header &apos;Access-Control-Allow-Headers&apos; &apos;DNT,X-Mx-ReqToken,Keep-Alive,User-Agent,X-Requested-With,If-Modified-Since,Cache-Control,Content-Type&apos;; return 204; &#125; # 配置html在本地打开 if ($request_method = &apos;POST&apos;) &#123; add_header &apos;Access-Control-Allow-Origin&apos; *; add_header &apos;Access-Control-Allow-Credentials&apos; &apos;true&apos;; add_header &apos;Access-Control-Allow-Methods&apos; &apos;GET, POST, OPTIONS&apos;; add_header &apos;Access-Control-Allow-Headers&apos; &apos;DNT,X-Mx-ReqToken,Keep-Alive,User-Agent,X-Requested-With,If-Modified-Since,Cache-Control,Content-Type&apos;; &#125; if ($request_method = &apos;GET&apos;) &#123; add_header &apos;Access-Control-Allow-Origin&apos; *; add_header &apos;Access-Control-Allow-Credentials&apos; &apos;true&apos;; add_header &apos;Access-Control-Allow-Methods&apos; &apos;GET, POST, OPTIONS&apos;; add_header &apos;Access-Control-Allow-Headers&apos; &apos;DNT,X-Mx-ReqToken,Keep-Alive,User-Agent,X-Requested-With,If-Modified-Since,Cache-Control,Content-Type&apos;; &#125; root html; index index.html index.htm; # 配置html在Nginx中打开 location ~* \.(html|htm)$ &#123; &#125; # 代理到8080端口 proxy_pass http://127.0.0.1:8080;&#125; 我们还必须把我们的html代码放在Nginx中html文件夹内，即使用Nginx当做我们的前端服务器。 URL重写有时候我们仅仅只想将/api下的url反向代理到后端，可以通过在nginx.conf中配置url重写规则如下： location / &#123; root html; index index.html index.htm; location ^~ /api &#123; rewrite ^/api/(.*)$ /$1 break; proxy_pass https://api.shanbay.com/; &#125; &#125; 这样的话，我们只用处理/api下的url。 在配置文件中我们通过rewrite将URL重写为真正要请求的URL，通过proxy_pass代理到真实的服务器IP或者域名。 Cookie如果Cookie的域名部分与当前页面的域名不匹配就无法写入。所以如果请求 www.a.com ，服务器 proxy_pass 到 www.b.com 域名，然后 www.b.com 输出 domian=b.com 的 Cookie，前端的页面依然停留在 www.a.com 上，于是浏览器就无法将 Cookie 写入。 可在nginx反向代理中设置： location / &#123; # 页面地址是a.com，但是要用b.com的cookie proxy_cookie_domain b.com a.com; #注意别写错位置了 proxy_cookie_path / /; proxy_pass http://b.com;&#125; 总结Nginx解决跨域问题通过Nginx反向代理将对真实服务器的请求转移到本机服务器来避免浏览器的”同源策略限制”。 参考文档： Nginx反向代理配置（解决跨域问题） 这两篇文章讲解了URL重写的规则和用法 nginx配置url重写 nginx配置location总结及rewrite规则写法 Nginx配置文件详解]]></content>
      <categories>
        <category>服务器</category>
      </categories>
      <tags>
        <tag>Nginx</tag>
        <tag>跨域</tag>
        <tag>反向代理</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[SpringMVC解决跨域问题]]></title>
    <url>%2FSpringMVC%E8%A7%A3%E5%86%B3%E8%B7%A8%E5%9F%9F%E9%97%AE%E9%A2%98%2F</url>
    <content type="text"><![CDATA[有个朋友在写扇贝插件的时候遇到了跨域问题。于是我对解决跨域问题的方式进行了一番探讨。 问题 API：查询单词URL： https://api.shanbay.com/bdc/search/?word={word}请求方式: GET参数： {word}, 必须，要查询的单词 报错为XMLHttpRequest cannot load http://localhost/home/saveCandidate. No &apos;Access-Control-Allow-Origin&apos; header is present on the requested resource. Origin &apos;null&apos; is therefore not allowed access. The response had HTTP status code 404. 这就是典型的跨域问题。 但是我在浏览器里输入URL是可以进行查询单词的操作的，有什么不同，即下面两个问题 为什么在浏览器地址栏输入URL不会出现跨域问题。 不在服务器运行的html是否可以完成一次http请求 经过Google和自己测试 跨域限制是浏览器行为，不是服务器行为。 浏览器认为地址栏输入时安全的,所以不限制认为是跨域。 可以，只要服务器配置为所有域都可以进行请求,那么不在服务器运行的HTML就可以完成http请求。 什么是跨域问题同源策略： 同源指的是域名（或IP），协议，端口都相同，不同源的客户端脚本(javascript、ActionScript)在没明确授权的情况下，不能读写对方的资源。 URL 解释 是否跨域 http://www.morethink.cn 原来的URL http://www.image.morethink.cn 子域名 跨域(cookie也无法访问) http://morethink.cn 不加www 跨域 https://www.morethink.cn 更改协议 跨域 http://www.morethink.cn:8080 更改端口号 跨域 原因： 同源政策的目的，是为了保证用户信息的安全，防止恶意的网站窃取数据。设想这样一种情况：A网站是一家银行，用户登录以后，又去浏览其他网站。如果其他网站可以读取A网站的Cookie，会发生什么？很显然，如果Cookie包含隐私（比如存款总额），这些信息就会泄漏。更可怕的是，Cookie往往用来保存用户的登录状态，如果用户没有退出登录，其他网站就可以冒充用户，为所欲为。因为浏览器同时还规定，提交表单不受同源政策的限制。由此可见，”同源政策”是必需的，否则 Cookie 可以共享，互联网就毫无安全可言了。 同源策略限制以下几种行为： Cookie、LocalStorage 和 IndexDB 无法读取 DOM 和 Js对象无法获得 AJAX 请求不能发送 模拟跨域问题测试URL为 http://localhost:80/home/allProductions 可以直接在浏览器console中执行 var xhr = new XMLHttpRequest();xhr.open('GET', 'http://localhost:80/home/allProductions',true);xhr.send();xhr.onreadystatechange=function() &#123; if(xhr.readyState == 4) &#123; if(xhr.status == 200) &#123; console.log(JSON.parse(xhr.responseText)); &#125; &#125;&#125; 在任意网站打开控制台，执行此段代码可以模拟跨域请求。 在知乎控制台打开报错如下 Mixed Content: The page at &apos;https://www.zhihu.com/question/26376773&apos; was loaded over HTTPS, but requested an insecure XMLHttpRequest endpoint &apos;http://localhost/home/allProductions&apos;. This request has been blocked; the content must be served over HTTPS. 因为知乎是https，报错与普通的http协议不同。 再澄清一下跨域问题： 并非浏览器限制了发起跨站请求，而是跨站请求可以正常发起，但是返回结果被浏览器拦截了。最好的例子是CRSF跨站攻击原理，无论是否跨域，请求已经发送到了后端服务器！ 但是，有些浏览器不允许从HTTPS的域跨域访问HTTP，比如Chrome和Firefox，这些浏览器在请求还未发出的时候就会拦截请求，这是一个特例。 在博客园控制台打开报错如下 XMLHttpRequest cannot load http://localhost/home/allProductions. No &apos;Access-Control-Allow-Origin&apos; header is present on the requested resource. Origin &apos;http://www.cnblogs.com&apos; is therefore not allowed access. 怎么解决跨域问题解决方案有很多 通过jsonp跨域 document.domain + iframe跨域 location.hash + iframe window.name + iframe跨域 postMessage跨域 跨域资源共享（CORS） 前端通过Nginx解决跨域问题 nodejs中间件代理跨域 WebSocket协议跨域 这里主要介绍SpringMVC解决跨域问题的方式。 JSONP CORS WebSocket JSONP可以直接参考Spring MVC 4.1 支持jsonp进行配置你的SpringMVC注解 JSONP 原理我虽然请求不了json数据，但是我可以请求一个Content-Type为application/javascript的JavaScript对象，这样就可以避免浏览器的同源策略。 就是当服务器接受到名为jsonp或者callback的参数时，返回Content-Type: application/javascript的结果，从而避免浏览器的同源策略检测。 在控制台中直接进行测试你的jsonp是否配置成功： function println(data) &#123; console.log(data);&#125;var url = "http://localhost:80/home/allProductions?&amp;callback=println";// 创建script标签，设置其属性var script = document.createElement('script');script.setAttribute('src', url);// 把script标签加入head，此时调用开始document.getElementsByTagName('head')[0].appendChild(script); 使用JQuery测试你的jsonp是否配置成功，因为控制台不能直接加载JQuery，需要自己建立html文件来进行测试： &lt;!DOCTYPE html&gt;&lt;html lang="en"&gt;&lt;head&gt; &lt;meta charset="UTF-8"&gt; &lt;title&gt;Title&lt;/title&gt; &lt;script src="http://cdn.bootcss.com/jquery/1.10.2/jquery.min.js"&gt;&lt;/script&gt; &lt;script type="text/javascript"&gt; function println(data) &#123; console.log(data); console.log('print'); &#125; function jsonp_test() &#123; $.ajax(&#123; type: "get", url: "http://localhost:80/home/allProductions", dataType: "jsonp", jsonp: "callback",//传递给请求处理程序或页面的，用以获得jsonp回调函数名的参数名(一般默认为:callback) jsonpCallback: "println", //返回后调用的处理函数 error: function () &#123; //请求出错的处理 alert("请求出错"); &#125; &#125;); &#125; &lt;/script&gt;&lt;/head&gt;&lt;body onload="jsonp_test()"&gt;&lt;/body&gt;&lt;/html&gt; CORSCORS是一个W3C标准，全称是”跨域资源共享”（Cross-origin resource sharing）。它允许浏览器向跨源服务器，发出XMLHttpRequest请求，从而克服了AJAX只能同源使用的限制。 CORS需要浏览器和服务器同时支持。 所有浏览器都支持该功能，IE浏览器不能低于IE10。整个CORS通信过程，都是浏览器自动完成，不需要用户参与。 对于开发者来说，CORS通信与同源的AJAX通信没有差别，代码完全一样。浏览器一旦发现AJAX请求跨源，就会自动添加一些附加的头信息，有时还会多出一次附加的请求，但用户不会有感觉。 实现CORS通信的关键是服务器。只要服务器实现了CORS接口，就可以跨源通信。 即CORS与普通请求代码一样。 CORS与JSONP相比 JSONP只能实现GET请求，而CORS支持所有类型的HTTP请求。 使用CORS，开发者可以使用普通的XMLHttpRequest发起请求和获得数据，比起JSONP有更好的错误处理。 JSONP主要被老的浏览器支持，它们往往不支持CORS，而绝大多数现代浏览器都已经支持了CORS。 @CrossOrigin注解此注解既可用于方法也可用于类 源码如下： @CrossOrigin(origins = "http://www.zhihu.com")@RequestMapping(value = "/allProductions", method = RequestMethod.GET)public Result getAllOldProductions() &#123;&#125; @CrossOrigin注解既可注解在方法上，也可注解在类上。 完成配置之后 XML全局配置所有跨域请求都可以访问 &lt;mvc:cors&gt; &lt;mvc:mapping path="/**" /&gt;&lt;/mvc:cors&gt; 更加细粒度的配置： &lt;mvc:cors&gt; &lt;mvc:mapping path="/api/**" allowed-origins="http://domain1.com, http://domain2.com" allowed-methods="GET, PUT" allowed-headers="header1, header2, header3" exposed-headers="header1, header2" allow-credentials="false" max-age="123" /&gt; &lt;mvc:mapping path="/resources/**" allowed-origins="http://domain1.com" /&gt;&lt;/mvc:cors&gt; WebSocketWebSocket是一种通信协议，使用ws://（非加密）和wss://（加密）作为协议前缀，在2008年诞生，2011年成为国际标准。所有浏览器都已经支持了。 它的最大特点就是，服务器可以主动向客户端推送信息，客户端也可以主动向服务器发送信息，是真正的双向平等对话，属于服务器推送技术的一种。 该协议不实行同源政策，只要服务器支持，就可以通过它进行跨源通信。 请求头信息：(多了个 origin) GET /chat HTTP/1.1Host: server.example.comUpgrade: websocketConnection: UpgradeSec-WebSocket-Key: x3JJHMbDL1EzLkh9GBhXDw==Sec-WebSocket-Protocol: chat, superchatSec-WebSocket-Version: 13Origin: http://example.com 响应头：(如果origin在白名单内) HTTP/1.1 101 Switching ProtocolsUpgrade: websocketConnection: UpgradeSec-WebSocket-Accept: HSmrc0sMlYUkAGmm5OPpG2HaGWk=Sec-WebSocket-Protocol: chat 相比于HTTP/2HTTP/2只是对HTML、CSS等JS资源的传输方式进行了优化，并没有提供新的JS API，不能用于实时传输消息，也无法推送指定的信息。 参考文档： 跨域 浏览器同源政策及其规避方法 跨域资源共享 CORS 详解 SpringMVC 跨域解决方法 Spring MVC 4.2 增加 CORS 支持 前端常见跨域解决方案（全）]]></content>
      <categories>
        <category>SpringMVC</category>
      </categories>
      <tags>
        <tag>跨域</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[SpringMVC空字符串转为null]]></title>
    <url>%2FSpringMVC-Empty-String-To-Null%2F</url>
    <content type="text"><![CDATA[空字符串转为null现在我遇到这样一个需求，那就是我想要吧前端传过来的值变为空，因为所谓前端的校验，其实都不是校验，如果前端传给后台一个表单，可是表单未填入值，我们后台进行判断的时候 既需要判断null，同时需要判断是否为&quot;&quot;, 并且如果你不希望数据库插入的是空字符串，而是null，那么转换和插入的就很麻烦 if (manager.getUsername().equals("") || manager.getUsername() == null) &#123; throw new ErrorException("用户名未填写"); &#125; if (manager.getPassword().equals("") || manager.getPassword() == null) &#123; throw new ErrorException("密码未填写"); &#125; 可是如果你在SpringMVC进行参数赋值处理之前就能把&quot;&quot;转换为null，那么你就只需要进行如下判断，并且插入数据库的一直是空值 if (manager.getUsername() == null) &#123; throw new ErrorException("用户名未填写"); &#125; if (manager.getPassword() == null) &#123; throw new ErrorException("密码未填写"); &#125; 实现方式 可以使用过滤器/拦截器把空字符串设置成null(未尝试) 注册一个SpringMVC HandlerAdapter来进行处理 使用SpringMVC时，所有的请求都是最先经过DispatcherServlet的，然后由DispatcherServlet选择合适的HandlerMapping和HandlerAdapter来处理请求，HandlerMapping的作用就是找到请求所对应的方法，而HandlerAdapter则来处理和请求相关的的各种事情。我们这里要讲的请求参数绑定也是HandlerAdapter来做的。 参数处理器我们需要写一个自定义的请求参数处理器，然后把这个处理器放到HandlerAdapter中，这样我们的处理器就可以被拿来处理请求了。 public class EmptyStringToNullModelAttributeMethodProcessor extends ServletModelAttributeMethodProcessor implements ApplicationContextAware &#123; ApplicationContext applicationContext; public EmptyStringToNullModelAttributeMethodProcessor(boolean annotationNotRequired) &#123; super(annotationNotRequired); &#125; @Override protected void bindRequestParameters(WebDataBinder binder, NativeWebRequest request) &#123; EmptyStringToNullRequestDataBinder toNullRequestDataBinderBinder = new EmptyStringToNullRequestDataBinder(binder.getTarget(), binder.getObjectName()); RequestMappingHandlerAdapter requestMappingHandlerAdapter = applicationContext.getBean(RequestMappingHandlerAdapter.class); requestMappingHandlerAdapter.getWebBindingInitializer().initBinder(toNullRequestDataBinderBinder, request); toNullRequestDataBinderBinder.bind(request.getNativeRequest(ServletRequest.class)); &#125; @Override public void setApplicationContext(ApplicationContext applicationContext) throws BeansException &#123; this.applicationContext = applicationContext; &#125;&#125; DataBinder继承自ExtendedServletRequestDataBinder，主要用来自定义数据绑定处理 public class EmptyStringToNullRequestDataBinder extends ExtendedServletRequestDataBinder &#123; public EmptyStringToNullRequestDataBinder(Object target, String objectName) &#123; super(target, objectName); &#125; protected void addBindValues(MutablePropertyValues mpvs, ServletRequest request) &#123; super.addBindValues(mpvs, request); for (PropertyValue propertyValue : mpvs.getPropertyValueList()) &#123; if (propertyValue.getValue().equals("")) propertyValue.setConvertedValue(null); &#125; &#125;&#125; 注册器 话要从HandlerAdapter里系统自带的处理器说起。我这边系统默认带了24个处理器，其中有两个ServletModelAttributeMethodProcessor，也就是我们自定义处理器继承的系统处理器。SpringMVC处理请求参数是轮询每一个处理器，看是否支持，也就是supportsParameter方法， 如果返回true，就交给你出来，并不会问下面的处理器。这就导致了如果我们简单的把我们的自定义处理器加到HandlerAdapter的Resolver列中是不行的，需要加到第一个去。 public class EmptyStringToNullProcessorRegistry implements ApplicationContextAware, BeanFactoryPostProcessor &#123; private ApplicationContext applicationContext; @Override public void setApplicationContext(ApplicationContext applicationContext) throws BeansException &#123; this.applicationContext = applicationContext; &#125; @Override public void postProcessBeanFactory(ConfigurableListableBeanFactory beanFactory) throws BeansException &#123; RequestMappingHandlerAdapter requestMappingHandlerAdapter = applicationContext.getBean(RequestMappingHandlerAdapter.class); List&lt;HandlerMethodArgumentResolver&gt; resolvers = requestMappingHandlerAdapter.getArgumentResolvers(); List&lt;HandlerMethodArgumentResolver&gt; newResolvers = new ArrayList&lt;HandlerMethodArgumentResolver&gt;(); for (HandlerMethodArgumentResolver resolver : resolvers) &#123; newResolvers.add(resolver); &#125; EmptyStringToNullModelAttributeMethodProcessor processor = new EmptyStringToNullModelAttributeMethodProcessor(true); processor.setApplicationContext(applicationContext); newResolvers.add(0, processor); requestMappingHandlerAdapter.setArgumentResolvers(Collections.unmodifiableList(newResolvers)); &#125;&#125; XML配置&lt;mvc:annotation-driven&gt; &lt;mvc:argument-resolvers&gt; &lt;bean class="studio.geek.databind.EmptyStringToNullModelAttributeMethodProcessor"&gt; &lt;constructor-arg name="annotationNotRequired" value="true"/&gt; &lt;/bean&gt; &lt;/mvc:argument-resolvers&gt;&lt;/mvc:annotation-driven&gt; 最后就可以完成将空字符串转为null 参考文档 SpringMVC对象绑定时自定义名称对应关系 SpringMVC源码分析和一些常用最佳实践]]></content>
      <categories>
        <category>SpringMVC</category>
      </categories>
  </entry>
  <entry>
    <title><![CDATA[什么是优秀的程序员]]></title>
    <url>%2F%E4%BB%80%E4%B9%88%E6%98%AF%E4%BC%98%E7%A7%80%E7%9A%84%E7%A8%8B%E5%BA%8F%E5%91%98%2F</url>
    <content type="text"><![CDATA[内容转自 再谈“我是怎么招聘程序员的”（上 - 陈皓 我以前写过一篇“我是怎么招聘程序员的”的文章（在CSDN那里有很多人进行了回复）。今天，我想再谈谈关于招聘和面试这方面的东西，主要是以下这些原因： 近半年来我在进行了大量的招聘工作，对面试有一些新的体会。 酷壳最近发布了几篇趣味面试题（面试题一，面试题二，面试题三），从回复中让我有一些思考。 我有一个同事最近面试了一家公司，他和我分享了一个博士专家对他的面试，也让我思考了一些。 在豆瓣上看到“知乎上某人写面试豆瓣产品经理的经历，很欢乐”(亮点是面试官现身知乎亲自作答) 所以，我很想把自己的这些新的想法再次写下来的。还是和以前一样，这篇文章同样是献给面试官的。我认为，面试的好坏完全在面试官而不是面试的人。下面是我对“我是怎么招聘程序员的”一文中的一些加强性的观点。（关于一些点评，请参看本文下篇） 为了让我的文章有连续性，请允许我重申一下前文的几个重要观点。 只有应聘者真实和自然的表现，才能了解到最真实的东西 重要的不是知识，重要的是其查找知识的能力 重要的不是那个解题的答案，而是解题的思路和方法 操作，知识，经验，能力 我们有很多的面试官似乎分不清，什么是操作能力，什么是知识，什么是经验，什么是能力，这导致了我们的面试官经常错误地对面试者下结论，我认为分不清这些事的人是没有资格做面试官的。所以，我有必要在这里把这个问题先讲清楚。 操作。我们的面试官分不清楚什么是操作技能，什么是知识，他们甚至认为操作技能就是知识甚至经验。比如他们会问如下的问题，请问:Java中的final是什么意思？怎么查看进程的CPU利用率？怎么编写一个管道程序？怎么查看进程的程序路径？VI中的拷贝粘贴命令是什么？包括面向对象的XX模式是什么。等等。我以为，这些能够通过查况相关操作手册或是能够google到的东西只能说明这个人的操作技术，并不能说明他有知识或有经验。 知识。知识是一个人认知和学习的体现，可能会是一些基础概念和知识。比如这些问题：TCP和UDP的优缺点比较，链表和哈希表的优缺点的比较。什么是堆什么是栈？进程间是怎么通信的？进程和线程的优缺点？同步和异步的优缺点？面向对象的XX设计模式的主要原则是什么，等等。我以为，“知其然”只是操作技术，”知其所以然”才是真正的知识。知识不够并不代表他不能工作，会操作技能就可以应付工作，但是知识的欠缺一定会限制你的经验和能力，同样会影响你的开发质量。 经验。经验通常跟一个人的经历有关系。一个人的知识范围，一个人经历过的事，通常会成为一个人经验的体现。面试中，我们会问这些问题：你解决过最难的问题是什么？你是怎么设计这个系统的？你是怎么调试和测试你的程序的？你是怎么做性能调优的？什么样的代码是好的 代码？等等。对于工作年限不长的人来说，经历和做过的事的确会成为其经验的主要因素，尤其是业务上的有行业背景的东西。但是，我更以为，经验可能更多的是你对知识的运用和驾驭，是你对做过事情的反思和总结，是你对他人的学习，观察和交流。 能力。一个人的能力并不会因为知道东西少而不行，也不会因为没有经验而没有能力。一个人的能力是他做事情的一种态度，性格，想法，思路，行为，方法和风格。只要有热情，有想法，有好的行为方法，以及好的行事风格，那么知识和经验对他来说只是一个时间问题。 比如：学习能力，专研精神，分析能力，沟通能力，组织能力，问题调查能力，合作能力等等。所以，对于一个新手来说，也许他的知识和经验有限，但并不代表他能力上有问题，但是对于一个老手来说，如果其存在知识和经验欠缺的问题，那么通常都是其能力的问题。你可能暂时怀才不遇，但我不相信你会长期怀才不遇。如果是的话，那么你必然些问题其让你的能力发挥不出来。而此时，”没有经历过”只会是你”没有能力”的一个借口。我不否认这四样东西对于一个优秀的程序员来说都很重要。但是，通过上述的分析，我们可以知道，能力和经验和知识需要分开对待。当然，这些东西是相辅相成的，你的能力可以让你获得知识，你的知识可以让你更有经验，你的经验又会改变你的想法和思路，从而改善你的能力。在面试中，我们需要清楚的认识到，应聘者的操作技能，知识和经验只是其能力的必要条件，并不是充要条件，而我们更应该关注于应聘者的能力。 如果面试只是考查这个人的操作技能的话，那么这个面试完全失败。这是一个没有资格的面试官。 如果面试只是在考查这个人的知识和经验的话，那么成功了一半。因为你了解了基础知和做过的事，但这并不代表你完全了解他的真正能力。 如果你能够在了解这个人的知识和经验的过程中重点关注其能力（态度、性格、想法，思路，行为，方法和风格），并能正确地评估这个人的能力，那么你的面试算是非常成功的。 也许用这四个词来描述定套东西并不太合适，但我相信你明白我想表达的。另外，我想说的是，我们不是出个题来考倒应聘者，而是要找到应聘者的亮点和长处。 不要肤浅地认识算法题和智力题 很多公司都会在面试的时候给一些算法题或是一些智力题或是一些设计题，我相信算法题或是智力题是程序员们在面试过程中最反感的事了。很多人都很BS面试官问的算法题，因为他们认为面试官问的这些算法题或智力题在实际工作当中用不到。但我想在这里说，问难的算法智力题并没有错，错的很多面试官只是在肤浅甚至错误地理解着面试中的难题的目的。 他们认为，能做出算法题和智力题的人就是聪明的人就是有能力的人，这种想法实在是相当的肤浅。 其实，能解难题并不意味着这个人就有能力就能在工作中解决问题，你可以想想，小学奥数题可能比这些题更难，但并不意味着那些奥数能手就有实际工作能力。你可 以想一想你们班考试得高分的同学并不一定就是聪明的人，也不一定就是有能力的人，相反，这样的人往往者是在应试教育下培养出来的书呆子。 所以，我认为解难题的过程更重要，你要主要是通过解题查看这个应聘者的思路，方法，运用到的知识，有没有一些经验，和你一起交互时和沟通得是否顺畅，等等，这些才是你重点要去观察的。当然，最终是要找到答案的。 我想，让面试者解决一个难题的真正思路是： 看看他对知识的应用和理解。 比如，他是否会用一些基础的数据结构和算法来解决算法题？ 看看他的整个解题思路和想法。 答案是次要的，他的想法和行为才是重要的。 看看他是如何和你讨论交流的。 把面试者当成你未来的同事，当成你的工作伙伴，一起解题，一起讨论，这样可以看看大家是否可以在一起工作。 这些方面才是考查应聘者的能力（思路，方法、态度，性格等），并顺带着考查面试者的经验和知识。下面是一些面试的点： 应聘者在解算法题时会不会分解或简化这个难题。这是分析能力。 应聘者在解算法题 时会不会使用一些基础知识，如数据结构和基础算法。这是知识。 应聘者在解题 时和你讨论的过程中你有没有感到应聘者的专研精神和良好的沟通。 应聘者在对待这个算法题的心态和态度。如，面试面是否有畏难情绪。 应聘者在解题时的思路和方法是否得当，是否是比较科学的方法？ 等等。 在解难题 的过程中考查应聘者的能力才是最终目的，而不是为难应聘者，不然，你只是一个傲慢而无知的面试官。 模拟实际中的挑战和能力 作为面试官的你，你应该多想想你的工作，以及你的成长经历。这会对你的面试很有帮助。你在工作中解决问题的实际情况是什么？你写代码的实际情况是什么？你的成长经历是什么？你是怎么获得知识和能力的？你喜欢和什么样的人工作？相信你不难会发现你工作中的实际情况和面试的情况完全是两码事，那么，你怎么可以用这种与实际情况差别那么大的面试来评估一个人的能力呢？ 所以，最为理想的面试是一起工作一段时间。当然，这个在招聘过程中，操作起来几乎不可能，因此，这就要求我们的面试官尽可能地把面试的过程模拟成平时工作的 过程。大家一些讨论来解决一个难题，和应聘者一起回顾一下他已经做过的事情，并在回础的过程中相互讨论相互学习。下面举一个例子。 我们知道，对于软件开发来说，开发软件不难，难是的下面是这些挑战： 软件的维护成本远远大于软件的开发成本。 软件的质量变得越来越重要，所以，测试工作也变得越来越重要。 软件的需求总是在变的，软件的需求总是一点一点往上加的。 程序中大量的代码都是在处理一些错误的或是不正常的流程。 所以，当我们在考查应聘者的代码能力时候，我们为什么不能模拟这样的过程呢？比如，让应聘者实现一个atoi()的函数，实现起来应该很简单，然后 不断地往上加新的需求或新的案例，比如：处理符号，处理非数字的字母的情况，处理有空格的情况，处理十六进制，处理二进制，处理“逗号”，等等，我们要看 应聘者是怎么修改他的代码的，怎么写测试案例的，怎么重构的，随着要处理的东西越来越多，他的代码是否还是那么易读和清晰。如果只是考查编码能力，一个小时，就问这一个问题，足矣。真正的程序员每天都在和这样的事打交道的。 如果要考查应聘者的设计能力，同样可以如法泡制。不断地加新的功 能，新的需求。看看面试者的思路，想法，分 析的方法，和你的讨论是否流畅，说没说在 点上，思想清不清晰，会应用什么样的知识，他在设计这个系统时的经验是会是什么样的，面对不断的修改和越来越复杂的需求，他的设计是否还是那么好？ 当然，因为时间比较短，所以，你不能出太复杂的问题，这需要你精心设计一些精制的有代表性的问题。]]></content>
      <categories>
        <category>编程思考</category>
      </categories>
      <tags>
        <tag>程序人生</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Windows下Nginx实现负载均衡]]></title>
    <url>%2FNginx-Load-Balancing-under-Windows%2F</url>
    <content type="text"><![CDATA[Apache,NginxApache和Nginx都属于属于 静态页面服务器，都有插件支持动态编程语言处理，但Nginx的IO模比Apache更适合跑代理。所以一般都作为前端缓冲代理(Nginx的反向代理功能)。 Tomcat,Jettytomcat和Jetty都是Java Servlet容器，可以用来生成动态页面，主要用来跑Java的Web功能，当然也提供一个简单静态页面转换： Jetty 是面向 Handler 的架构，就像 Spring 是面向 Bean 的架构，iBATIS 是面向 statement 一样，而 Tomcat 是以多级容器构建起来的，它们的架构设计必然都有一个“元神”，所有以这个“元神“构建的其它组件都是肉身 Jetty 可以很容易被扩展和裁剪，相比之下，Tomcat 要臃肿很多，Tomcat 的整体设计上很复杂 负载均衡tomcat的最大优势在于处理动态请求，处理静态内容的能力不如Apache和Nginx，并且经过测试发现，tomcat在高并发的场景下，其接受的最大并发连接数是由限制的，连接数过多会导致tomcat处于”僵死”状态，因此，在这种情况下，我们可以利用Nginx的高并发，低消耗的特点与tomcat一起使用。因此，tomcat与Nginx、Apache结合使用共有如下几点原因： Tomcat处理html的能力不如Apache和Nginx，tomcat处理静态内容的速度不如Apache和Nginx。 tomcat接受的最大并发数有限，接连接数过多，会导致tomcat处于”僵尸”状态，对后续的连接失去响应，需要结合Nginx一起使用。 通常情况下，tomcat与Nginx、Apache结合使用，Nginx、Apache既可以作为 静态页面服务器，也可以 转发动态请求 至tomcat服务器上。但在一个高性能的站点上，通常Nginx、Apache只提供代理的功能，也就是转发请求至tomcat服务器上，而对于静态内容的响应，则由前端 负载均衡 器来转发至专门的静态服务器上进行处理。其架构类似于如下图： 在这种架构中: Nginx作为前端代理时，如果是静态内容，如html、css等内容，则直接交给静态服务器处理；如果请求的图片等内容，则直接交给图片服务器处理。 如果请求的是动态内容，则交给tomcat服务器处理。 因此在这里，我们通过用Nginx作为代理服务器来转发来自前端的请求， 安装多个Tomcat 修改Tomcat端口为:8081，8082(在同一台电脑上，避免端口冲突) SHUTDOWN HTTP/1.1 redirectPort AJP/1.3 redirectPort 默认 8005 8080 8443 8009 8443 Tomcat7.1 8101 8081 - 8009 - Tomcat7.2 8102 8082 - 8009 - 在两个Tomcat中context.xml中加入(与redis客户端交互) &lt;Manager className="com.radiadesign.catalina.session.RedisSessionManager" host="localhost" port="6379" database="0" maxInactiveInterval="60" /&gt; 两个Tomcat中加入jar包 commons-pool2-2.4.2.jar jedis-2.7.3.jar tomcat-redis-session-manager-1.2-tomcat-7-java-7.jar Redis安装redis(Google，百度一下redis作为nosql，但是下载我的redis版本，在最后)，看一下bin下的RedisService.docx Windows下Nginx命令 启动直接点击Nginx目录下的Nginx.exe 或者 cmd运行start Nginx 关闭Nginx -s stop 或者 Nginx -s quitstop表示立即停止Nginx,不保存相关信息quit表示正常退出Nginx,并保存相关信息 重启(因为改变了配置,需要重启)Nginx -s reload 关闭进程tskill Nginx Nginx负载均衡配置在Nginx.conf配置文件中加入注释部分 # upstream localhost &#123;# server localhost:8081;# server localhost:8082;# &#125;server &#123; listen 80; server_name localhost; location / &#123; root html; index index.html index.htm; # proxy_pass http://localhost; # proxy_set_header Host $host; # proxy_set_header X-Real-IP $remote_addr; # proxy_set_header X-Forwarded-For $proxy_add_x_forwarded_for; &#125;&#125; 测试 在两个Tomcat中的webapps下的ROOT下加入一个session.jsp &lt;%=session.getId()%&gt; + Tomcat7.1 &lt;%=session.getId()%&gt; + Tomcat7.2 依次运行redis，Tomcat，Nginx 成功jar包，redis，Nginx，Tomcat ： http://pan.baidu.com/s/1c1NSPzU 总结Tomcat9配置最开始，我采用的是Tomcat9，在我安装的时候总是有问题，很多东西已经改了，tomcat9安装来安装去，结果都是同一个Tomcat，有时候版本高就是有点坑。 Tomcat9必须要配置 CATALINA_HOME 问题 ： 必须配置CATALINA_HOME变量，不然就闪退，但是如果配置就打开了自己的用的Tomcat，不是需要测试的Tomcat,结果是加的东西改变了他的属性 具体配置参考 ： windows下安装多个tomcat服务 安装服务在命令行中进入/Tomcat路径/bin/，执行“service.bat install”：说明： 服务名和显示名称：service.bat中设置了默认的服务名称，不同版本分别命名为Tomcat4、Tomcat5、Tomcat6，如果需要自 定义服务名或服务的显示名称，可在service.bat中修改SERVICE_NAME或PR_DISPLAYNAME； 防火墙的影响：/bin/tomcat6.exe（或tomcat4.exe、tomcat5.exe）将被作为服务程序，如果有防火墙，需要设为允许作为服务。 卸载服务在命令行中进入/Tomcat路径/bin/，执行“service.bat remove”： Jar包jar包是一个很大的问题，有些jar包不支持(错误为仔细记录)，而且Tomcat-session jar的实现版本也不一样。 不过正因为踩了坑，才更有趣，不是吗？]]></content>
      <categories>
        <category>服务器</category>
      </categories>
      <tags>
        <tag>Nginx</tag>
        <tag>反向代理</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[SpringMVC实现PUT请求上传文件]]></title>
    <url>%2FSpringMVC-Achieve-RESTful-And-Put-With-File-Upload%2F</url>
    <content type="text"><![CDATA[在JQuery中，我们可以进行REST ful中delete和put的请求，但是在java EE标准中，默认只有在POST请求的时候，servlet 才会通过getparameter()方法取得请求体中的相应的请求参数的数据。而PUT，delete请求的请求体中数据则默认不会被解析。 关于delete请求：delete请求用来从服务器上删除资源。因此我们只需要把要删除的资源的ID上传给服务器，即使是批量删除的时候，也可以通过URL传参的方式将多个id传给servlet，因此，可以满足我们的需求，可以直接发送请求。 关于put请求(指的是带有请求体) 没有文件时：SpringMVC提供了一个将post转换为put和delete的方法，通过在web.xml中注册一个HiddenHttpMethodFilter过滤器。 上传文件时：我们可以通过在web.xml中注册一个MultipartFilter，一定要在HiddenHttpMethodFilter之前。 SpringMVC实现PUT,DELETE请求&lt;context-param&gt; &lt;param-name&gt;contextConfigLocation&lt;/param-name&gt; &lt;param-value&gt;classpath:applicationContext.xml&lt;/param-value&gt; &lt;/context-param&gt; &lt;listener&gt; &lt;listener-class&gt;org.springframework.web.context.ContextLoaderListener&lt;/listener-class&gt; &lt;/listener&gt; &lt;servlet&gt; &lt;servlet-name&gt;dispatcher&lt;/servlet-name&gt; &lt;servlet-class&gt;org.springframework.web.servlet.DispatcherServlet&lt;/servlet-class&gt; &lt;init-param&gt; &lt;param-name&gt;contextConfigLocation&lt;/param-name&gt; &lt;param-value&gt;classpath:dispatcher-servlet.xml&lt;/param-value&gt; &lt;/init-param&gt; &lt;load-on-startup&gt;1&lt;/load-on-startup&gt; &lt;/servlet&gt; &lt;servlet-mapping&gt; &lt;servlet-name&gt;dispatcher&lt;/servlet-name&gt; &lt;url-pattern&gt;/&lt;/url-pattern&gt; &lt;/servlet-mapping&gt; &lt;filter&gt; &lt;filter-name&gt;HiddenHttpMethodFilter&lt;/filter-name&gt; &lt;filter-class&gt;org.springframework.web.filter.HiddenHttpMethodFilter&lt;/filter-class&gt; &lt;/filter&gt; &lt;filter-mapping&gt; &lt;filter-name&gt;HiddenHttpMethodFilter&lt;/filter-name&gt; &lt;servlet-name&gt;dispatcher&lt;/servlet-name&gt; &lt;/filter-mapping&gt; 然后我们看源码：@Override protected void doFilterInternal(HttpServletRequest request, HttpServletResponse response, FilterChain filterChain) throws ServletException, IOException &#123; String paramValue = request.getParameter(this.methodParam); if ("POST".equals(request.getMethod()) &amp;&amp; StringUtils.hasLength(paramValue)) &#123; String method = paramValue.toUpperCase(Locale.ENGLISH); HttpServletRequest wrapper = new HttpMethodRequestWrapper(request, method); filterChain.doFilter(wrapper, response); &#125; else &#123; filterChain.doFilter(request, response); &#125; &#125; this.methodParam属性被默认初始化为”_method“，通过request.getParameter(this.methodParam);判断是put还是delete， &quot;POST&quot;.equals(request.getMethod())，而且必须要求是post方式提交的， 然后它把request进行包装后传给下一个filter。 因此，我们需要在提交的时候添加一个字段 &lt;form action="" id="formData" name="formData" method="post"&gt; &lt;input type="text" name="username" id="username"/&gt; &lt;input type="hidden" name="_method" value="delete"/&gt; &lt;input type="submit" value="submit"/&gt;&lt;/form&gt; 或者在$.ajax中function login() &#123; $.ajax(&#123; type: "post",//请求方式 url: "", //发送请求地址 timeout: 30000,//超时时间：30秒 data: &#123; "username": $('#username').val(), "password": $("#password").val(), "_method": delete &#125;, dataType: "json",//设置返回数据的格式 success: function (data) &#123; console.log(data); &#125;, error: function () &#123; //请求出错的处理 &#125; &#125;);&#125; 然后我们就可以在后台@RequestMapping(value = &quot;&quot;, method = RequestMethod.PUT)注解中标识我们的方法，最后就可以成功地获得数据。 SpringMVC实现PUT请求上传文件可是后来我又有遇到另外一个需求那就是修改的时候需要传送文件到put方法中，于是这种方法就不可行了，但是我在HiddenHttpMethodFilter源码中看到这样一句话* &lt;p&gt;&lt;b&gt;NOTE: This filter needs to run after multipart processing in case of a multipart* POST request, due to its inherent need for checking a POST body parameter.&lt;/b&gt;* So typically, put a Spring &#123;@link org.springframework.web.multipart.support.MultipartFilter&#125;* &lt;i&gt;before&lt;/i&gt; this HiddenHttpMethodFilter in your &#123;@code web.xml&#125; filter chain. 和MultipartFilter源码中这样的注释/** * Set the bean name of the MultipartResolver to fetch from Spring&apos;s * root application context. Default is &quot;filterMultipartResolver&quot;. */ 也就是说我们可以通过在web.xml中注册一个MultipartFilter，一定要在HiddenHttpMethodFilter之前。 &lt;filter&gt; &lt;filter-name&gt;MultipartFilter&lt;/filter-name&gt; &lt;filter-class&gt;org.springframework.web.multipart.support.MultipartFilter&lt;/filter-class&gt; &lt;/filter&gt; &lt;filter-mapping&gt; &lt;filter-name&gt;MultipartFilter&lt;/filter-name&gt; &lt;servlet-name&gt;dispatcher&lt;/servlet-name&gt; &lt;/filter-mapping&gt; &lt;filter&gt; &lt;filter-name&gt;HiddenHttpMethodFilter&lt;/filter-name&gt; &lt;filter-class&gt;org.springframework.web.filter.HiddenHttpMethodFilter&lt;/filter-class&gt; &lt;/filter&gt; 然后再在Spring的 root application context中添加如下代码 &lt;bean id="filterMultipartResolver" class="org.springframework.web.multipart.commons.CommonsMultipartResolver"&gt; &lt;property name="maxUploadSize" value="209715200"/&gt; &lt;property name="defaultEncoding" value="UTF-8"/&gt; &lt;property name="resolveLazily" value="true"/&gt;&lt;/bean&gt; FormData对象是html5的一个对象，目前的一些主流的浏览器都已经兼容。FormData对象是html5的一个对象，目前的一些主流的浏览器都已经兼容。 function test() &#123; var form = new FormData(document.getElementById("tf")); form.append("_method", 'put'); $.ajax(&#123; url: url, type: 'post', data: form, processData: false, contentType: false, success: function (data) &#123; window.clearInterval(timer); console.log("over.."); &#125;, error: function (e) &#123; alert("错误！！"); window.clearInterval(timer); &#125; &#125;); get();//此处为上传文件的进度条&#125; &lt;form id="tf" method="post" name="formDada" enctype="multipart/form-data"&gt; &lt;input type="file" name="file"/&gt; &lt;input type="text" name="id"/&gt; &lt;input type="text" name="name"/&gt; &lt;input type="button" value="提" onclick="test()"/&gt;&lt;/form&gt; 最后，就可以实现将文件上传提交给put方法。]]></content>
      <categories>
        <category>SpringMVC</category>
      </categories>
  </entry>
  <entry>
    <title><![CDATA[Servlet 浅析]]></title>
    <url>%2FServlet%2F</url>
    <content type="text"><![CDATA[在我们学习Servlet之前，有必要了解一下Web容器的工作模式 我们所有的请求其实都是先到达了web容器，然后才分发给已经注册好的Servlet 请求由Servlet的service方法调用doGet()和doPost()进行反应。 JSPJSP的本质是一个Servlet类，他弥补了Servlet不好做界面的劣势。 Servlet 工作原理解析url匹配和url-patternurl的匹配当一个请求发送到servlet容器的时候，容器先会将请求的url减去当前应用上下文的路径作为servlet的映射url，比如我访问的是http://localhost/test/aaa.html，我的应用上下文是test，容器会将http://localhost/test去掉，剩下的/aaa.html部分拿来做servlet的映射匹配。这个映射匹配过程是有顺序的，而且当有一个servlet匹配成功以后，就不会去理会剩下的servlet了（filter不同，后文会提到）。其匹配规则和顺序如下： 精确路径匹配。例子：比如servletA 的url-pattern为 /test，servletB的url-pattern为 /* ，这个时候，如果我访问的url为http://localhost/test ，这个时候容器就会先进行精确路径匹配，发现/test正好被servletA精确匹配，那么就去调用servletA，也不会去理会其他的servlet了。 最长路径匹配。例子：servletA的url-pattern为/test/，而servletB的url-pattern为/test/a/，此时访问http://localhost/test/a时，容器会选择路径最长的servlet来匹配，也就是这里的servletB。 扩展匹配，如果url最后一段包含扩展，容器将会根据扩展选择合适的servlet。例子：servletA的url-pattern：*.action 如果前面三条规则都没有找到一个servlet，容器会根据url选择对应的请求资源。如果应用定义了一个default servlet，则容器会将请求丢给default servlet（什么是default servlet？后面会讲）。根据这个规则表，就能很清楚的知道servlet的匹配过程，所以定义servlet的时候也要考虑url-pattern的写法，以免出错。 对于filter，不会像servlet那样只匹配一个servlet，因为filter的集合是一个链，所以只会有处理的顺序不同，而不会出现只选择一个filter。Filter的处理顺序和filter-mapping在web.xml中定义的顺序相同。 url-pattern详解在web.xml文件中，以下语法用于定义映射： 以”/’开头和以”/*”结尾的是用来做路径映射的。 以前缀”*.”开头的是用来做扩展映射的。 “/” 是用来定义default servlet映射的。 剩下的都是用来定义详细映射的。比如： /aa/bb/cc.action 所以，为什么定义”/*.action”这样一个看起来很正常的匹配会错？因为这个匹配即属于路径映射，也属于扩展映射，导致容器无法判断 生命周期 Servlet装载的三种情况： 自动装载：某些Servlet如果需要在Servlet容器启动时就加载，需要在web.xml下它的&lt;Servlet&gt;标签里中，添加优先级代码：&lt;Servlet&gt;&lt;load-on-startup&gt;1&lt;load-on-startup&gt;&lt;/Servlet&gt; 数字越小表示该servlet的优先级越高，会先于其他自动装载的优先级较低的先装载,在SpringMVC中有这个配置&lt;servlet&gt; &lt;servlet-name&gt;dispatcher&lt;/servlet-name&gt; &lt;servlet-class&gt;org.springframework.web.servlet.DispatcherServlet&lt;/servlet-class&gt; &lt;load-on-startup&gt;1&lt;/load-on-startup&gt; &lt;/servlet&gt; &lt;servlet-mapping&gt; &lt;servlet-name&gt;dispatcher&lt;/servlet-name&gt; &lt;url-pattern&gt;*.form&lt;/url-pattern&gt; &lt;/servlet-mapping&gt; Servlet容器启动后，客户首次向某个Servlet发送请求时，Tomcat容器会加载它 当Servlet类文件被更新后，也会重新自动加载Servlet是长期驻留在内存里的。某个Servlet一旦被加载，就会长期存在于服务器的内存里，直到服务器关闭Servlet被装载后，Servlet容器通过反射创建一个Servlet实例并且调用Servlet的init()方法进行初始化。在Servlet的整个生命周期内，init()方法只被调用一次 Servlet得到JSP对象 Servlet 3.0为什么出现了Servlet3.0，一切的一切都是为了效率的提高，恰如，框架的出现。Servlet 3.0 新特性详解]]></content>
      <categories>
        <category>Java</category>
      </categories>
      <tags>
        <tag>Servlet</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[JSP与HTML及前后分离]]></title>
    <url>%2FJSP%E4%B8%8EHTML%E5%8F%8A%E5%89%8D%E5%90%8E%E5%88%86%E7%A6%BB%2F</url>
    <content type="text"><![CDATA[JSP是什么首先要知道JSP的本质其实是个Servlet，index.jsp在访问的时候首先会自动将该页面翻译生一个index_jsp.java文件，即Servlet代码。 打开这个类你会发现这个类继承了类org.apache.jasper.runtime.HttpJspBase.SUN在JSP API中定义了一个接口HttpJspBase,这个接口继承了JspPage接口，而JspPage接口又继承了Servlet接口，因此WEB容器必须实现这些接口。org.apache.jasper.runtime.HttpJspBase就是Tomcat对JSP API中HttpJspBase接口的实现。因此JSP页面在本质上就是Servlet程序，而Servlet程序要被WEB容器调用执行，必须在WEB.XML中注册映射，对于JSP，这些则由WEB容器自动完成。 HTML是什么参考维基百科的定义： 超文本标记语言（英语：HyperText Markup Language，简称：HTML）是一种用于创建网页的标准标记语言。HTML是一种基础技术，常与CSS、JavaScript一起被众多网站用于设计令人赏心悦目的网页、网页应用程序以及移动应用程序的用户界面[1]。网页浏览器可以读取HTML文件，并将其渲染成可视化网页。HTML描述了一个网站的结构语义随着线索的呈现，使之成为一种标记语言而非编程语言。 HTML元素是构建网站的基石。HTML允许嵌入图像与对象，并且可以用于创建交互式表单，它被用来结构化信息——例如标题、段落和列表等等，也可用来在一定程度上描述文档的外观和语义。HTML的语言形式为尖括号包围的HTML元素（如），浏览器使用HTML标签和脚本来诠释网页内容，但不会将它们显示在页面上。 HTML可以嵌入如JavaScript的脚本语言，它们会影响HTML网页的行为。网页浏览器也可以引用层叠样式表（CSS）来定义文本和其它元素的外观与布局。维护HTML和CSS标准的组织万维网联盟（W3C）鼓励人们使用CSS替代一些用于表现的HTML元素[2]。 JSP VS HTMLJSP与Html相比，谁更适合做页面？ java的web开发里我们一般使用jsp来编写页面，当然也可以使用先进点的模板引擎开发页面例如velocity，freemark等，不管我们页面使用的是jsp还是模板引擎，这些类似html的文件其实并不是真正的html，例如jsp本质其实是个servlet也就是一个java程序，所以它们的本质是服务端语言和html的一个整合技术，在实际运行中web容器会根据服务端的返回数据将jsp或模板引擎解析成浏览器能解析的html，然后传输这个html到浏览器进行解析。 现在比较提倡前后分离，前后端通过API来相互连接，后端给出接口，前端通过AJAX访问接口，拿到数据，动态更新。 前后分离前后端分离是趋势，但是短时间内不可能取代不分离的。 因为前后分离导致：数据和表现分离，只需要静态的html和动态的接口（例如jsp），数据在浏览器端实现动态加载。因此存在问题，例如SEO，搜索引擎难以识别等，我们可以参考淘宝前端的设计，在 java 接口和 html 输出之前用 NodeJS 代理一层，暂时能解决 SEO 的问题。 理想情况是，先出文档（前后端都认可），然后后端、前端都按照文档来，一切以接口规定的为准。 重点在于接口！靠 API 来分离前后端，解决前后端大团队、多版本、复杂功能协作的问题。定义好了接口，前端就可以用不用等后端，直接用模拟的数据格式，方便地进行前端测试了。 前端可以通过Nginx来解决跨域问题。具体可以查看 前端通过Nginx反向代理解决跨域问题 说重点，API 相比前后端混写、模板引擎之类的东西的好处： 方便设计、开发、测试（前端不再需要依赖后端，后端也不需要依赖前端，就可以各干各的，独立测试代码） 方便记录和统计功能使用（后端相同功能的入口位置统一，不同功能的位置也可以合理有序地组织） 方便修改和版本控制等（后端可以提供多版本的 API，不需要修改已有代码，不影响已有 API 的功能） 最重点的是：你的Team要是分工不明确、人少、功能简单直接、代码修改不多，就完全不需要分离，就酱。 最明显的：前端代码不用被后端粘贴来粘贴去了，后端的相同代码，也不需要各种位置粘贴来粘贴去了。 隐藏的好处：到时候出了问题，照着 API 设计文档一对比，就知道是前端用的不对，还是后端写的不对，分分钟找到背锅侠。 可以一套接口，浏览器，Android通用，极大减少开发量。 RESTFul API我们知道前后分离之后是靠API统一前后端的开发，而RESTFul API目前是前后端分离的最佳实践。 它有如下好处： 轻量，直接通过http，不需要额外协议，post/get/put/delete操作 面向资源，一目了然，具有自解释性 数据描述简单，一般通过json或者xml做数据通信 下面参考阮一峰的博客解释一下RESTful架构。 要理解RESTful架构，最好的方法就是去理解Representational State Transfer这个词组到底是什么意思，它的每一个词代表了什么涵义。如果你把这个名称搞懂了，也就不难体会REST是一种什么样的设计。 资源（Resources）REST的名称”表现层状态转化”中，省略了主语。”表现层”其实指的是”资源”（Resources）的”表现层”。 所谓”资源”，就是网络上的一个实体，或者说是网络上的一个具体信息。它可以是一段文本、一张图片、一首歌曲、一种服务，总之就是一个具体的实在。你可以用一个URI（统一资源定位符）指向它，每种资源对应一个特定的URI。要获取这个资源，访问它的URI就可以，因此URI就成了每一个资源的地址或独一无二的识别符。 所谓”上网”，就是与互联网上一系列的”资源”互动，调用它的URI。 表现层（Representation）“资源”是一种信息实体，它可以有多种外在表现形式。我们把”资源”具体呈现出来的形式，叫做它的”表现层”（Representation）。 比如，文本可以用txt格式表现，也可以用HTML格式、XML格式、JSON格式表现，甚至可以采用二进制格式；图片可以用JPG格式表现，也可以用PNG格式表现。 URI只代表资源的实体，不代表它的形式。严格地说，有些网址最后的”.html”后缀名是不必要的，因为这个后缀名表示格式，属于”表现层”范畴，而URI应该只代表”资源”的位置。它的具体表现形式，应该在HTTP请求的头信息中用Accept和Content-Type字段指定，这两个字段才是对”表现层”的描述。 状态转化（State Transfer）访问一个网站，就代表了客户端和服务器的一个互动过程。在这个过程中，势必涉及到数据和状态的变化。 互联网通信协议HTTP协议，是一个无状态协议。这意味着，所有的状态都保存在服务器端。因此，如果客户端想要操作服务器，必须通过某种手段，让服务器端发生”状态转化”（State Transfer）。而这种转化是建立在表现层之上的，所以就是”表现层状态转化”。 客户端用到的手段，只能是HTTP协议。具体来说，就是HTTP协议里面，四个表示操作方式的动词：GET、POST、PUT、DELETE。它们分别对应四种基本操作：GET用来获取资源，POST用来新建资源（也可以用于更新资源），PUT用来更新资源，DELETE用来删除资源。 总结RESTful架构综合上面的解释，我们总结一下什么是RESTful架构： 每一个URI代表一种资源； 客户端和服务器之间，传递这种资源的某种表现层； 客户端通过四个HTTP动词，对服务器端资源进行操作，实现”表现层状态转化”。 GraphQLGraphQL是一种API查询语言，是由Facebook创建并最终开源的，可以认为是REST的一种替代品。来自AXA Banque的API架构师Lauret给出了一些可对二者进行比较的切入点： GraphQL能够通过一次查询得到所有需要的数据，从而减少网络跳转的次数。 GraphQL采用所见即所得模型，这样客户端代码不易出错。 RESTful HTTP通过使用状态码和HTTP verb，提高了结果的一致性和可预测性。 借助超媒体，在用户使用API时可以“发现”资源间的关系，这简化了RESTful用户的具体实现。 HTTP实现了缓存机制而GraphQL还没有实现。 GraphQL给用户提供了schema，这很有用，但是需要注意的是接口描述并非API文档。 Lauret认为GraphQL的主要优势是其使用的所见即所得(WYSIWYG)模型。也就是说，查询结果的结构是查询结构本身的精确映射，这样的话，用户在解排（unmarshal）响应的时候不容易出错。 有兴趣的朋友可以看看这两篇解释GraphQL的文章： GraphQL和REST对比时需要注意些什么 安息吧 REST API，GraphQL 长存 我觉得GraphQL更多的是作为REST的一种替代品。 参考文档： JSP本质 JSP/Servlet 工作原理 HTML - 维基百科，自由的百科全书 关于大型网站技术演进的思考（九）–网站静态化处理–总述（1） 这样是不是就叫 前后端分离开发了？ http://2014.jsconf.cn/slides/herman-taobaoweb/index.html#/ 理解RESTful架构]]></content>
      <categories>
        <category>Web前端</category>
      </categories>
      <tags>
        <tag>JSP</tag>
        <tag>HTML</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[String解析]]></title>
    <url>%2FString%2F</url>
    <content type="text"><![CDATA[可以证明，字符串操作是计算机程序设计中最常见的行为。String对象是不可变的，查看JDK文档你就会发现，String类每一个看起来会修改String值得方法，实际上都是创建了一个全新的String对象，以包含修改后的字符串内容。而最初的string对象则丝毫未动。–Thinking in Java 常量池String str1 = "abc";String str2 = "abc";String str3 = new String("abc");String str4 = new String("abc");String str5 = "abc" + "li";String str6 = "abc" + "li";String str7 = str1 + "li";String str8 = str1 + "li";String str9 = str1 + str2;String str10 = str1 + str2;System.out.println(str1 == str2);System.out.println(str3 == str4);System.out.println(str5 == str6);System.out.println(str7 == str8);System.out.println(str9 == str10); 结果：truefalsetruefalsefalse 说明： 在JAVA虚拟机（JVM）中存在着一个字符串池，其中保存着很多String对象，并且可以被共享使用，因此它提高了效率。由于String类是final的，它的值一经创建就不可改变，因此我们不用担心String对象共享而带来程序的混乱。字符串池由String类维护，我们可以调用intern()方法来访问字符串池。 我们再回头看看String str1 = “abc”; , 这行代码被执行的时候，JAVA虚拟机首先在字符串池中查找是否已经存在了值为”abc”的这么一个对象，它的判断依据是String类equals(Object obj)方法的返回值。如果有，则不再创建新的对象，直接返回已存在对象的引用；如果没有，则先创建这个对象，然后把它加入到字符串池中，再将它的引用返回。 new 就是在堆中重新分配了一块内存，虽然 str3 和 str4 内容相同，但它们指向的是不同的内存。 使用 “ + ”时，“字符1 “ + “字符2” 在上面第一条已说明，“字符1”+ 引用 和引用 + 引用 都将创建新的对象。 String 相等 ? 引用相等：“==”，通过“==”运算符来比较内存地址是否相等 C++String类重载了==运算符以检测字符串内容的相等性。 内容相等：equals 方法比较两个String对象的内容是否相同时是使用equals方法，equals方法是String类继承与Object类的，在Object类的equals方法的本质其实是和“==”一样的，都是比较两个对象引用是否指向同一个对象（即两个对象是否为同一对象）。那为什么String类的equals方法却又是比较两个String对象的内容是否相同呢？ 原来是这样的，String类继承Object类后，也继承了equals方法，但String类对equals方法进行了重写，改变了equals方法的比较形式。其实很多其他类继承Object类后也对equals方法进行了重写。 jdk1.8.0_92 中String类中equals 方法public boolean equals(Object anObject) &#123; if (this == anObject) &#123; return true; &#125; if (anObject instanceof String) &#123; String anotherString = (String)anObject; int n = value.length; if (n == anotherString.value.length) &#123; char v1[] = value; char v2[] = anotherString.value; int i = 0; while (n-- != 0) &#123; if (v1[i] != v2[i]) return false; i++; &#125; return true; &#125; &#125; return false;&#125; 编码：代码点和代码单元 字节是计算机存储信息的基本单位，1 个字节等于 8 位， gbk 编码中 1 个汉字字符存储需要 2 个字节，1 个英文字符存储需要 1 个字节。所以我们看到上面的程序运行结果中，每个汉字对应两个字节值，如“学”对应 “-47 -89” ，而英文字母 “J” 对应 “74” 。同时，我们还发现汉字对应的字节值为负数，原因在于每个字节是 8 位，最大值不能超过 127，而汉字转换为字节后超过 127，如果超过就会溢出，以负数的形式显示。 正则：String类自带正则表达式：public boolean matches(String regex)告知此字符串是否匹配给定的正则表达式。调用此方法的 str.matches(regex) 形式与以下表达式产生的结果完全相同：Pattern.matches(regex, str) java 的 “ \ “等于其他语言的” \ 知识点：空串是一个对象 常用APIpublic char **charAt**(int index) 返回指定索引处的char值]]></content>
      <categories>
        <category>Java</category>
      </categories>
      <tags>
        <tag>String</tag>
      </tags>
  </entry>
</search>
